WEBVTT

00:00:00.000 --> 00:00:05.580
 I wish there was like a public chart, like where you could just see the number of normal people and the number of people who they're lost to the ether.

00:00:05.580 --> 00:00:06.340
 Are you normal?

00:00:06.340 --> 00:00:06.680
 Yeah.

00:00:30.000 --> 00:00:30.800
 How are you?

00:00:30.800 --> 00:00:40.060
 I'm good, it's Friday, and we're trying to do a thing where we usually go out to dinner every Friday, but we're trying to switch it up and do lunch every Friday.

00:00:40.060 --> 00:00:46.280
 Oh, okay, just doing this little Friday afternoon lunch, a little siesta.

00:00:46.280 --> 00:00:48.560
 Oh, siesta comes after.

00:00:48.560 --> 00:00:50.840
 Oh, really? That's after lunch?

00:00:50.840 --> 00:00:59.180
 A siesta is a nap that people take after lunch and I think it was invented in Spain, and it's literally like a real thing, like everything just shuts down for two hours.

00:00:59.180 --> 00:01:00.760
 Yeah, I'm just shut, that's so cool.

00:01:00.760 --> 00:01:01.760
 I wish we did that.

00:01:01.760 --> 00:01:05.160
 I mean, I can't really, I wouldn't want to take a nap, but it sounds fun.

00:01:05.160 --> 00:01:18.080
 I'm not someone that really gets sleepy after eating and like late afternoon, but so many people do that I'm just convinced there's some biological thing we're supposed to do that for the most part.

00:01:18.080 --> 00:01:19.920
 So I thought I didn't know it was Spain.

00:01:19.920 --> 00:01:26.880
 I thought it was like Italians, and they all just ate like a ton of pasta, and then I get sleepy after pasta, so I could imagine wanting to take a nap.

00:01:26.880 --> 00:01:29.760
 Like a big bag of it. I don't know. Where's that France?

00:01:29.760 --> 00:01:31.840
 Bag of it's French. Well, you're really confused about you.

00:01:31.840 --> 00:01:32.800
 I'm really confused.

00:01:32.800 --> 00:01:37.240
 What are people eating Spain? I have no idea what they eat in Spain.

00:01:37.240 --> 00:01:49.000
 Like I'm imagining in your head, you're like imagining Europe, and it's just like a table with like pasta and a baguette pizza and like a pizza, American pizza.

00:01:49.000 --> 00:01:56.880
 What do Spanish people eat? Spanish food is really good. We eat a lot of it here. Like tapas.

00:01:56.880 --> 00:01:58.280
 Like, oh, tapas. What's the tapa?

00:01:58.280 --> 00:02:05.640
 Tapas isn't really a thing really. It just means a lot of small plates, which is a great way to just give you like 10 different things.

00:02:05.640 --> 00:02:14.520
 Yeah, it's kind of like if you have food, I mean, same concept where you're just getting a lot of things, but it's not like stuff you dip and other stuff. It's all standalone.

00:02:14.520 --> 00:02:24.520
 Oh, gotcha. Do they like like fried rice? No, that's Asian. Wild rice? What am I thinking about with rice in Spanish people?

00:02:24.520 --> 00:02:31.680
 Yeah. What's there's that famous dish with the seafood? I'm forgetting, I'm forgetting the term now.

00:02:31.680 --> 00:02:34.240
 It was probably something different because I didn't know there was seafood involved.

00:02:34.240 --> 00:02:38.680
 There's a lot of seafood in Spanish food. Yeah, that makes sense. Spain only ocean.

00:02:38.680 --> 00:02:46.920
 Paella. Paella. Yeah. No, never heard of it. Sounds awesome. Well, I don't like seafood, but but there's rice. I like rice a lot.

00:02:46.920 --> 00:02:53.640
 I forgot you don't eat meat for a second. I forget. I do eat a lot of rice. I love rice.

00:02:53.640 --> 00:03:00.680
 So my favorite thing, I mean, oh, no, this isn't the thing. I'm thinking of isn't isn't.

00:03:00.680 --> 00:03:09.680
 Hang on. I gotta look this up because it's driving me crazy. Ceviche. I'm thinking of Ceviche. Ceviche. I do not. OK.

00:03:09.680 --> 00:03:23.680
 Well, Ceviche is raw. It's kind of like sushi. It's like a mix of different raw fishes or like just stuff like that, but they use a ton of lime and citrus and a citrus acid cooks the fish.

00:03:23.680 --> 00:03:33.680
 I think this is like nice, like tangy. Do she like flavor? But it's like, you know, technically cooked. And that's interesting. I think it's like it literally cooks like they don't use heat. Just the citric acid.

00:03:33.680 --> 00:03:42.680
 Yeah. It's like a form. It like it's like a version of it being cooked. Yeah. And it's interesting. The history behind is interesting because I recently understood this.

00:03:42.680 --> 00:03:52.680
 So that comes from Peru. And you think Peru is South America. You have like an image in your head. But there's a there were some point in history where a ton of Japanese people moved to Peru.

00:03:52.680 --> 00:04:00.680
 And their cuisine is crazy interesting because of that. Ceviche is like sushi, but like South American style.

00:04:00.680 --> 00:04:11.680
 But South America is interesting because Japanese people moved. That's so cool. Yeah. And now they have the whole weird dedicated cuisine. But it's from that. So there you go. A little fun fact for you.

00:04:11.680 --> 00:04:25.680
 Is paella actually a thing, though? That is a thing. It's a Spanish dish. And I don't like it because they just put like my perception of it is they're just like, let me just take every possible sea creature and just throw it in the pot and provides with it.

00:04:25.680 --> 00:04:34.680
 They just put everything. Not even know what I don't even know if there's like an official set of ingredients, but they'll be like clams and shrimp and like oysters and like just like.

00:04:34.680 --> 00:04:42.680
 That might not be correct, but I just know that I just don't like it. It's like every time we take a spoon, there's like a different sea creature on your.

00:04:42.680 --> 00:04:53.680
 So I don't like paella. It is really popular. I don't like it. Oh, right. I feel like that word paella. I think that's a town in Maui. I think that's why that word is familiar to me. I'm pretty sure.

00:04:53.680 --> 00:05:01.680
 Yeah. It's like a little town in Hawaii and they had this awesome health food store called Mana Health Foods. And it's super cool.

00:05:01.680 --> 00:05:08.680
 Anyway, how are you imagining? When I say paella, how are you imagining? It's spelled like P-A-I-A, maybe. There's two L's in it.

00:05:08.680 --> 00:05:18.680
 What? Oh, yeah, you have the flu. That's I. We didn't do last week because I was sick and I've still got the cough. So it's kind of gross.

00:05:18.680 --> 00:05:24.680
 And I'll try not to too much in the mic. Maybe Chris can edit out all the coughs. Sounds fun.

00:05:24.680 --> 00:05:27.680
 I'm just going to cough over everything you say and just make it hard for him.

00:05:27.680 --> 00:05:33.680
 Is this a new thing where when you get sick, you just stay sick for a month? I feel like this just was not a thing before.

00:05:33.680 --> 00:05:40.680
 This was not a thing before. So I thought I had a cold because it started out just like every other cold I get and I get like one every quarter.

00:05:40.680 --> 00:05:47.680
 I don't know. Like a little two day like kind of have like sore throat, whatever. A cold. I thought it was one of those.

00:05:47.680 --> 00:05:53.680
 It was not one of those. It was the flu, I think. Influenced A, bird flu. I don't know. There's a lot of words floating around.

00:05:53.680 --> 00:05:58.680
 But I was out for a whole week and the cough is still going and it's been two weeks.

00:05:58.680 --> 00:06:03.680
 So I think and I've heard that from other people that the cough like lasted three weeks or whatever, which sucks.

00:06:03.680 --> 00:06:14.680
 Yeah. When I had COVID that that's when I had that. So I'm wondering if like it's like COVID related now because actually everyone just has when they get sick, they're just off for like a month, you know, better after a week.

00:06:14.680 --> 00:06:27.680
 I thought I had COVID because like before Casey saw on TikTok that it's influenced a I thought it was COVID because I could have sworn I couldn't taste anything for like two days stuff that I ate all the time just tasted like nothing.

00:06:27.680 --> 00:06:31.680
 Maybe COVID mixed with the bird flu. I don't know. I'm not a scientist.

00:06:31.680 --> 00:06:44.680
 When COVID was really peaking, you know, when all the news reports were like, New York is the epicenter of COVID. It is a disaster. Pop up tents and morgues or whatever. Yeah.

00:06:44.680 --> 00:06:48.680
 I was like, I'm going for really nice walks every day because of my topic.

00:06:48.680 --> 00:06:54.680
 Liz lost her sense of taste for two days and had no other symptoms at all. Really.

00:06:54.680 --> 00:06:58.680
 It was really weird, but she got to taste back because I've heard some people taste forever.

00:06:58.680 --> 00:07:07.680
 Yeah. Some people die forever because they can't taste the food. No, no, like some people died and then they're dead forever from COVID.

00:07:07.680 --> 00:07:09.680
 Yeah. Okay. Yeah.

00:07:09.680 --> 00:07:12.680
 You can lose your taste forever or your life forever.

00:07:12.680 --> 00:07:18.680
 You're right. You're right. COVID's not good. I agree. It's not controversial.

00:07:18.680 --> 00:07:21.680
 Is it still? Is it actually still active though? Is there still COVID?

00:07:21.680 --> 00:07:24.680
 I think so. I think it's kind of like the flu now, you know.

00:07:24.680 --> 00:07:34.680
 I also like this is also very stupid. I didn't put together that the flu and the Spanish flu are the same thing.

00:07:34.680 --> 00:07:38.680
 And the reason we have the flu every year is because the Spanish flu happened.

00:07:38.680 --> 00:07:41.680
 Wait, what was Spanish flu? I don't know history that well.

00:07:41.680 --> 00:07:44.680
 The Spanish flu was like the last big pandemic prior to COVID.

00:07:44.680 --> 00:07:48.680
 Oh, that was the first that was the start of the flu. That was like the first flu.

00:07:48.680 --> 00:07:56.680
 Yeah, it was word. I think it was called endemic. Well, like a thing like that shows up and then becomes endemic, which means it's just like part of the world.

00:07:56.680 --> 00:08:03.680
 Yeah. So that's what COVID. Okay. Well, then that's actually comforting that like there's precedent for this new disease humans have never had like COVID.

00:08:03.680 --> 00:08:07.680
 And then it's just kind of around and it's not going to just suddenly morph into something that kills us all.

00:08:07.680 --> 00:08:09.680
 It sounds like the flu didn't do that.

00:08:09.680 --> 00:08:17.680
 Well, I think there's like funny evolutionary pressure with diseases because the most optimal disease.

00:08:17.680 --> 00:08:25.680
 It doesn't kill the host. It like keeps the host alive so the host can infect as many other people on everybody.

00:08:25.680 --> 00:08:32.680
 Yeah. So there's a pressure to like not be as lethal, but then I think in theory, it could still be lethal.

00:08:32.680 --> 00:08:37.680
 It could be effective was lethal after like a month, right? So that's why that makes it so scary.

00:08:37.680 --> 00:08:42.680
 But I think that's why COVID rapidly like became more about infecting people than being lethal.

00:08:42.680 --> 00:08:51.680
 Yeah. I heard this back in the COVID, like when the 2020, 2021 days, I remember somebody talking about like a disease that just kills you immediately, like doesn't get to spread.

00:08:51.680 --> 00:08:56.680
 So it's not that actually dangerous. Yeah. Well, to like a population. Yeah, that makes sense.

00:08:56.680 --> 00:09:03.680
 But yeah, what about the one that's like a time release kills you after two weeks exactly after you've infected everybody who that's scary.

00:09:03.680 --> 00:09:09.680
 Let's not let's not create any of that in like a lab somewhere. That sounds bad.

00:09:09.680 --> 00:09:18.680
 Yeah. What's the deal with that? Could you actually tell me? Because I didn't keep up all that well, but I know like everyone says people lied and it actually was made in a lab.

00:09:18.680 --> 00:09:23.680
 Is that true now? That's a fact. I mean, we're never going to know. I think somebody out there knows for sure.

00:09:23.680 --> 00:09:28.680
 It's like someone in the Chinese government knows, but like I doubt we're ever going to know anytime soon.

00:09:28.680 --> 00:09:31.680
 Okay. So it's just not established fact as Twitter made me believe.

00:09:31.680 --> 00:09:40.680
 It was just so funny because when COVID was happening and John Stewart has a really funny bit about this where he goes in the Colbert show and he just keeps repeating.

00:09:40.680 --> 00:09:48.680
 The lab in Wuhan was called the novel coronavirus lab. Do you think it somehow related?

00:09:48.680 --> 00:09:54.680
 Because it goes on like a five minute. Was it like just really close to where the first case was or something?

00:09:54.680 --> 00:10:00.680
 It was literally in the city and then people were like, no, the labs there because it's coronavirus is there and they're there to study them.

00:10:00.680 --> 00:10:12.680
 And then it's just like there was a time where saying that, hey, maybe it was leaked out of the lab by accident was like this weird political landmine and then you're like, you weren't allowed to digest that.

00:10:12.680 --> 00:10:22.680
 Not saying that like they engineered it on purpose and released or anything just yeah, you can study diseases and they can get released by accident and like an Occam's razor thing is like.

00:10:22.680 --> 00:10:30.680
 The simple as the Corona virus left. Where does fires come from? Maybe we should check there first.

00:10:30.680 --> 00:10:44.680
 Yeah, the hard thing for me about these debates are about like hot topics like that is I can't follow the discourse like I just can't get in the headspace of either side and I lose the arguments or the same way was computer conspiracy theories like I can't ever like keep track.

00:10:44.680 --> 00:10:51.680
 Okay, so why do you think they did that? What was their motivation? Like I can't keep it all straight in my head and then I just lose the train of thought and I'm out.

00:10:51.680 --> 00:10:56.680
 It's like I'm not smart enough to be a conspiracy theorist. I feel like you have to be pretty smart.

00:10:56.680 --> 00:11:08.680
 Well, it's funny you bring this up because I had this, I was doing a bunch of self reflection yesterday and I had this thing I'm going to try where I'm interested in stuff like I'm interested in stuff that's going on me to the world.

00:11:08.680 --> 00:11:10.680
 Oh, okay. Yeah.

00:11:10.680 --> 00:11:18.680
 I'm sure you're interested in that too. I'm sure most people are things have just gotten to a really bad place. I think when you're trying to learn about the world.

00:11:18.680 --> 00:11:31.680
 Yes, I feel this. You have these two options, right? So yesterday, I heard that there were some like explosions in Israel and I was curious. Okay, what what's going on like is another attack.

00:11:31.680 --> 00:11:45.680
 So your options are Google it and you land on like the worst written article ever where there's like the site is garbage. It doesn't load the information you want is like one sentence and like, and then an ad and then.

00:11:45.680 --> 00:11:49.680
 Yeah, it's just like, it's just so hard to parse it there.

00:11:49.680 --> 00:12:00.680
 Twitter will have the information pretty clear, but it's really hard to avoid the crazy extreme stuff in the comments where everyone is just saying like the most batshit insane thing.

00:12:00.680 --> 00:12:03.680
 And that just like is a drag mental drag on you.

00:12:03.680 --> 00:12:16.680
 And I realized, hey, I think I just want to experience a world through AI because AI solved this perfectly where when I as a rock. Hey, what's going on here? It tells me exactly what I need to know. There's no garbage in there.

00:12:16.680 --> 00:12:29.680
 It ignores all the extreme political stuff that that shows up everywhere. And I'm like, I think maybe AI came out at the perfect time because I think just taking the raw feed of the Internet.

00:12:29.680 --> 00:12:34.680
 It just got to a point where I don't think I can deal with it anymore. It's too much.

00:12:34.680 --> 00:12:45.680
 Yeah, it's too much. I'm not able to sift through it. I feel very vulnerable to like being swayed or just like believing things that I thought were just supposed to believe and that wasn't actually true.

00:12:45.680 --> 00:12:47.680
 Like, I just don't know what's real anymore.

00:12:47.680 --> 00:12:50.680
 It's funny. The Groc 3 thing like Groc 3 just came out.

00:12:50.680 --> 00:12:55.680
 So you use Groc like for your daily questions about the world. It makes sense. It's connected to Twitter, right?

00:12:55.680 --> 00:13:01.680
 Yeah, it makes it like 10 times better than any other LM because I can ask it about what's going on today.

00:13:01.680 --> 00:13:06.680
 Yeah, yeah. So Groc 3 is a good example where it came out and it's like some people are like, it's the greatest thing ever.

00:13:06.680 --> 00:13:11.680
 And so people are like, this is the most dog shit model that's ever been put in the world. And I don't know what's true.

00:13:11.680 --> 00:13:17.680
 Yeah. It's everything that happens is like that. It's like there are people on extreme ends and they're the loudest and they get the most reach.

00:13:17.680 --> 00:13:19.680
 And that's the thing that ends up in your feed.

00:13:19.680 --> 00:13:38.680
 Yeah. And Jay was bringing this up yesterday. He was like, you know, for us as SST, we're in, you know, we have a company and being able to parse what's going on in AI and like navigate that is very critical for like our future as a company.

00:13:38.680 --> 00:13:55.680
 Like we really need to like have a good understanding and get these as a make good bets to like make it through to the other side. And he was like, I think we need to not consume anything in like the public discourse about it because it's just such ridiculous

00:13:55.680 --> 00:14:06.680
 insane noise and you like can it's very hard to have a clear head and like a clear vision of what's going to happen when you just have all this crazy stuff being talked about all day.

00:14:06.680 --> 00:14:20.680
 So yeah, I'm trying to like cut back on that. I'm trying to like only again this whole thing of right only thing that I've talked about before like I want to like try to really focus on making sure I'm not taking in that energy.

00:14:20.680 --> 00:14:29.680
 Yeah. Yeah. No, I feel that and I'm not doing a good job. I think I'm still bringing in that energy. I don't need it. It's really, really hard.

00:14:29.680 --> 00:14:37.680
 It's it becomes like you're, yeah, you get stuck on a problem or whatever you just you want to numb for five minutes and like you have your go to place.

00:14:37.680 --> 00:14:48.680
 Some people that's happiness. I don't understand it, but for some people it is. Some people's read it. Some people's Twitter and you're inevitably getting fed a lot of stuff, I guess, at this stage.

00:14:48.680 --> 00:14:53.680
 Yeah. And I think, like I said, I think there's a lot of potential for AI to help fix this.

00:14:53.680 --> 00:15:07.680
 And yeah, it made a funny point the other day where I was talking about how sometimes I'll suddenly feel like, Hey, for the past couple weeks, like going through Twitter has been making me feel really negative.

00:15:07.680 --> 00:15:22.680
 I won't realize it right away. It'll take a couple weeks. And then I'll realize, Oh, I followed this person a month ago, and they're not explicitly doing anything wrong, but they're like reposting stuff or like somehow injecting stuff into my feed that

00:15:22.680 --> 00:15:33.680
 technically making me feel that way. And it always takes me a little while to notice. And when I notice I like tracked down the source. Okay, this is this is what's going on. This is why I'm feeling this way. And I'll like, cut the source out.

00:15:33.680 --> 00:15:50.680
 And then Jay made the point was like, yeah, I bet that happens all the time due to some algorithm change that in this case is something I did I controlled it so I can undo it. Yeah, theoretically, there's some algorithm change that could happen somewhere that now suddenly is like making me feel a certain way.

00:15:50.680 --> 00:16:06.680
 And that must happen a lot of time. It might be hard to deal with. So I think there's an opportunity to make some kind of product that I still want to take information. I still want to scroll. I still want to like, you know, consume stuff that's going on. Like, I'm not trying to cut all that out.

00:16:06.680 --> 00:16:18.680
 I'm not some people are. That's not where I'm trying to go. Yeah, but give it to me through like this cold neutral AI that is pretty boring in a lot of ways. Run it through that.

00:16:18.680 --> 00:16:30.680
 And I can kind of see how that could be reliable. Can you just, can you just ask an LLM like, or ask Rock, like, give me the news? Like, what's going on? You can, yeah. And it does, like, give you the top. Okay.

00:16:30.680 --> 00:16:46.680
 Yeah. So that's like a great starting point. And I think that's what I'm going to try to like build some habits around. But I can see how there's some kind of opportunity for like a new kind of product that doesn't involve other people at all. It feels like social media feels like something.

00:16:46.680 --> 00:17:05.680
 But yeah, it's not generated by other people. But okay. Now I have questions. Like, do you imagine this thing, like having comments that are not from human? Yeah. Yeah. Like Reddit, but like AI in the all the actors are AI.

00:17:05.680 --> 00:17:13.680
 Yeah. And you have to be like, you know, if you want to go that far, you'd have to be pretty clever about how to like make that interesting and engaging like generically.

00:17:13.680 --> 00:17:24.680
 I think, hey, that does a great job of summarizing. Hey, Microsoft put out this new quantum CPU. Here's the deal. Here's the criticisms. Like, here's why it might be great. Here's why it might not be.

00:17:24.680 --> 00:17:33.680
 I think it does a fantastic job of this coldly. Yeah, handling that and taking a step further to have like something that seems a bit more human. I think is harder, but I can see how I do that.

00:17:33.680 --> 00:17:49.680
 It is interesting just to think just something as simple as the Reddit model where like AI agent, sorry, AI models or whatever, little people that are actually AI, they have their own little account could actually upvote stuff and actually comment on stuff.

00:17:49.680 --> 00:18:04.680
 Like, what would what would that result in? Like, yeah, yeah, like, because the LMM output is very like vanilla down the middle, but if you took like a whole bunch of it and you let them all interact like that with interesting novel stuff actually come out of it.

00:18:04.680 --> 00:18:21.680
 Yeah, like there's definitely like tropes of people that like there's all the negative ones, but you can create a version of the AI that tends to lean very libertarian or you can make one that then Celine, like, you know, very socialist and whatever and you could like have them

00:18:21.680 --> 00:18:37.680
 interact engage in something within parameters of like not being too extreme and like actually understanding what the other person saying before responding. Okay, I'm sold. Maybe is the answer. It solves all this information overload and just biases and all these weird things that are going on the world.

00:18:37.680 --> 00:18:53.680
 Yeah. Yeah. So I think that there's something there and to be a consumer product, which is great. And especially now there's just so much appetite to try anything. Yeah, I related. So someone's interested in building something. I think there's something to explore in this category.

00:18:53.680 --> 00:19:10.680
 I'd like, I feel like I want, I want this. We'll build it and we'll build it in the terminal now. Great building the terminal. Perfect. So I want to talk now that you've just said that like you're trying not to consume all the public discourse.

00:19:10.680 --> 00:19:26.680
 And some public discourse. But before we do that. So that'll be dessert. We can eat a little meat. Have you not meat, but whatever. Go into Korean barbecue for lunch today. I'm sure you are. Yeah.

00:19:26.680 --> 00:19:42.680
 I'd like to actually eat out tonight. I've been trying for like a week to get us to order. We just don't eat out ever. And there's like a restaurant in Nixa that is like, they're an Italian restaurant and they have a whole vegan menu, which is super rare in the Ozarks for there to be a restaurant with a vegan menu.

00:19:42.680 --> 00:19:58.680
 It's a husband wife and the wife is vegan and they have like a whole vegan menu. It's amazing. Anyway, shout out to Piccolo's. Did you see the Satya Satya? Is that the quantum thing or the thing where you were trying to wind back the hype?

00:19:58.680 --> 00:20:11.680
 But I'd also like to hear about the quantum thing. That sounds exciting. You go first. I guess did you hear his comments? Yeah. You know where I heard about it? I was trying this rock thing and that's where I heard about it.

00:20:11.680 --> 00:20:25.680
 Nice. Okay. Well, you probably know more than me. I've just heard like a reaction to I haven't heard him in the actual statement. Just kind of like, I've heard what he generally said. But basically like kind of calming down.

00:20:25.680 --> 00:20:40.680
 And just, he said things that just like resonated as a person who's also tired of people like Sama. I don't know if he's actually tired of Sama, but what's there something he said about like pulling back on the OpenAI partnership?

00:20:40.680 --> 00:20:52.680
 Or was that just a reaction stuff? Okay. So I think there's like a lot of weird dynamics here where OpenAI has a weird deal with Microsoft where if they achieve AGI.

00:20:52.680 --> 00:21:02.680
 They're like not that they kind of release Microsoft's ownership. It's something weird like that. And a lot of his comments were about like how much he hates this term AGI and like.

00:21:02.680 --> 00:21:14.680
 Yeah. So I kind of wonder if that's related where he's like trying to change the definition, not change. I think his point of view is correct. Like why do we, we had AI and we're like.

00:21:14.680 --> 00:21:24.680
 But then we like assign that label to something that was pretty primitive. So we're like, okay, now we need AGI. Cause like we already used AI. Oh shit. But now we assign AGI to something too primitive.

00:21:24.680 --> 00:21:38.680
 Now we need ASI. And that's like the next level. So just like stop stop doing this. Yeah, he's saying like let's call it 10% gross. Like the world is growing economically at 10% rates. Like let's call that that could be AGI. Yeah.

00:21:38.680 --> 00:21:47.680
 Basically, is that what you think? Yeah. Yeah. And that. And so I actually just posted something before I got on and kind of really I'm just going to read it out loud. Yeah.

00:21:47.680 --> 00:21:59.680
 So I wrote, I'm not sure if the highly visible AI companies we see online a representative. We see a lot of activity in unheard niches integrating AI to handle very specific business flows.

00:21:59.680 --> 00:22:08.680
 I thought there's going to be 10,000 of these companies each doing 10 million era. We're obviously biased because we only see like whatever segment of the market we focus on.

00:22:08.680 --> 00:22:18.680
 But there are a lot of AI companies that are using SST and every single one of them is identical. They're in some weird space.

00:22:18.680 --> 00:22:28.680
 I hung out with a guy yesterday and he's like in the ERP space focusing on manufacturers in South America. Okay. Like you never hear about this.

00:22:28.680 --> 00:22:41.680
 And he's building a and like his father has been in the space for 40 years and now they're building technically AI based tooling that is getting into these businesses that.

00:22:41.680 --> 00:22:48.680
 And now we can kind of solve these problems that you know you couldn't really do before with software like stuff that I got kind of unlocks.

00:22:48.680 --> 00:22:55.680
 Yeah. But they are just if you look at them, they're just a standard software company. There's nothing that crazy new about them.

00:22:55.680 --> 00:23:05.680
 90% of their work is AWS setting up glue jobs, getting a lot of data in one place. Yeah. And then like AI unlocks a bunch of things for the reasons they exist.

00:23:05.680 --> 00:23:12.680
 But there's like 10 years of work for them. Yeah. And there's like 10 years of work ahead of them for them to do to get in all these little places and then continue this product.

00:23:12.680 --> 00:23:20.680
 And I think I keep bringing up Jay, but he had another good point here too, where he was like, we had this whole software in the world concept.

00:23:20.680 --> 00:23:30.680
 But it didn't really go that far. And what we're seeing with AI might just be a continuation of software continuing to get into more places where it still hasn't gotten into.

00:23:30.680 --> 00:23:37.680
 And I think we're very aligned with what's what you're saying, which is that might just be it's a very boring outcome.

00:23:37.680 --> 00:23:49.680
 But 10% year over your growth is insane. And it might be involved with again 10,000 of these companies getting into all these little places and covering the last set of.

00:23:49.680 --> 00:23:52.680
 Possible to automate things that we haven't automated yet.

00:23:52.680 --> 00:23:58.680
 And it seems more and more likely that the quote unquote boring outcome is maybe what's more realistic.

00:23:58.680 --> 00:24:03.680
 Yeah, I loved you've tweeted something about this. I think the last couple of weeks, just the idea of like building apps.

00:24:03.680 --> 00:24:15.680
 So like enough of the shovels and we're just so plugged into the whole like dev tools scene, I guess, on Twitter that it does feel like that's the big focus for the people that we're around.

00:24:15.680 --> 00:24:22.680
 I love the idea of like this technology enables all kinds of new products go build those products like end user products.

00:24:22.680 --> 00:24:29.680
 Yeah, I mean, that was like, when we started Stat Muse, it was like, Hey, what, like all this NLP tech exists.

00:24:29.680 --> 00:24:33.680
 And this is like old school, you know, nothing like what you could do today.

00:24:33.680 --> 00:24:38.680
 But all this like all these tools exist. And there's just no consumer products that are like using an LP.

00:24:38.680 --> 00:24:45.680
 It was like Wolfram Alpha. And that was it. And it was like, why can't why doesn't this exist for like looking up my favorite sports player stats.

00:24:45.680 --> 00:24:50.680
 So that's very much always been my thesis of like, use the stuff that's available.

00:24:50.680 --> 00:24:57.680
 And so often we think we have to build new tools when like there's plenty of tools to build cool applications.

00:24:57.680 --> 00:25:01.680
 It's way more fun, I think. Yeah, it's way more fun.

00:25:01.680 --> 00:25:09.680
 When you're in a moment where everyone's trying to build shovels, it's like 10 times more fun because every single week, there's a new show.

00:25:09.680 --> 00:25:21.680
 It was like, Oh, suddenly your models are now 10 times cheaper. Suddenly there's like a better, you know, it's just, it's so much better to be in the receiving end versus trying to compete in this crazy as noisy place, right?

00:25:21.680 --> 00:25:32.680
 And I think the final thing is, is I think everyone, if people love, people love this stupid quote, which is when there's a gold rush, build shovels like, yeah, just look in history.

00:25:32.680 --> 00:25:39.680
 The winners are the people that use the shovels. They're not the people that built the shovel and tell is about to die, right?

00:25:39.680 --> 00:25:50.680
 Apple built their company on top of Intel. And then they eventually just escaped from Intel because they own the end value of the thing that they built with the end and user, right?

00:25:50.680 --> 00:26:02.680
 It's always better to be there. And people just keep trying to build these shovels. And what bothers me is, we're so early, we're like, nobody really knows a question of, what are good ways to use AI?

00:26:02.680 --> 00:26:08.680
 What are good architectures to use AI? What kind of product features does this make sense with? Like, we haven't discovered any of that yet.

00:26:08.680 --> 00:26:16.680
 And then there's already people writing stuff about, here's how you should build AI stuff. And they don't, that's not their product. Their product is something unrelated.

00:26:16.680 --> 00:26:24.680
 Like, how can you possibly know, like, go build the end product? Then you'll know what shovels are needed. Five years from now, yeah, we'll look back in hindsight.

00:26:24.680 --> 00:26:31.680
 Oh, like, this is a common thing. Let's like, you know, make a standalone service for that. But yeah, the ordering of this stuff just makes no sense to me.

00:26:31.680 --> 00:26:40.680
 Yeah, I love that tweet you had talking about like AWS came after Amazon, like all the different examples of the big shovels coming after the products.

00:26:40.680 --> 00:26:51.680
 Yeah, you develop expertise first and you can't get expertise without shipping something. Other things that you said, the infrastructure, like, basically, obviously, we're going to build way too much.

00:26:51.680 --> 00:26:59.680
 We're going to invest way too much. CapEx. And he's excited to lease from all the people that build too much infrastructure.

00:26:59.680 --> 00:27:10.680
 Yeah, I mean, that's a basic rule of it's impossible to invest the correct amount. You can only over invest. If you want to invest, you lose.

00:27:10.680 --> 00:27:21.680
 It's unlikely you're going to precisely invest the current correct amount. So the only result is over investing. And then when the froth ends, then there's some interesting stuff.

00:27:21.680 --> 00:27:34.680
 I was kind of thinking this other day. I was like, you know, everyone likes to say they're contrarian and they make contrarian bets. But I think the craziest contrarian bet you can make is to build a business that needs GPU capabilities.

00:27:34.680 --> 00:27:48.680
 But it's too expensive currently betting that in five years, all this GPU stuff is going to be on the market for crazy cheap. Like, is there some kind of business that can really take advantage of all that stuff?

00:27:48.680 --> 00:27:55.680
 You know, having to have to be sold off when some of these, you know, there's only going to be a couple winners, but losers are going to have to liquidate.

00:27:55.680 --> 00:28:00.680
 Can I make a really dumb comment or observation? Yeah.

00:28:00.680 --> 00:28:16.680
 Like, we're sure the GPU thing is the right way to do this, right? Like, I don't know. It's just, it's so weird to me, like, that a thing that was made for like rendering video games, then turned out to be really good at like this AI training

00:28:16.680 --> 00:28:27.680
 and just like the serendipity of that and then like thinking how many bazillions of dollars are going to be put into building these GPUs and these huge data centers full of these GPUs.

00:28:27.680 --> 00:28:34.680
 And then what if somebody's like, Oh, actually, if we just do this whole thing, it's the way better. And like, it looks nothing like possible.

00:28:34.680 --> 00:28:50.680
 And it's kind of like, if you look at, okay, so in favor or kind of against what you're saying, a GPU, yes was used for graphics, but if you think about what it is, it's just picking an opposite set of trade offs from a CPU.

00:28:50.680 --> 00:28:59.680
 The CPU is about having really, really fast performance, but not a lot of parallel work, right? You were trying to get the fastest single thread thing.

00:28:59.680 --> 00:29:05.680
 If you use like, okay, what if we flip the requirements and said, it's okay if each core is in that fast, we just need to do a lot in parallel.

00:29:05.680 --> 00:29:06.680
 Parallel. Okay.

00:29:06.680 --> 00:29:12.680
 It happened to be that graphics was a great entry point into that, but then parallel computing in general is useful everywhere.

00:29:12.680 --> 00:29:17.680
 I see. This is why I said it was a dumb comic because I knew I don't know anything about any of this.

00:29:17.680 --> 00:29:23.680
 It just feels like a huge amount of money and time and effort are going to be put into building these huge data centers.

00:29:23.680 --> 00:29:29.680
 And it just feels like it's hinging on some very, I don't know, assumptions.

00:29:29.680 --> 00:29:32.680
 Yeah, some solid assumptions around GPUs.

00:29:32.680 --> 00:29:38.680
 I think the part where you're right is why NVIDIA and why NVIDIA GPUs?

00:29:38.680 --> 00:29:44.680
 And that's not really related to the hardware. It's more related to the software stack on top vibes.

00:29:44.680 --> 00:29:46.680
 People like using CUDA.

00:29:46.680 --> 00:29:54.680
 There's not much of an alternative. Everyone kind of under invested in the CUDA concept and so NVIDIA is winning.

00:29:54.680 --> 00:30:02.680
 But yeah, like conceptually, I don't see why that gap is enclosed given how much money is on the table.

00:30:02.680 --> 00:30:10.680
 Yeah, so I know the answer that we're still decades or whatever away from this, but what about quantum?

00:30:10.680 --> 00:30:13.680
 Because I feel like that's the thing that Microsoft announced yesterday.

00:30:13.680 --> 00:30:18.680
 Yeah, yeah, there's constantly been headlines that like it feels like we're actually making progress in the last year

00:30:18.680 --> 00:30:21.680
 on things that people thought were going to be decades away.

00:30:21.680 --> 00:30:27.680
 So like what happens when quantum comes along and it's way better at AI training and inference and like...

00:30:27.680 --> 00:30:28.680
 No, it's possible.

00:30:28.680 --> 00:30:29.680
 Oops.

00:30:29.680 --> 00:30:34.680
 But maybe that's 20 years from now and it's like you got to do this in the meantime anyway.

00:30:34.680 --> 00:30:35.680
 I don't know.

00:30:35.680 --> 00:30:36.680
 Yeah, it probably is.

00:30:36.680 --> 00:30:44.680
 Yeah, I think it might end up being... I'm trying to think of comparisons where something like this happened,

00:30:44.680 --> 00:30:49.680
 where there was like investing in something and then something kind of came out that made it useless.

00:30:49.680 --> 00:30:55.680
 I'm sure it's happened before, but I feel like transitions are always slower than we'd expect.

00:30:55.680 --> 00:30:56.680
 Yeah.

00:30:56.680 --> 00:30:57.680
 Yeah.

00:30:57.680 --> 00:31:00.680
 Well, it was the announcement though. What did Microsoft announced?

00:31:00.680 --> 00:31:06.680
 They built a chip which has what they're saying is a new form of matter or put with a new form of matter.

00:31:06.680 --> 00:31:08.680
 Whoa, I'm hooked. I'm in.

00:31:08.680 --> 00:31:09.680
 You got me.

00:31:09.680 --> 00:31:17.680
 And they're saying they've achieved things with it that are a little bit further ahead than they expected.

00:31:17.680 --> 00:31:20.680
 I read a bunch of the criticisms.

00:31:20.680 --> 00:31:29.680
 It ranges to this is nothing to like, oh, this is good progress, but I don't think it's going to like really lead to the next step in the way that I think it will.

00:31:29.680 --> 00:31:37.680
 But from Microsoft's point of view, they're like, this is the next big step and it makes our vision of enabling this like possible in the next five years.

00:31:37.680 --> 00:31:39.680
 That's kind of how they're pitching it.

00:31:39.680 --> 00:31:41.680
 But Microsoft, man.

00:31:41.680 --> 00:31:46.680
 That does seem like... Yeah, how are they... It's amazing how relevant Microsoft had remained.

00:31:46.680 --> 00:31:51.680
 When it's like it seemed like they were irrelevant 20 years ago and then they've just been more and more...

00:31:51.680 --> 00:31:52.680
 Even 10 years ago.

00:31:52.680 --> 00:31:53.680
 Yeah.

00:31:53.680 --> 00:31:54.680
 Yeah.

00:31:54.680 --> 00:31:55.680
 Turned it all around.

00:31:55.680 --> 00:31:56.680
 Amazing.

00:31:56.680 --> 00:31:57.680
 Pretty crazy.

00:31:57.680 --> 00:32:03.680
 Yeah, I do think like all the headlines I've seen with quantum stuff seems like in like 10 years, we're going to have something.

00:32:03.680 --> 00:32:08.680
 There's going to be like actual products that are quantum powered.

00:32:08.680 --> 00:32:09.680
 Yeah.

00:32:09.680 --> 00:32:10.680
 This is an area I need to like look.

00:32:10.680 --> 00:32:12.680
 I need to understand better.

00:32:12.680 --> 00:32:20.680
 I understand some of it because one of my friends took a quantum computing course in college and like tell me all about what he's learning.

00:32:20.680 --> 00:32:22.680
 I know the word cubit.

00:32:22.680 --> 00:32:23.680
 That's all I know.

00:32:23.680 --> 00:32:26.680
 I don't understand any of it.

00:32:26.680 --> 00:32:27.680
 Yeah.

00:32:27.680 --> 00:32:30.680
 It just confuses me because I'm like, how is this in the chip?

00:32:30.680 --> 00:32:32.680
 Like they were showing pictures of it and they were like...

00:32:32.680 --> 00:32:33.680
 Yeah, I don't know.

00:32:33.680 --> 00:32:34.680
 Yeah.

00:32:34.680 --> 00:32:36.680
 Quantum computing is in here and I'm like, but like how though?

00:32:36.680 --> 00:32:38.680
 How is it in there?

00:32:38.680 --> 00:32:41.680
 Yeah, it's very confusing.

00:32:41.680 --> 00:32:45.680
 Lots of fun, exciting like things happening in technology.

00:32:45.680 --> 00:32:48.680
 When you think about like the fission, fusion, fusion.

00:32:48.680 --> 00:32:49.680
 Fusion, yeah.

00:32:49.680 --> 00:32:51.680
 Fusions is, that's the hard one.

00:32:51.680 --> 00:32:53.680
 Like there's been huge announcements on the fusion side.

00:32:53.680 --> 00:32:59.680
 And I feel like all these big progress like news headlines with quantum and with fusion and AI.

00:32:59.680 --> 00:33:02.680
 It's all converging and then AI is just going to help us like get over the hump.

00:33:02.680 --> 00:33:03.680
 Yeah.

00:33:03.680 --> 00:33:04.680
 Awesome.

00:33:04.680 --> 00:33:06.680
 Well, AI did something in the research setting like it helped somebody.

00:33:06.680 --> 00:33:08.680
 This was a Google paper, right?

00:33:08.680 --> 00:33:11.680
 When I saw the headline, I was like, whatever, but then it was like, oh, it's from Google.

00:33:11.680 --> 00:33:15.680
 There was some like thing that helped them develop a something medicine.

00:33:15.680 --> 00:33:16.680
 Yeah.

00:33:16.680 --> 00:33:19.680
 I think there's been a bunch of stuff in research.

00:33:19.680 --> 00:33:23.680
 Yeah, that just lets flashy, but more just, there's a bunch of tedious work.

00:33:23.680 --> 00:33:27.680
 Again, kind of what I was saying earlier, there's been a bunch of tedious work that was hard

00:33:27.680 --> 00:33:29.680
 to automate before the now has been automated.

00:33:29.680 --> 00:33:33.680
 It's funny because they were like, some more from Google, I think trying to like do this

00:33:33.680 --> 00:33:38.680
 whole hype thing that, you know, Sam Altman's doing, he was like, oh, yeah, with DeepMind,

00:33:38.680 --> 00:33:43.680
 we've done like 2 billion PhD hours of research already.

00:33:43.680 --> 00:33:46.680
 There's some, I'm making them or some like ridiculous number like that.

00:33:46.680 --> 00:33:49.680
 And I was like, oh, that's really crazy.

00:33:49.680 --> 00:33:54.680
 And I was talking to Liz and because she used to work in research and she used to run like a,

00:33:54.680 --> 00:33:55.680
 I guess a lab.

00:33:55.680 --> 00:33:59.680
 And then she was like, this wasn't related to this topic, but she was describing something

00:33:59.680 --> 00:34:04.680
 about some of the work that they were doing back then.

00:34:04.680 --> 00:34:10.680
 And you realize what 2 billion hours of PhD research means is they were like doing some

00:34:10.680 --> 00:34:15.680
 research on like analyzing social media and like correlating with other stuff.

00:34:15.680 --> 00:34:20.680
 They were mainly going through every single host and like checking off a bunch of things.

00:34:20.680 --> 00:34:22.680
 Yeah, that's perfectly sitting for AI.

00:34:22.680 --> 00:34:23.680
 Yeah, exactly.

00:34:23.680 --> 00:34:25.680
 So it's not like, oh, like a genius.

00:34:25.680 --> 00:34:27.680
 It's not a genius research.

00:34:27.680 --> 00:34:30.680
 It's more just, they're just tedious work that exists.

00:34:30.680 --> 00:34:32.680
 And it definitely super useful.

00:34:32.680 --> 00:34:36.680
 But when they say like PhD level research, they're trying to imply that it's really smart,

00:34:36.680 --> 00:34:39.680
 but it's probably just stuff like that.

00:34:39.680 --> 00:34:41.680
 Interesting.

00:34:41.680 --> 00:34:44.680
 Okay, if I have to pee, do we need to just go?

00:34:44.680 --> 00:34:45.680
 I probably need to leave in a couple of minutes.

00:34:45.680 --> 00:34:46.680
 But what was your dessert?

00:34:46.680 --> 00:34:47.680
 You said we had dessert?

00:34:47.680 --> 00:34:48.680
 Well, the dessert.

00:34:48.680 --> 00:34:49.680
 Yeah, I don't want to leave everybody hanging.

00:34:49.680 --> 00:34:52.680
 I just feel like the world is crazy right now with all this stuff.

00:34:52.680 --> 00:34:53.680
 What is the deal?

00:34:53.680 --> 00:34:57.680
 The US and Russia are like buddies and like Elon and all this stuff is so bizarre.

00:34:57.680 --> 00:35:00.680
 I don't know what to think about anything anymore.

00:35:00.680 --> 00:35:05.680
 That's actually what triggered myself reflection yesterday because I was being something from

00:35:05.680 --> 00:35:12.680
 Elon being like, oh, all the community notes about on me about the stuff I said about Ukraine

00:35:12.680 --> 00:35:13.680
 are wrong.

00:35:13.680 --> 00:35:15.680
 And so we're going to fix the community notes thing.

00:35:15.680 --> 00:35:16.680
 I got it.

00:35:16.680 --> 00:35:17.680
 What is going on?

00:35:17.680 --> 00:35:24.800
 Like, why we are trying to help Russia effectively, we're like taking the Russian angle on this

00:35:24.800 --> 00:35:27.440
 situation, weird on its own.

00:35:27.440 --> 00:35:28.440
 Weird.

00:35:28.440 --> 00:35:29.440
 Yeah.

00:35:29.440 --> 00:35:30.440
 Why is it affecting my social media experience?

00:35:30.440 --> 00:35:31.440
 Exactly.

00:35:31.440 --> 00:35:32.440
 Why is the social media changing?

00:35:32.440 --> 00:35:33.440
 Because of this.

00:35:33.440 --> 00:35:34.440
 It's so bizarre.

00:35:34.440 --> 00:35:43.080
 I'm like, I think I'm very, I can deal with a lot, I think, but I really think the world

00:35:43.080 --> 00:35:45.600
 is guy and too weird for even me.

00:35:45.600 --> 00:35:47.200
 It's too weird.

00:35:47.200 --> 00:35:55.520
 It feels like it really feels like there's this dynamic now where if your enemy politically

00:35:55.520 --> 00:35:59.520
 in the US believes something, you have to believe the opposite and you have to like go

00:35:59.520 --> 00:36:01.240
 very hard that direction.

00:36:01.240 --> 00:36:08.080
 Like there is no letting some things just be like, I don't know, like that side cares

00:36:08.080 --> 00:36:09.080
 about it, but I don't care.

00:36:09.080 --> 00:36:10.080
 You have to care.

00:36:10.080 --> 00:36:11.080
 You have to care hard.

00:36:11.080 --> 00:36:12.080
 Yeah.

00:36:12.080 --> 00:36:16.280
 And that feels like this whole Russia thing is like, oh, they like Zelensky, well, we

00:36:16.280 --> 00:36:17.280
 love Russia.

00:36:17.280 --> 00:36:18.280
 We love things.

00:36:18.280 --> 00:36:19.280
 They're the best.

00:36:19.280 --> 00:36:20.280
 What in the world?

00:36:20.280 --> 00:36:21.280
 Oh, wait.

00:36:21.280 --> 00:36:22.280
 Because some of the exhausts the aisle.

00:36:22.280 --> 00:36:23.280
 I think the exhausts the aisle.

00:36:23.280 --> 00:36:24.280
 We did that thing yesterday.

00:36:24.280 --> 00:36:25.280
 Yes.

00:36:25.280 --> 00:36:26.280
 I'm going crazy.

00:36:26.280 --> 00:36:27.280
 Okay.

00:36:27.280 --> 00:36:28.280
 I'm realizing that this all triggered it.

00:36:28.280 --> 00:36:29.280
 I saw your post for that.

00:36:29.280 --> 00:36:30.280
 I'm like, what's Adam talking about?

00:36:30.280 --> 00:36:34.680
 And then I looked at the Explorer tab and then I was like, oh, this is probably what he's

00:36:34.680 --> 00:36:35.680
 talking about.

00:36:35.680 --> 00:36:38.000
 And then I had this whole like, yeah, no, I've been on the four U tab because my following

00:36:38.000 --> 00:36:44.800
 tab's terrible because like I follow people randomly, but the four U tab is something right

00:36:44.800 --> 00:36:45.800
 now.

00:36:45.800 --> 00:36:46.800
 It's something.

00:36:46.800 --> 00:36:49.280
 It's just the bizarro timeline.

00:36:49.280 --> 00:36:51.280
 I didn't know we were headed towards, I guess.

00:36:51.280 --> 00:36:58.240
 It's so weird to end in like the president of Argentina did a crypto rug pull like last

00:36:58.240 --> 00:36:59.240
 week.

00:36:59.240 --> 00:37:01.640
 What is going on?

00:37:01.640 --> 00:37:02.640
 Yeah.

00:37:02.640 --> 00:37:06.200
 And then it's like, then Elon is like, we're going to give every citizen $5,000.

00:37:06.200 --> 00:37:08.600
 I'm like, what the?

00:37:08.600 --> 00:37:09.600
 What?

00:37:09.600 --> 00:37:12.000
 It's just the whole thing is a little too much for me.

00:37:12.000 --> 00:37:16.720
 The whole Elon and Trump partnership is just, it's just not hidden in good places.

00:37:16.720 --> 00:37:19.760
 And I don't really have strong opinions about those people.

00:37:19.760 --> 00:37:22.040
 It just feels like too much for the world.

00:37:22.040 --> 00:37:23.040
 It's just, it's too much.

00:37:23.040 --> 00:37:29.360
 I think the way the one little bright spot I'm seeing is that I had a lot of people that

00:37:29.360 --> 00:37:34.440
 I like generally looked up to and was like, Oh, these are people that, you know, I like

00:37:34.440 --> 00:37:37.520
 the way they view things and, you know, I can learn from them.

00:37:37.520 --> 00:37:42.200
 Then I witnessed how whatever this thing is, it's happening where like there's like this

00:37:42.200 --> 00:37:47.440
 crazy political effectively, like virus that exists out there.

00:37:47.440 --> 00:37:49.120
 I see it.

00:37:49.120 --> 00:37:57.320
 I see these people I admire coming in contact with it and then just not being able to get

00:37:57.320 --> 00:37:58.320
 past it.

00:37:58.320 --> 00:38:02.040
 Like I remember, like I remember during this happened in like the other way.

00:38:02.040 --> 00:38:06.880
 So in during COVID, I remember there were people that, like COVID just broke them.

00:38:06.880 --> 00:38:13.800
 Like they just, they were just like, so scared of doing anything that was like going to like

00:38:13.800 --> 00:38:14.800
 get them sick.

00:38:14.800 --> 00:38:16.560
 And like they just couldn't escape it.

00:38:16.560 --> 00:38:19.040
 And they like never really recovered even after COVID.

00:38:19.040 --> 00:38:21.840
 I mean, I've seen many such cases.

00:38:21.840 --> 00:38:22.840
 Oh, sorry.

00:38:22.840 --> 00:38:23.840
 Yeah.

00:38:23.840 --> 00:38:24.840
 Yeah.

00:38:24.840 --> 00:38:25.840
 Many such cases.

00:38:25.840 --> 00:38:29.480
 And I see the same thing with politics where I'm like, this is like the most potent drug

00:38:29.480 --> 00:38:36.840
 ever created and it is like messing people up where they just cannot look at anything outside

00:38:36.840 --> 00:38:40.320
 of this lens and they obsess over every, every thing.

00:38:40.320 --> 00:38:43.120
 And I see so many people that I'm like, they're clearly smart.

00:38:43.120 --> 00:38:44.120
 They're clearly capable.

00:38:44.120 --> 00:38:46.240
 They've achieved things that I haven't achieved in my life.

00:38:46.240 --> 00:38:52.120
 Like they're like, they've done more than I have, but they couldn't beat this thing.

00:38:52.120 --> 00:38:54.800
 And there are people that have, like there are people that kind of like, you know, I admire

00:38:54.800 --> 00:38:59.800
 them and they like escaped it, but it just like narrowed the field of people that I'm like,

00:38:59.800 --> 00:39:01.040
 I can look up to them.

00:39:01.040 --> 00:39:04.320
 And I think that's kind of like a good thing is that kind of cleared out a bunch of noise.

00:39:04.320 --> 00:39:05.320
 Good.

00:39:05.320 --> 00:39:10.680
 But also like my, the, the thing that I would always fall back on when it felt like social

00:39:10.680 --> 00:39:13.920
 media just wears me out and I want to get off of it for a while, the thing I'd fall back

00:39:13.920 --> 00:39:16.000
 on is like, most people don't feel this way.

00:39:16.000 --> 00:39:17.000
 Most people are normal.

00:39:17.000 --> 00:39:18.000
 But I don't know.

00:39:18.000 --> 00:39:22.000
 Is it increasingly like, is it, is it a dwindling number of people that are normal?

00:39:22.000 --> 00:39:24.880
 Is this potent drug infecting more of the general population?

00:39:24.880 --> 00:39:27.360
 Is it becoming like, is this where we're headed?

00:39:27.360 --> 00:39:30.160
 Because I feel like I've not taken the pill or whatever.

00:39:30.160 --> 00:39:33.920
 I'm not, I don't feel politically charged, but is it just inevitable?

00:39:33.920 --> 00:39:34.920
 Are we all going to end up?

00:39:34.920 --> 00:39:38.960
 Am I going to be on here ranting about how great some terrible person is?

00:39:38.960 --> 00:39:39.960
 Some leader.

00:39:39.960 --> 00:39:45.440
 I mean, I look at Elon and I'm just like, I think about what his potential is.

00:39:45.440 --> 00:39:47.080
 And he just got hit with this thing.

00:39:47.080 --> 00:39:52.640
 And now he's just wasting his time in a bureaucracy, doing all this like really stupid stuff and

00:39:52.640 --> 00:39:58.520
 like, completely detached from reality at this point, where like, yeah, I'm sure he's

00:39:58.520 --> 00:40:05.000
 still working on everything, but whatever mission there was to get to Mars, it's going

00:40:05.000 --> 00:40:08.720
 to happen a little bit slower now, right, because he's like, derailed by this.

00:40:08.720 --> 00:40:13.280
 I'm like, this is, you know, first aside that has an Elon, we don't need him here.

00:40:13.280 --> 00:40:17.760
 We need him working on Mars, he's actually extremely bad here, and he's extremely good

00:40:17.760 --> 00:40:18.760
 at the other thing.

00:40:18.760 --> 00:40:25.240
 And yeah, it's just crazy, like, it's just, no matter how smart the person is, like, you

00:40:25.240 --> 00:40:29.800
 just never know who's going to be like, especially infected by it.

00:40:29.800 --> 00:40:33.440
 Yeah, I wish, I wish there was like a public chart, like where you could just see the number

00:40:33.440 --> 00:40:37.680
 of normal people and the number of people who have lost their loss to the user.

00:40:37.680 --> 00:40:38.680
 Yeah.

00:40:38.680 --> 00:40:40.880
 So I can know like how dire it is.

00:40:40.880 --> 00:40:42.880
 Anyway, I'll let you go lunch.

00:40:42.880 --> 00:40:46.280
 Enjoy your siesta or whatever.

00:40:46.280 --> 00:40:47.280
 All right.

00:40:47.280 --> 00:40:48.280
 You're Korean barbecue.

00:40:48.280 --> 00:40:49.280
 Yes.

00:40:49.280 --> 00:40:50.280
 All right.

00:40:50.280 --> 00:40:51.280
 All right.

00:40:51.280 --> 00:40:52.280
 See ya.

00:40:52.280 --> 00:40:53.280
 See ya.

00:40:53.280 --> 00:40:53.280
 Yeah.

