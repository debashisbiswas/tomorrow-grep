WEBVTT

00:00:00.000 --> 00:00:07.820
 We should put it at the beginning of our podcast. Oh probably so that everyone who hears about cuz there's probably like 10 people that listen to the end. Yeah

00:00:31.000 --> 00:00:38.000
 I don't know where I saw it. I think it was just like a YouTube thumbnail and I inferred it. But they put it up here like where I have it now.

00:00:38.000 --> 00:00:45.000
 And they do that thing where they take a picture of their background. It's so dark. I can't see your microphone. Oh, it's like a pie.

00:00:45.000 --> 00:00:53.000
 Yeah, so it's like to the side of my head. And they do that thing where they take a picture of their background and then they just overlay it over the mic.

00:00:54.000 --> 00:01:03.000
 So it looks like there is no mic in the frame. They can't, so they can't move their face. They have to be completely still and they have to be like right here.

00:01:03.000 --> 00:01:12.000
 I mean, so my thing with this mic is I feel like I have to like be right up. I feel like make out with it when I talk otherwise I don't really get a good sound.

00:01:12.000 --> 00:01:19.000
 It was different. It was a little bit further away. So he had some range of movement. Yeah. That's a clever trick.

00:01:19.000 --> 00:01:26.000
 It is a clever trick. I've got the, I've got a shotgun mic. A shotgun. But that's not good for this type of thing, right?

00:01:26.000 --> 00:01:33.000
 It's not good for the pocket. Well, it's, it picks up a lot more background noise. So that my secret with the podcast is I can get less of my children screaming.

00:01:33.000 --> 00:01:38.000
 If I use this mic because you kind of do, you have to make out with it. I think we have the same mic, right?

00:01:38.000 --> 00:01:44.000
 Yeah. Yeah. It's the same one. The SM7B. How many of these things do you think they've sold? Because I see them everywhere.

00:01:45.000 --> 00:01:53.000
 Every single, like the Michael Jackson thriller microphone, right? Yeah. Like as soon as that trivia fact went out, every single person with a podcast is like, that's the one I need.

00:01:53.000 --> 00:01:58.000
 I could be Michael Jackson. Yeah. I could have something in common with Michael Jackson.

00:01:58.000 --> 00:02:06.000
 But I see it everywhere. I saw I was walking by a cigar store, which is a normal, I got very common in Miami cigar stores.

00:02:06.000 --> 00:02:20.000
 But like half of the retail space was dedicated to like a podcast set up. And they had like a couch and then they had, they had like six of these things on, on a mountain, everything.

00:02:20.000 --> 00:02:26.000
 And I was like, this is so crazy. I literally everyone, and they're not cheap. They're expensive. Like 300 bucks or something.

00:02:26.000 --> 00:02:38.000
 Oh, yeah. It's just the mic. And it's so needy. It needs so much around it. It sounds good. Yeah, it's like me. I need dacks around me to sound good.

00:02:38.000 --> 00:02:45.000
 Can you imagine six people talking on a podcast? Like at the same time? That's a lot. That'd be intense. That'd be a lot.

00:02:45.000 --> 00:02:51.000
 I've seen clips like that, you know, where like it's like two sides of people and they're all just talking bullshit at each other.

00:02:51.000 --> 00:02:56.000
 It's just that the dumbest people in the world just arguing with each other.

00:02:56.000 --> 00:03:02.000
 Maybe we need to add some people to this podcast, some regulars. We're going to. Sounds like we're going on Tuesday.

00:03:02.000 --> 00:03:09.000
 I mean, besides like guests. Yeah. Oh, I see. Like one of guests. But maybe we need like expand the hosts. Maybe we need four hosts.

00:03:09.000 --> 00:03:15.000
 Let's try it. Let's try it with a lot of people at once. Anyone with the SM7B, you're welcome.

00:03:15.000 --> 00:03:27.000
 Yeah, yeah. Got an SM7B. Hit us up. I saw an SM7B on a Sesame Street, like album cover, like Cookie Monster is talking into one. No joke.

00:03:27.000 --> 00:03:33.000
 I was like, yeah, I know what that mic is. How do I suck your time?

00:03:33.000 --> 00:03:41.000
 That's funny. Well, we're going to kind of like for React Miami, we're going to have four people doing the live stream.

00:03:41.000 --> 00:03:49.000
 Yes. So that's going to kind of be all beyond that same. It's you, Prime, Theo, and Madison.

00:03:49.000 --> 00:03:53.000
 Oh, so it's the same people that did some other conference, but plus you.

00:03:53.000 --> 00:04:00.000
 Plus me. Next comp. Yeah. Yeah. I'm banned from that one. So I couldn't go to that one.

00:04:00.000 --> 00:04:02.000
 I'm just kidding. I'm not banned.

00:04:02.000 --> 00:04:10.000
 Are you? I had a question about that. Oh, did you see? Oh, it was not about that. It was about is prime starting a podcast.

00:04:10.000 --> 00:04:17.000
 Oh, I didn't say something about that. Yeah. And Pirate Software also said, what's the name of the podcast? Are they starting a podcast together? I'm so curious.

00:04:17.000 --> 00:04:30.000
 Interesting. Oh, man. More competition. I'd listen to his competition yesterday.fm. Oh, he got competition. Oh, man, I love it.

00:04:30.000 --> 00:04:38.000
 That was great. Yeah. They didn't even change the background of the thumbnail. It was just so sick tomorrow.

00:04:38.000 --> 00:04:45.000
 Even then, I thought that was way higher effort than, again, for people that aren't aware, we have some competition.

00:04:45.000 --> 00:04:54.000
 There is a competing podcast called yesterday.fm hosted by Melke and Jay, who is CEO of SST.

00:04:54.000 --> 00:05:00.000
 They're not really recording episodes, either. It was just a joke. It's 100% is a joke.

00:05:00.000 --> 00:05:04.000
 But are they going to record joke episodes? Because that would be great. Actually, I would love to listen.

00:05:04.000 --> 00:05:12.000
 It's just the most niche joke ever. The little effort to people that would get it does not make any sense.

00:05:12.000 --> 00:05:16.000
 But yeah, I mean, even an even a thumbnail they made, I thought was higher effort than.

00:05:16.000 --> 00:05:25.000
 Oh, yeah. No, they did a great job. I mean, it would have been extra mile for sure to like make the actual yesterday.fm background.

00:05:25.000 --> 00:05:31.000
 Yeah. But good job, guys. That was a good one. Got a good check on me. You got me.

00:05:31.000 --> 00:05:36.000
 So I have technical steps to talk about. Wow.

00:05:36.000 --> 00:05:44.000
 I don't need to announce that every time, I guess. I think you should, because it's like, I get why you do it because it's infrequent.

00:05:44.000 --> 00:05:49.000
 Yeah. Okay. So I have a technical topic and a non-technical topic. I'll let you choose which one's first.

00:05:49.000 --> 00:05:53.000
 Let's do the technical one first. You might have things you want to talk about, too. I don't want to just say.

00:05:53.000 --> 00:06:02.000
 I don't. We could spend like five minutes each online and then you could talk for like 45 minutes. We got to give the people what they want, you know, some smart conversation.

00:06:02.000 --> 00:06:13.000
 Okay. So technical first, do you ever have one of those nights where it's like one long dream, but you also feel like you're half awake and you're thinking really hard, like in your sleep.

00:06:13.000 --> 00:06:18.000
 Do you ever have that? It's like where you're working through something extremely rarely. Okay.

00:06:18.000 --> 00:06:23.000
 I mean, it's not often for me. I wouldn't say like, yeah, but I can relate to it. I get what you're talking about.

00:06:23.000 --> 00:06:37.000
 Okay. You kind of know what I'm saying. The other night, for whatever reason, I think it was the day after the Carmack tweet and Carpathie invested in that magic magic.

00:06:37.000 --> 00:06:45.000
 Yeah. I think it was the night after like those two data points. And what's funny is like, saw the tweet and saw you quoted it.

00:06:45.000 --> 00:06:53.000
 Saw the Carpathie investment and kind of read up on magic. Didn't even think about it that day. Like, it's just kind of interesting.

00:06:53.000 --> 00:07:03.000
 AI stuff happening. Didn't it didn't really like click in my mind or make me have thoughts of existential dread or whatever as a software developer.

00:07:03.000 --> 00:07:14.000
 But that night, it's like all night long. I was thinking about this idea that like, we're definitely going to have like in the next few years, we're going to have like coworkers.

00:07:14.000 --> 00:07:25.000
 Better AI, not coworkers, but like we're going to have like end to end, like describe a feature, get a PR from these like AI developers.

00:07:25.000 --> 00:07:34.000
 I mean, just large language models, whatever they are, some form of the current version of AI, like making changes in our code bases.

00:07:34.000 --> 00:07:43.000
 That's definitely going to be a thing. I don't know. There was like maybe six months ago or whatever. It was kind of like, oh, that could happen in our lifetime or, yeah, maybe or it'll just make us all more productive.

00:07:43.000 --> 00:07:56.000
 But like, it's very clear to me now, and maybe it's been very clear to everybody for a while. But like, this is going to be a thing where there are junior developers replaced by AI that's just pushing PRs.

00:07:56.000 --> 00:08:04.000
 And the more I thought about it in my sleep, again, I don't know why I can't just sleep like a normal person in my half awake state.

00:08:04.000 --> 00:08:15.000
 It like clicked in a very positive way for me that like, wow, if we could like lower the cost of implementation to like trend towards zero, that's the part I actually hate.

00:08:15.000 --> 00:08:18.000
 I mean, I got to love coding for fun. That's fun. Sure.

00:08:18.000 --> 00:08:23.000
 But like being an individual contributor is tiresome.

00:08:23.000 --> 00:08:25.000
 Like I've done it for 15 years.

00:08:25.000 --> 00:08:38.000
 And the thing that sometimes I wish, I just wish I enjoyed managing people sometimes because I don't. I don't like it at all. And I've spent periods at my startup being a manager and hated it.

00:08:38.000 --> 00:08:49.000
 But I think I might love it. If it were just a bunch of little AI things that I'm just having to like give it the right prompt or whatever you want to call it, give it the right direction.

00:08:49.000 --> 00:08:57.000
 And then it's just handing me PRs to review. That sounds actually super productive and like an amazing thing for society.

00:08:57.000 --> 00:09:07.000
 If one developer could kind of have like a little legion of or just like one instant junior developer that can just literally like churn stuff out for you.

00:09:07.000 --> 00:09:17.000
 It just made me think like, this is going to open the floodgates to what we can accomplish and how much faster a company that has direction and knows what they want to do can do it.

00:09:17.000 --> 00:09:29.000
 So it's actually like it went from like a very scary, I'm a software developer. They're teaching robots to do like it's like the self checkout thing like if you worked at Walmart.

00:09:29.000 --> 00:09:33.000
 Uh oh, and it was that feeling as a developer like it's coming for us.

00:09:33.000 --> 00:09:43.000
 So now it's like, no, this is amazing. We're going to be able to do so much and so fast like as someone who runs a company works at a company and has developers on my team.

00:09:43.000 --> 00:09:47.000
 I can only imagine what we could all do if we all had access to this kind of thing.

00:09:47.000 --> 00:09:53.000
 So I have talked way too long without letting your smart brain react and correct and do the things you should do right now.

00:09:53.000 --> 00:10:03.000
 That's two a.m. By the way, I didn't sleep well last night either. Oh my god. Not related, totally unrelated, but yeah. Anyway, that's that's my thought.

00:10:03.000 --> 00:10:11.000
 I totally agree with you. It's this weird thing where I get why a lot of people are anxious and to be honest. I think a lot of people should be anxious.

00:10:11.000 --> 00:10:15.000
 If I'm going to be completely honest with like, yeah, like Frank about this stuff.

00:10:15.000 --> 00:10:24.000
 There is the reality is there's a type of person that is trying to do stuff and they learn a bunch of skills to do the thing.

00:10:24.000 --> 00:10:29.000
 Then there's a type of person that learns the skills because other people are trying to do a thing.

00:10:29.000 --> 00:10:36.000
 If you're in the latter group, yeah, I see why this is a problem because the thing you described where like you have all these things you want to do.

00:10:36.000 --> 00:10:43.000
 I totally agree, right? Like we always have ideas and it's always like the like process to get to a version one of it.

00:10:43.000 --> 00:10:54.000
 So you can like actually have a tangible idea. It's such a massive cost to anything that lowers that for someone that has ideas and is someone trying to like put ideas out in the world and to build things.

00:10:54.000 --> 00:11:04.000
 This is obviously great and we've been experiencing some portion, some version of this forever. Like the cost of that is slowly lowered over time.

00:11:04.000 --> 00:11:17.000
 Yeah, it required learning new skills required, like figuring out new things and it seems like we're on the cusp of it lowering basically to the lowest it could possibly be without having to like really learn too much.

00:11:17.000 --> 00:11:28.000
 Yeah, so I think yeah for people that are trying to do stuff and programming is a way to do the thing and that's if that's a way you've perceived the world in your work.

00:11:28.000 --> 00:11:44.000
 I think yeah, your conclusion is exactly the right one. It's kind of how I feel too. The little details I do wonder about are so that company magic is trying to make it so that what's the best version of an AI programmer.

00:11:44.000 --> 00:11:59.000
 How can we push this concept to infinity. The thing is, this stuff could be so disruptive that that is actually exactly the wrong idea that presumes that you still need software to be built and written, right?

00:11:59.000 --> 00:12:16.000
 It could there is like a version of real of how this all plays out where the idea of software goes away. If the idea of software goes away and AI is the final software, final piece of software that needs to be written.

00:12:16.000 --> 00:12:30.000
 Like, why do you need an AI programmer? You get what I'm saying? Sure. Yeah, long term I seriously. Yeah, if there is an AI thing and it has access to a database and it can read and write from the database and figure out if it needs to be done just in time.

00:12:30.000 --> 00:12:40.000
 Writing software is just kind of like caching that behavior in a way, right? Like if you can figure out just in time, maybe it costs too much so then you like codify that into a bunch of rules and you like hard coded as software.

00:12:40.000 --> 00:12:50.000
 But there is like a future where a lot less software is written and it's possible that there isn't really a business to like make an AI programmer.

00:12:50.000 --> 00:13:02.000
 And I kind of believe that because if you look at the SaaS category, which is where a lot of people in our field have jobs, they work at companies that build tech for other companies.

00:13:02.000 --> 00:13:13.000
 Yeah, I'm going to call them, they build tech for real businesses. So there are certain things that need to happen in the world. And tech companies just build software that makes that happen faster.

00:13:13.000 --> 00:13:26.000
 And most of us have jobs in that field. That field could just go away. Right. That doesn't mean every single industry goes away. It's really just that field of helping companies be more efficient goes away.

00:13:26.000 --> 00:13:33.000
 So like let's say you have an oil company that like drills oil and does a bunch of stuff.

00:13:33.000 --> 00:13:44.000
 That's still going to be around like we still need that at least for some amount of time. But they don't necessarily need to be buying software. Maybe they don't need to be buying like a thousand pieces of software like they do today.

00:13:44.000 --> 00:13:56.000
 So I kind of wonder if the way that people are going to be pushed out of work isn't really that directly that they're going to be replaced by a programmer by the company they work at.

00:13:56.000 --> 00:14:07.000
 Either company might stop making sense at some point. Yeah, I remember your tweet now. We even talked about it. I think where the company workforce more likely to go under than you are to lose your job.

00:14:07.000 --> 00:14:20.000
 Yeah. So then if you look at what you were saying, if the thing you want to do is something that exists outside of tech, like the thing you want to do can't be like, I want to build software.

00:14:20.000 --> 00:14:28.000
 Right. It has to be something different. Like I want to like, I don't know, like make this good and like sell it on his website.

00:14:28.000 --> 00:14:36.000
 Then like, yeah, I can make that cost like basically zero. I think that makes total sense. But if your idea is like, I want to build productivity software for this group of people.

00:14:36.000 --> 00:14:40.000
 Like, I don't, I don't think the guy helps you there. Something I would just do that job.

00:14:40.000 --> 00:14:51.000
 Yeah. So do you think what you're saying is specific to the SaaS like B2B kind of stuff like productivity software? Because I think so.

00:14:51.000 --> 00:14:58.000
 I think so. I'm trying to think like, so I mean, I can't not talk about statues. I try not to like get too deep into like my specific situation.

00:14:58.000 --> 00:15:10.000
 But we, you know, if you have a consumer facing website, it's kind of like website slash media thing slash information news.

00:15:10.000 --> 00:15:21.000
 Think like Wikipedia, Yahoo finance. I don't know. If you have a site like that, I guess AIs already serving up a lot of stat mu stuff.

00:15:21.000 --> 00:15:28.000
 So perplexity being an example, like they're kind of a new aged AI based search engine.

00:15:28.000 --> 00:15:32.000
 But they do what Google does, which is when you ask sports questions, they reference our stuff.

00:15:32.000 --> 00:15:42.000
 So we still have this kind of like resource that's valuable to the AI, I think, and that we've curated this very like specific database.

00:15:42.000 --> 00:15:51.000
 But I guess also like the long tail here, I feel like you've still got like most of the world on WordPress and like it's very slow.

00:15:51.000 --> 00:16:03.000
 And like if you think about how slow things go, like I guess what I'm getting at is I could see the AI programmer thing being useful for five to ten years.

00:16:03.000 --> 00:16:10.000
 While the in state that you're kind of describing takes shape and becomes a thing.

00:16:10.000 --> 00:16:19.000
 Yeah, I think practically you're right. Like that's what I'm describing is like an overly simplified scenario where I'm just like talking about what the final thing is.

00:16:19.000 --> 00:16:25.000
 And there might be like millions of opportunities, it might take 50 years, who knows, like the pathway to that.

00:16:25.000 --> 00:16:28.000
 So I'm not saying like magic can't work out.

00:16:28.000 --> 00:16:34.000
 But right now, if I'm going to make like a wild ass bet and start a company like that.

00:16:34.000 --> 00:16:42.000
 If you're a venture, like looking at it from a venture perspective, like the big giant bet, then maybe that's not the best kind of intermediate step that you're betting on.

00:16:42.000 --> 00:16:49.000
 Yeah, like if I could build anything right now, it probably wouldn't, I probably wouldn't be trying to do what magic is doing.

00:16:49.000 --> 00:16:55.000
 It kind of works in two ways, right? Like what's great about their business, it's there's such an incremental step.

00:16:55.000 --> 00:17:01.000
 Like people, companies are already producing a lot of code. They already have, you know, people that they hire to do that.

00:17:01.000 --> 00:17:05.000
 They have workflows and processes and they can like incrementally adopt this stuff.

00:17:05.000 --> 00:17:11.000
 So from a business point of view, like makes total sense and like I get why there's a lot of opportunity there.

00:17:11.000 --> 00:17:22.000
 But again, if it's like a moonshot type of bet, that one doesn't seem that in a lot of ways, it seems like too practical and like too likely to work to actually work.

00:17:22.000 --> 00:17:25.000
 You know what I mean? Yeah, it's like a paradox.

00:17:25.000 --> 00:17:30.000
 Yeah, but if I look at it as like a potential customer, I'm very excited about it.

00:17:30.000 --> 00:17:33.000
 I hope I succeed because I would love to use it and pay for it.

00:17:33.000 --> 00:17:52.000
 And like across multiple things, like not just at, you know, my day job, but also like every side project I ever think of just being able to like leverage something like that and not spend so much time implementing sounds very attractive.

00:17:52.000 --> 00:18:07.000
 Yeah, a lot of things change, right? Let's say that thing is really good and you never have to directly manipulate or read the code. You just have to, you would do is still iterate on it and you maybe iterate on it in a very familiar way to the way we iterate on software right now.

00:18:07.000 --> 00:18:10.000
 But you don't have to do the direct part.

00:18:10.000 --> 00:18:16.000
 What does the best programming language mean at that point? Yeah, so I think we've kind of talked about this.

00:18:16.000 --> 00:18:26.000
 And I think it's the end state thing to again, like with the end state, it's like, sure, it should just be spitting out binary or assembly language or whatever.

00:18:26.000 --> 00:18:33.000
 I don't know, maybe Wasm has a point in the first in the after all, like there's a reason for Wasm to exist. I don't know.

00:18:33.000 --> 00:18:44.000
 But in the intermediate state, it kind of be nice if it was spitting out TypeScript or whatever you used to reading so that you could actually review it and catch stuff like as it's getting better.

00:18:44.000 --> 00:18:50.000
 Imagine there might be things that it does that you're like, Oh, I didn't understand exactly what I was going for here.

00:18:50.000 --> 00:18:55.000
 And maybe you fix it directly or you give feedback and it responds to the feedback. I don't know.

00:18:55.000 --> 00:18:59.000
 I could see that intermediate state benefiting from it being a language you're familiar with.

00:18:59.000 --> 00:19:00.000
 Yeah.

00:19:00.000 --> 00:19:09.000
 That's the immediate intermediate state, but you can see how you might be more willing to choose a language with different properties, right?

00:19:09.000 --> 00:19:18.000
 If you feel like, if you start using this way, it turns out you're like really just editing and tweaking the stuff it's outputting.

00:19:18.000 --> 00:19:25.000
 You might be more than willing to give up a familiar language. You might say, like, I'm down to use it more like performance language.

00:19:25.000 --> 00:19:28.000
 I mean, it's a little harder to iterate traditionally.

00:19:28.000 --> 00:19:35.000
 Even with the new SSCLI when I start and we wrote it and go, and I'm familiar with go like I did it for years, but I had it in a while.

00:19:35.000 --> 00:19:46.000
 So I was rusty. And one of the things I tried to do is, OK, I'm starting a new workflow. Basically, I'm in a new language effectively, not what I've been doing the past couple of years.

00:19:46.000 --> 00:19:53.000
 Let me try to like overuse AI. Even when I feel like I can do it, let me like try to do the AI first.

00:19:53.000 --> 00:19:59.000
 And just so like, I just kept, I just keep feeling like I'm not using it enough. So this was a good opportunity for me to use it more.

00:19:59.000 --> 00:20:04.000
 And yeah, you know, AI wrote a lot of the CLI, a lot of the code that's that we're shipping.

00:20:04.000 --> 00:20:10.000
 Yeah. And it did a good job. And I mostly just like babysat it and like pointed it in the right direction a lot of times.

00:20:10.000 --> 00:20:18.000
 And I do feel like sometimes it found better ways to do things than I would have. I think it did do things faster.

00:20:18.000 --> 00:20:26.000
 So I don't like to use the word faster because I think whenever the concept of faster comes up, people are always like, it's only like three seconds faster.

00:20:26.000 --> 00:20:33.000
 Who cares? What actually matters is it took me less energy. It definitely takes me less energy to tell Chad JBT something and have it spit it out.

00:20:33.000 --> 00:20:38.000
 Yeah. It's a little awkward at first because you're a little more familiar just typing the code versus describing the problem.

00:20:38.000 --> 00:20:43.000
 But once you get used to it, it is easier. So yeah. And then so if you take this one step further.

00:20:43.000 --> 00:20:51.000
 Okay. Now you're at the stage where people are willing to relax their immediate feelings about languages because they're not the ones that are manipulating it.

00:20:51.000 --> 00:20:57.000
 Now, if you're a language designer, what does designing a language for AI mean? Like, what does that?

00:20:57.000 --> 00:21:03.000
 Would you do things differently? Would you care about certain things? AI needs a lot of examples.

00:21:03.000 --> 00:21:09.000
 So like that also maybe makes it hard to create a new language now because it's like a cold start problem.

00:21:09.000 --> 00:21:14.000
 Yeah, exactly. And it's funny. So we're doing new docs for SST for ION.

00:21:14.000 --> 00:21:21.000
 And we're putting a shit ton of examples everywhere, not for people, but so they can learn. So we're changing our behavior.

00:21:21.000 --> 00:21:26.000
 That's awesome. Oh, I definitely think of SST when I think of like building stuff really fast with AI.

00:21:26.000 --> 00:21:33.000
 It would be really great to have this sort of integrated deployment picture in there with the code that you're also building.

00:21:33.000 --> 00:21:42.000
 Like just the idea of using serverless stuff and being like literally stand up a startup in like such little time.

00:21:42.000 --> 00:21:48.000
 Yeah. Relative to the old barriers of entry kind of goes hand in hand with that whole movement, I think.

00:21:48.000 --> 00:21:58.000
 Yeah. And that's why I think what is so that's really exciting. The part that I think we will have to adapt delis for me because I've always worked like in tech tech.

00:21:58.000 --> 00:22:03.000
 Like I never worked at like a real company. I've only worked at a company helping real companies. Yeah.

00:22:03.000 --> 00:22:14.000
 Make a shovels. Yeah, exactly. So my whole mindset, I think I need to shift it a little because I don't think selling shovels is going to be as viable.

00:22:14.000 --> 00:22:19.000
 Like do the actual digging now, you know, because like the ultimate shovel has been made in some ways.

00:22:19.000 --> 00:22:30.000
 And even with SSC, you know, I think about this where our whole thing is creating a nice abstraction for people is simple and progressively discloses into something more complicated.

00:22:30.000 --> 00:22:33.000
 That takes a lot of work and a lot of constraints and a lot of trade offs.

00:22:33.000 --> 00:22:41.000
 But if an AI can just in time always generate the right thing, like what level of abstraction does it need? Like does it need all that?

00:22:41.000 --> 00:22:51.000
 And if the user isn't really directly manipulating this stuff, like is it okay to sacrifice a whole bunch of things that we currently are not willing to?

00:22:51.000 --> 00:23:02.000
 Yeah. I can't remember if it was your tweet or I saw somebody tweet something online of like things that don't matter when it comes to AI and a programming language and they mentioned like type safety.

00:23:02.000 --> 00:23:15.000
 Like it's an interesting thought that like things that we value so much for refactoring and for all these things, like those concepts, those concerns do just kind of like go away if AI is doing the work, doing the implementing, at least.

00:23:15.000 --> 00:23:20.000
 It's super interesting. I can't remember what else was on that list. Was that a tweet of yours or was that something else?

00:23:20.000 --> 00:23:25.000
 I did mention that I did have a tweet about how type safety is something less important.

00:23:25.000 --> 00:23:35.000
 That one clicked for me immediately when I started using co-pilot because co-pilot's auto-complete sometimes does stuff you like get from type safety.

00:23:35.000 --> 00:23:36.000
 Yeah. Yeah.

00:23:36.000 --> 00:23:40.000
 You know, like it figures out like the right thing and it's like it's actually doing a better job than the type safety.

00:23:40.000 --> 00:23:44.000
 I just had the thought today that like my neo-vim configuration.

00:23:44.000 --> 00:23:53.000
 I can't remember how it's prioritized, but it does feel like my LSP and like co-pilot are competing and often I'm choosing co-pilot.

00:23:53.000 --> 00:24:03.000
 It's just interesting that the LSP's already been diminished somewhat because my first tab down, like it's a co-pilot option and it does exactly what I needed to do.

00:24:03.000 --> 00:24:08.000
 It's something the LSP couldn't have predicted. It's just that level beyond. Yeah, it's interesting.

00:24:08.000 --> 00:24:09.000
 Yeah, exactly.

00:24:09.000 --> 00:24:18.000
 Sometimes I do wish like I could just go with the, like sometimes I'm frustrated when I choose the co-pilot thing and it doesn't get the last bracket or something.

00:24:18.000 --> 00:24:23.000
 Like it does that stupid stuff for them. It's like, I got to figure out what's missing.

00:24:23.000 --> 00:24:27.000
 Yeah, yeah, yeah. It's like randomly delete parentheses and they're like, add them into the words.

00:24:27.000 --> 00:24:29.000
 Yeah, until it's good. Yeah.

00:24:29.000 --> 00:24:30.000
 Yeah.

00:24:30.000 --> 00:24:36.000
 So that was nice at LSP's. They're a little more precise or a lot more precise, but they can't do things.

00:24:36.000 --> 00:24:41.000
 They're just in a totally different class. Like they're completely outclassed in other ways by things like co-pilot.

00:24:41.000 --> 00:24:45.000
 And there's someone in the, in the Twitch chat saying, I think you're exaggerating a bit on how good AI will be in the future.

00:24:45.000 --> 00:24:51.000
 So my, my responses is actually everything I'm saying doesn't require AI to get much better.

00:24:51.000 --> 00:24:55.000
 Even if you take what it can do today in terms of raw capability,

00:24:55.000 --> 00:25:01.000
 every problem I have with it, at least today, is a coordination and workflow problem.

00:25:01.000 --> 00:25:09.000
 It's like, I actually need it to do these like six different, like, do this and pipe it into this program and then take it from this program.

00:25:09.000 --> 00:25:16.000
 And it's like, to me, it's like a very much so an orchestration problem, which is very simple.

00:25:16.000 --> 00:25:21.000
 So if AI gets like 20% better and the orchestration stuff gets a lot better,

00:25:21.000 --> 00:25:26.000
 a lot of what we're talking about starts to become like a reality.

00:25:26.000 --> 00:25:31.000
 So, yeah, in a lot of ways, like the AGI in a form is kind of here.

00:25:31.000 --> 00:25:34.000
 It's just we don't know how to like connect it together in the right order.

00:25:34.000 --> 00:25:42.000
 Yeah, yeah, I don't feel like the actual models themselves need to get better for like so many things to open up.

00:25:42.000 --> 00:25:51.000
 It's when we started Samus, I mean, that was like the whole thing was just like natural language processing tech was really advanced even 10 years ago,

00:25:51.000 --> 00:25:53.000
 but like there's so few consumer applications of it.

00:25:53.000 --> 00:25:57.000
 And it was just like, hey, we could apply this to the thing we care about.

00:25:57.000 --> 00:26:01.000
 And a lot of people love that. And it was just like, it's the same thing today. I feel like with AI.

00:26:01.000 --> 00:26:07.000
 It's like all the pieces are there. It's just now people putting together into products that are really useful.

00:26:07.000 --> 00:26:11.000
 And yeah, that last mile, that orchestration bit, I feel that pain too.

00:26:11.000 --> 00:26:19.000
 But yeah, I don't think we're like banking on some huge step function of complexity or stuff that's opened up and the accuracy of the models or whatever.

00:26:19.000 --> 00:26:23.000
 I feel like they're good enough to do so much of what I do day to day.

00:26:23.000 --> 00:26:31.000
 The other thing is, okay, so given all this, you know, what actually matters now as like, what does this all require if the input isn't being able to write code well?

00:26:31.000 --> 00:26:35.000
 One of the ways I've been thinking, that's funny because that means I think fits perfectly into this.

00:26:35.000 --> 00:26:44.000
 If an AI can do everything can be useful and you want to put a product where an AI is involved, it still needs a clean source of data.

00:26:44.000 --> 00:26:50.000
 So whatever the thing you're doing, the cleaner your data is, the better it can do the work.

00:26:50.000 --> 00:26:54.000
 So me and Liz are working on radiant, which is for personal finance.

00:26:54.000 --> 00:27:09.000
 We are the way we're thinking about this is we want to make it the first step is to make it stupid, easy to like clean up your data and create rules that like this human created rules that make that like the best source of financial data that's ever existed for you.

00:27:09.000 --> 00:27:13.000
 We can layer in some like general AI stuff to help you clean that up.

00:27:13.000 --> 00:27:25.000
 That's new hasn't been able to have been able to do that before. It works really well already. And what we end up with is like a really interesting clean set of events, financial events for you.

00:27:25.000 --> 00:27:32.000
 But the way we've built things, we're not limiting it to financial events. We'd love to get all sorts of stuff in there.

00:27:32.000 --> 00:27:42.000
 So just take something completely random. Let's say you're like browsing history. I don't know if we would actually do this, but like every website you visited, that's also now ingested in there.

00:27:42.000 --> 00:27:54.000
 And you can see as AI gets better, we now have this amazing set of data that is like your personal data that this can now tap into and we can build probably something really cool out of it.

00:27:54.000 --> 00:28:00.000
 Yeah, Ken with it was coming with like really funny examples on Twitter, but it's the exact type of thing that we could enable.

00:28:00.000 --> 00:28:11.000
 Like, you know, it's like 5pm. And then your AI messages you being like, hey, it's dinner time. You haven't ordered Indian food in a couple months.

00:28:11.000 --> 00:28:21.000
 Maybe you should have Indian food to time and order that for you. Today, AI could do that. They can totally do that. The problem is there's no pipeline to like, it had that information about you to do that.

00:28:21.000 --> 00:28:32.000
 So we're making a way to like connect all your financial stuff, but also your Amazon account. You're going to eat some account, connect everything right now under the pretense of like just getting a good financial picture of your life.

00:28:32.000 --> 00:28:42.000
 But you can see how if this data set becomes very clean, it just feeds into whatever and interface AI interface is going to come out at some point.

00:28:42.000 --> 00:28:49.000
 And so that means the same thing, right? Just having that clean data set and the way people access it might be different in a couple years.

00:28:49.000 --> 00:29:03.000
 Man, yeah, you're just reminding me that like everything I was thinking about up to this point in the conversation was like what we've come to know is a traditional way to interact with AI where like it's generating a bunch of text.

00:29:03.000 --> 00:29:12.000
 But there's all these other ways AI is super useful, like summarizing, cleaning up data. And that's whole other vectors that companies will get better, new products will emerge.

00:29:12.000 --> 00:29:21.000
 Yeah, it's just, I think we're in for like a pretty great few years if you're into technology. Yeah, this episode is turning into the AI episode.

00:29:21.000 --> 00:29:27.000
 And I feel like it'd be really cool if at some point we're like, this whole episode was generated with AI.

00:29:27.000 --> 00:29:30.000
 This isn't actually Adam and Dax.

00:29:30.000 --> 00:29:39.000
 Yeah, yeah, no, it's, it's, it's exciting. Yeah, some of that stuff, like you said, like outside the generative stuff, because we've been playing with it just to clean up data and it's.

00:29:39.000 --> 00:29:43.000
 It like, it solves things I've seen. Yeah.

00:29:43.000 --> 00:29:53.000
 Over and over and over my life. And finally, it does it correctly. So a classic thing with financial data is sometimes it'll be the merchant name and for some reason, the location.

00:29:53.000 --> 00:30:07.000
 It'll be like Starbucks, Miami, Florida. But like, there won't be spacing between these things. It'll be Starbucks, Miami, Florida. So it's impossible to really write a parser that is like, give me the counterparty for this. It should just be Starbucks.

00:30:07.000 --> 00:30:18.000
 Yeah, because as a human, you can, you have a sense of, oh, this is the name and this is probably a location, even if you don't immediately recognize location because it has a location sounding type name, right?

00:30:18.000 --> 00:30:21.000
 And you can very easily split those apart.

00:30:21.000 --> 00:30:25.000
 Historically, very hard to do, but AI does this like near perfectly.

00:30:25.000 --> 00:30:30.000
 There's another thing I've been kind of trying to figure out because I think this is going to be something that comes up a lot.

00:30:30.000 --> 00:30:34.000
 There's like a set of things we can ask AI and it's going to give you back a response.

00:30:34.000 --> 00:30:40.000
 And oftentimes we want a deterministic response, right? So again, Starbucks, Miami, Florida, what is a counterparty?

00:30:40.000 --> 00:30:46.000
 That kind of only ever needs to be asked once ever and then it can be cached for like all time, right?

00:30:46.000 --> 00:30:51.000
 Because we don't want it every time a transaction comes in and we don't want to like ask it. We're going to build some kind of caching layer.

00:30:51.000 --> 00:31:01.000
 So it's kind of like, yeah, like is open AI like caching every single question that people ask and then, oh, you know what I'm saying?

00:31:01.000 --> 00:31:08.000
 Like, eventually we'll have asked all the questions. I mean, not literally, but we'll have asked, like, it's like a lot more than a thing, right?

00:31:08.000 --> 00:31:15.000
 Like most questions have been asked before already. And if you don't want a creative response, like, you know, you can set the temperature to zero or whatever.

00:31:15.000 --> 00:31:24.000
 Yeah. Yeah. If you're asking a question with a temperature zero, if someone else, like, if someone else asks basically the same question, like, you can give you the exact same response.

00:31:24.000 --> 00:31:30.000
 So that's kind of interesting too. Yeah, I think, but you hit one of the key sticky points, which is that there are other parameters.

00:31:30.000 --> 00:31:41.000
 Like, and like I have custom instructions in my GPT, so like, how does that impact? I presumably can't just cache unless there's something, some intermediate thing that they can cache that.

00:31:41.000 --> 00:31:50.000
 Yeah, then runs through. I don't know. It depends on the use case because I don't think JB T could, but this case where I'm, I'm like filtering and cleaning up data.

00:31:50.000 --> 00:32:00.000
 Yeah. Mm-hmm. Yeah. I'm basically 100%. Yeah. But then I also wonder, can it introspect and create deterministic rules for me?

00:32:00.000 --> 00:32:08.000
 So when I say, give me the counterparty, can I tell it to write code that gets me the counterparty from this?

00:32:08.000 --> 00:32:17.000
 And it writes in like crazy ass, like, thing that doesn't make any sense, but it does work and that can just run that code every time instead of directly querying the LM.

00:32:17.000 --> 00:32:27.000
 It's really weird. It's a weird like dynamic. Yeah, it just opens up so many different things that you think like when we started our careers, there were problems that you just wouldn't even approach.

00:32:27.000 --> 00:32:38.000
 Like, there were things that you just couldn't even think about solving. And it's almost like, it's like things you, five years ago, things you would have reached for mechanical Turk.

00:32:38.000 --> 00:32:46.000
 Right. Exactly. It's like, oh, this is a human, but it's just, it's still all within a system, but it has like human reasoning skills.

00:32:46.000 --> 00:32:56.000
 It's pretty wild. Yeah. I'm very excited. I hope to sleep through the nights and not have another night like that, but it was a turning point for me.

00:32:56.000 --> 00:33:05.000
 Somehow that night of sleep, I went from like anxiety over AI. And what does it mean for our profession to like, oh, this is going to be awesome.

00:33:05.000 --> 00:33:13.000
 Yeah. And yeah, maybe the key is like being entrepreneurial. If you're not entrepreneurial in any way, shape or form, maybe it's terrifying.

00:33:13.000 --> 00:33:18.000
 But if you got any bit of that in your, if you think you could, I think it's a huge opportunity.

00:33:18.000 --> 00:33:30.000
 The one other thing that you touched on that made the last, last point I'll make is a, because I think the key thing in John Carmack's post was he talked about how he doesn't like managing humans and he probably will have a better time managing AI.

00:33:30.000 --> 00:33:38.000
 And there is a really funny thing where I've already experienced this and it's been kind of crazy to realize, I don't know if we taught those already.

00:33:38.000 --> 00:33:43.000
 When I asked Chad JBT for stuff, sometimes I talk to it for like some kind of complicated things.

00:33:43.000 --> 00:33:58.000
 I'm like trying to get to design me something or the feedback you can give it is so much more precise and direct because you do not have to worry about a long-term relationship, damaging a long-term relationship with this thing.

00:33:58.000 --> 00:34:03.000
 Yeah. So if it doesn't think bad, I should, I'm sure like that sucks. Like here's exactly why it sucks.

00:34:03.000 --> 00:34:12.000
 Whereas the person, you like have to like give them a few tries and then like kind of like layer and feedback slowly, that is an insane productivity boost.

00:34:12.000 --> 00:34:16.000
 When you can just be like, this sucks, you're being too corny, like more like this.

00:34:16.000 --> 00:34:20.000
 And you just can't interact with a person that way. So it's quite different.

00:34:20.000 --> 00:34:30.000
 Yeah, you can't. There's so much more that comes into play when you hire a human being and you are, or would you just put in charge of them?

00:34:30.000 --> 00:34:42.000
 And they're your direct report. There's one on ones every other week. There's, I mean, there's just all these things that an AI developer, you don't have to deal with.

00:34:42.000 --> 00:34:49.000
 And we just, we sound like the evil empire right now. I'm sure to some people that are listening, they're like, oh, these are the jerks.

00:34:49.000 --> 00:34:57.000
 They're going to hire some AI developer instead of me. Yeah, I don't, I think it's the, it's the, what's abundance mindset.

00:34:57.000 --> 00:35:02.000
 It's like, this is going to make, is that just a trope that it's, yeah, it's going to take jobs.

00:35:02.000 --> 00:35:06.000
 It's going to make more jobs. I really feel like there's just going to be more opportunity because of all this.

00:35:06.000 --> 00:35:12.000
 But I can't say that like in individual circumstances, there won't be people that don't have the opportunity they had.

00:35:12.000 --> 00:35:23.000
 I don't know. Yeah. And I think that's the case. And at this point, my mindset is I can really just worry about my own path and like how it affects my own path.

00:35:23.000 --> 00:35:30.000
 Yeah, I don't know how it's going to affect everyone else's. Yeah. But I'm trying to work myself. Sorry.

00:35:30.000 --> 00:35:39.000
 Yeah. I mean, it's just like people, I mean, part of me, if I'm being honest, it's, some of these differences are not, what's the word?

00:35:39.000 --> 00:35:49.000
 They are, they are, they're not like clean differences. Like, I've had differences with people in my life where I felt like their view of the world was getting in the way of mine.

00:35:49.000 --> 00:35:55.000
 And they made it harder for like me to do the version of my life that I wanted.

00:35:55.000 --> 00:36:05.000
 So in some ways, yeah, I will say, like I will admit that this type of thing is way more in my favor than this other thing that I've kind of been fighting against.

00:36:05.000 --> 00:36:16.000
 So in some ways, I like don't really feel that bad. So yeah, that is like a little bit there as weird as it is to admit, like a shade in Freud, I guess, is a word.

00:36:16.000 --> 00:36:22.000
 If anybody, if anybody's listening to this and they're feeling anxiety now because they feel like they're going to lose your job, everything moves slow.

00:36:22.000 --> 00:36:30.000
 So just remember that, like things move slow. It's not an overnight thing. Now getting laid off at a fang company, that's overnight. That happens all the time.

00:36:30.000 --> 00:36:35.000
 It's been happening in mass and it has nothing to do with AI. I don't think.

00:36:35.000 --> 00:36:42.000
 So yeah, could you get laid off at your cushy job or you're not so cushy job? Yes, that is a fact of life.

00:36:42.000 --> 00:36:48.000
 But the AI thing, it's like long term trends and shifts in the way we work.

00:36:48.000 --> 00:36:51.000
 This is not like you need to be looking over your shoulder.

00:36:51.000 --> 00:37:09.000
 Yeah, there's such like a it's weird because I like I've been made so aware of how on the edge of things that I focus on like just because of and I think for a lot of us that are friends that were kind of all in the same place where we're just like always right on the edge.

00:37:09.000 --> 00:37:17.000
 So we get hyper obsessed with this stuff that takes so long to make it to the vast majority of the world.

00:37:17.000 --> 00:37:23.000
 There's companies that are just moving AWS and just figuring out the cloud.

00:37:23.000 --> 00:37:29.000
 It's going to take them a while to figure out all those other stuff. So yeah, things do move really slow.

00:37:29.000 --> 00:37:50.000
 Go and that's what you said about hiring. We're like the jerks are going to have replaced people and hire and I already made a tweet about this. I don't know if you remember, I was like, I can't wait for GPT five so I never have to hire one of you idiots again.

00:37:50.000 --> 00:37:55.000
 I like people. I'm not one of those people that's like I hate people and I just want to like be myself and talk to you.

00:37:55.000 --> 00:38:11.000
 I love people and I like want to continue to work with people. I just don't think it needs to be a lot of a lot of people and I've been this way pre AI like this has like been my dream for a very, very long time which is a small group of people that I love and we're just doing.

00:38:11.000 --> 00:38:26.000
 I have a massive impact like I've always wanted that is just, you know, except more in that direction. Yeah, just further enables it. What is E slash a CC mean. Oh man. I don't know what that is, but I can. I thought I did these things.

00:38:26.000 --> 00:38:38.000
 What does it mean? I think Ken Wheeler's roasting of them was probably the most accurate. Yeah, I think I did see one of his tweets. Is it like it's AI related like it's people who are in the AI.

00:38:38.000 --> 00:38:46.000
 So there's this idea, which is, which I think I roughly agree with, which is we want to accelerate AI as fast as possible. And that's what's best for humanity.

00:38:46.000 --> 00:39:01.000
 Like we shouldn't be like wasting time asking all these questions that our primary goal should be to like who says because any theoretically it fixes so much and it helps the world so much and you believe in that you put.

00:39:01.000 --> 00:39:11.000
 And like they've codified that into like a thing, you see, I don't know what something accelerate for something. I think they're effective acceleration. It's like something like that.

00:39:11.000 --> 00:39:19.000
 Okay, this like comes from like the there's been a bunch of things in the in this shape. And so it's kind of in that category.

00:39:19.000 --> 00:39:23.000
 But so, you know, some that makes sense. You know, the way described it.

00:39:23.000 --> 00:39:35.000
 But when people put this kind of shit in their Twitter bio, it's just like a weird identity thing. They're like, I am someone that likes AI. But if you like dig into that, it's I'm someone that's afraid of AI.

00:39:35.000 --> 00:39:42.000
 But if I put this in my identity, then it feels like I'm going to be one of the people that benefit, you know, spare me the AI overlords.

00:39:42.000 --> 00:39:47.000
 Yeah, exactly. It's like, I'm going to be one of the people that benefit and you're all going to get left behind and I'm going to be.

00:39:47.000 --> 00:39:56.000
 And it's like someone that just puts that in their profile is not doesn't fit the persona of someone that would actually like, yeah, benefit that from that.

00:39:56.000 --> 00:40:04.000
 It's I think like you see this with like crypto to like a bunch of crypto people put stuff in their profile that like signals that something.

00:40:04.000 --> 00:40:10.000
 So yeah, it's purely an identity thing at this point. And whenever I see them, it's like, yeah, okay.

00:40:10.000 --> 00:40:18.000
 So we'll know. We'll know how many of our listeners are into this stuff or crypto based on how many fewer viewers we have after this.

00:40:18.000 --> 00:40:25.000
 Just occurred to me like we have enough people to listen that I'm sure we offend someone every single episode.

00:40:25.000 --> 00:40:31.000
 Someone has EACC and their profile and I'm hoping after this episode they they reconsider it because I am judging.

00:40:31.000 --> 00:40:33.000
 Yeah, that's what I'm very directly judging.

00:40:36.000 --> 00:40:43.000
 It's funny. There were other tech things that happened like news, if you will, the White House says you should write rust.

00:40:43.000 --> 00:40:45.000
 I didn't read anything beyond it.

00:40:45.000 --> 00:40:46.000
 I didn't read it.

00:40:46.000 --> 00:40:51.000
 Did they literally say rust or did they say that they don't we should encourage memory safe languages?

00:40:51.000 --> 00:40:52.000
 Is that what they say?

00:40:52.000 --> 00:40:56.000
 I saw memory safe. Yeah, literally what I know about this is from like a couple of YouTube thumbnails.

00:40:56.000 --> 00:40:57.000
 That's literally it.

00:40:57.000 --> 00:41:00.000
 That's how I learned about tech these days.

00:41:01.000 --> 00:41:06.000
 To be honest, I think there is literally nothing more than what you've learned from YouTube thumbnails.

00:41:06.000 --> 00:41:13.000
 It makes sense like you know, cybersecurity is a huge thing and a lot of tech exploits are from memory safety issues.

00:41:13.000 --> 00:41:15.000
 So of course, this makes a little sense.

00:41:15.000 --> 00:41:21.000
 And of course, the best thing to take away from it are a bunch of jokes, which I think is what I can do.

00:41:21.000 --> 00:41:28.000
 Did rust ever make a comeback after like all the foundation stuff?

00:41:28.000 --> 00:41:31.000
 Like did it come back in favor or are people still upset?

00:41:31.000 --> 00:41:33.000
 A lot of stuff is being written in rust.

00:41:33.000 --> 00:41:37.000
 I don't know if that stuff really made a dent in like the actual usage of it.

00:41:37.000 --> 00:41:39.000
 It was just a Twitter perception of it.

00:41:39.000 --> 00:41:41.000
 Yeah, I think I think some energy was lost.

00:41:41.000 --> 00:41:45.000
 It hit all 50 of us in the Twitch dev community.

00:41:45.000 --> 00:41:47.000
 That was it.

00:41:47.000 --> 00:41:48.000
 Yeah.

00:41:48.000 --> 00:41:52.000
 And then the 50 of those people that never really ship anything anyway stop writing rust.

00:41:52.000 --> 00:41:53.000
 So yeah.

00:41:53.000 --> 00:41:54.000
 Okay.

00:41:54.000 --> 00:41:55.000
 Cool.

00:41:55.000 --> 00:41:56.000
 Cool.

00:41:56.000 --> 00:41:58.000
 Yeah, what else?

00:41:58.000 --> 00:42:00.000
 I feel like there was other stuff on Twitter.

00:42:00.000 --> 00:42:02.000
 It's been, it's been semi active in the tech world.

00:42:02.000 --> 00:42:03.000
 Has it?

00:42:03.000 --> 00:42:04.000
 Maybe not.

00:42:04.000 --> 00:42:06.000
 Maybe I made that up.

00:42:06.000 --> 00:42:07.000
 What have you been tweeting?

00:42:07.000 --> 00:42:08.000
 That's always fun.

00:42:08.000 --> 00:42:12.000
 I feel like we actually have segments and one of the segments is reviewing my tweets.

00:42:12.000 --> 00:42:13.000
 Yeah, we do.

00:42:13.000 --> 00:42:14.000
 It's, it's looking at your Twitter.

00:42:14.000 --> 00:42:15.000
 Yeah.

00:42:15.000 --> 00:42:18.000
 You just saw your tweet from an hour ago.

00:42:18.000 --> 00:42:20.000
 You've all had CO2 monitors for over a month down.

00:42:20.000 --> 00:42:21.000
 I haven't seen it before.

00:42:21.000 --> 00:42:26.000
 I love it.

00:42:26.000 --> 00:42:28.000
 I do have one sitting right here.

00:42:28.000 --> 00:42:31.000
 You say you open your window a lot more now?

00:42:31.000 --> 00:42:32.000
 A lot more.

00:42:32.000 --> 00:42:34.000
 It's 1193 right now.

00:42:34.000 --> 00:42:35.000
 That's not good.

00:42:35.000 --> 00:42:39.000
 When I turn on my studio lights for some reason, it's probably because I talk more than the

00:42:39.000 --> 00:42:40.000
 lights around.

00:42:40.000 --> 00:42:42.000
 But the CO2 levels, they climb.

00:42:42.000 --> 00:42:44.000
 It's just too cold right now.

00:42:44.000 --> 00:42:45.000
 I can't open the window.

00:42:45.000 --> 00:42:46.000
 Yeah.

00:42:46.000 --> 00:42:50.000
 I mean, like I said, I live in like a very containment, but again, my AC is always on.

00:42:50.000 --> 00:42:53.000
 And I think that basically does it for me.

00:42:53.000 --> 00:42:54.000
 I don't know.

00:42:54.000 --> 00:42:55.000
 I don't have a CO2 monitor.

00:42:55.000 --> 00:42:56.000
 So I could be dying.

00:42:56.000 --> 00:42:59.000
 By the way, this is going to be my last episode from this location.

00:42:59.000 --> 00:43:00.000
 So this is.

00:43:00.000 --> 00:43:01.000
 Sorry.

00:43:01.000 --> 00:43:02.000
 What?

00:43:02.000 --> 00:43:03.000
 Oh, you're moving.

00:43:03.000 --> 00:43:04.000
 Yeah.

00:43:04.000 --> 00:43:05.000
 Next door.

00:43:05.000 --> 00:43:06.000
 To an identical, like this room is identical.

00:43:06.000 --> 00:43:07.000
 It's going to be identical.

00:43:07.000 --> 00:43:08.000
 Like the houses are similar.

00:43:08.000 --> 00:43:09.000
 Yeah.

00:43:09.000 --> 00:43:12.000
 So I'm going to set everything back up and then we can check like a frame of frame thing and

00:43:12.000 --> 00:43:14.000
 see how much is actually changing.

00:43:14.000 --> 00:43:15.000
 That's awesome.

00:43:15.000 --> 00:43:16.000
 Yeah.

00:43:16.000 --> 00:43:17.000
 I love it.

00:43:17.000 --> 00:43:19.000
 So there's no way anyone will notice if they don't hear us say.

00:43:19.000 --> 00:43:20.000
 Yeah.

00:43:20.000 --> 00:43:21.000
 You just moved.

00:43:21.000 --> 00:43:23.000
 Did you see Jay posted a picture of the.

00:43:23.000 --> 00:43:24.000
 Yeah.

00:43:24.000 --> 00:43:25.000
 Was that the back of your house?

00:43:25.000 --> 00:43:26.000
 Yeah.

00:43:26.000 --> 00:43:27.000
 Did he come over?

00:43:27.000 --> 00:43:28.000
 No, no, I know.

00:43:28.000 --> 00:43:29.000
 I send him that picture and then he posted it on Twitter.

00:43:29.000 --> 00:43:30.000
 That's what we do.

00:43:30.000 --> 00:43:36.000
 Anytime someone sends us a picture in our team chat, someone else posted on Twitter.

00:43:36.000 --> 00:43:40.000
 I still see pictures of you that pop up all the time.

00:43:40.000 --> 00:43:43.000
 Like the one I generated in Dolly.

00:43:43.000 --> 00:43:46.000
 And then there's the one you posted of you like.

00:43:46.000 --> 00:43:49.000
 Looking insane.

00:43:49.000 --> 00:43:50.000
 Yeah.

00:43:50.000 --> 00:43:51.000
 You look insane.

00:43:51.000 --> 00:43:52.000
 Yeah.

00:43:52.000 --> 00:43:53.000
 Oh, man.

00:43:53.000 --> 00:43:55.000
 That's going to be the face I make when I replace everyone with the guy.

00:43:55.000 --> 00:43:57.000
 That's the exact energy of that photo.

00:43:57.000 --> 00:43:59.000
 That's what you should be anticipating.

00:43:59.000 --> 00:44:01.000
 If your job's on the line.

00:44:01.000 --> 00:44:03.000
 You can just switch my Twitter profile picture to that.

00:44:03.000 --> 00:44:05.000
 Like what kind of energy would I give?

00:44:05.000 --> 00:44:07.000
 It'd be weird if you didn't have this avatar.

00:44:07.000 --> 00:44:09.000
 You've had this avatar as long as I've known.

00:44:09.000 --> 00:44:10.000
 No, I think I'm never going to change it.

00:44:10.000 --> 00:44:11.000
 I do have a.

00:44:11.000 --> 00:44:12.000
 I did.

00:44:12.000 --> 00:44:13.000
 Okay.

00:44:13.000 --> 00:44:15.920
 So when I started being more public, I made a rule to myself, which is I'm going to pick

00:44:15.920 --> 00:44:17.920
 an avatar and I'm never going to change it.

00:44:17.920 --> 00:44:19.920
 Because when you change it, like.

00:44:19.920 --> 00:44:21.920
 No, no, no, no, no, no, no, no, no.

00:44:21.920 --> 00:44:22.920
 People.

00:44:22.920 --> 00:44:23.920
 Yeah.

00:44:23.920 --> 00:44:24.920
 I've changed.

00:44:24.920 --> 00:44:25.920
 I'm just committed to this forever.

00:44:25.920 --> 00:44:29.920
 And to be honest, I think my avatar, even if I'm like 30 years older, I think it still

00:44:29.920 --> 00:44:30.920
 works.

00:44:30.920 --> 00:44:31.920
 It'll still work.

00:44:31.920 --> 00:44:33.920
 How did you make that, by the way?

00:44:33.920 --> 00:44:34.920
 It was actually a gift.

00:44:34.920 --> 00:44:37.920
 Someone gave me a water color.

00:44:37.920 --> 00:44:40.920
 Someone knew someone that did water colors.

00:44:40.920 --> 00:44:41.920
 And for.

00:44:41.920 --> 00:44:42.920
 I forgot what it was.

00:44:42.920 --> 00:44:44.920
 It might have been for Valentine's Day or something.

00:44:44.920 --> 00:44:49.920
 They gave us me and Liz a water color of like me and her together.

00:44:49.920 --> 00:44:51.920
 And that's just like my head.

00:44:51.920 --> 00:44:52.920
 That's cool.

00:44:52.920 --> 00:44:57.920
 So Liz could have one that's of the other part of the water color.

00:44:57.920 --> 00:44:59.920
 I think she should.

00:44:59.920 --> 00:45:00.920
 But I think she should.

00:45:00.920 --> 00:45:03.920
 I think it's different with women because like her hair is different now.

00:45:03.920 --> 00:45:05.920
 So like, yeah, that doesn't match.

00:45:05.920 --> 00:45:07.920
 And she feels like a different person.

00:45:07.920 --> 00:45:10.920
 Whereas me, you know, not to worry about that.

00:45:10.920 --> 00:45:11.920
 Yeah.

00:45:11.920 --> 00:45:14.920
 Yeah, I was going to say I could do a water color of you.

00:45:14.920 --> 00:45:15.920
 It's a little easier.

00:45:15.920 --> 00:45:16.920
 That's the long last thing.

00:45:16.920 --> 00:45:17.920
 The average person.

00:45:17.920 --> 00:45:21.920
 Your banner photo on Twitter.

00:45:21.920 --> 00:45:24.920
 Is that like a Zuko kind of.

00:45:24.920 --> 00:45:28.920
 That was like when like AI generate images first came out and I was like.

00:45:28.920 --> 00:45:30.920
 Oh, that's why I generated.

00:45:30.920 --> 00:45:31.920
 Yeah.

00:45:31.920 --> 00:45:33.920
 Well, he's looking right at your head.

00:45:33.920 --> 00:45:35.920
 Like he's pointing right at you.

00:45:35.920 --> 00:45:36.920
 If you notice that.

00:45:36.920 --> 00:45:39.920
 I do want to change it, but I'm very uninspired.

00:45:39.920 --> 00:45:40.920
 I don't know what to do with that.

00:45:40.920 --> 00:45:42.920
 I forget what mine is half the time.

00:45:42.920 --> 00:45:45.920
 Uh, did you realize you've tweeted 15,000 times?

00:45:45.920 --> 00:45:46.920
 Is that true?

00:45:46.920 --> 00:45:49.920
 Uh, how, how bad do you feel about yourself now?

00:45:49.920 --> 00:45:51.920
 I don't think about all the tweets.

00:45:51.920 --> 00:45:52.920
 I feel good.

00:45:52.920 --> 00:45:53.920
 Okay.

00:45:53.920 --> 00:45:54.920
 That makes me feel good.

00:45:54.920 --> 00:45:57.920
 It's a lot of thoughts that I have documented that I can go back and read.

00:45:57.920 --> 00:45:58.920
 You crank them out.

00:45:58.920 --> 00:46:02.920
 That's why we can always talk about tweets of yours because you always have new tweets to talk about.

00:46:02.920 --> 00:46:03.920
 Yeah.

00:46:03.920 --> 00:46:04.920
 The CO2 one today was good.

00:46:04.920 --> 00:46:05.920
 Oh, what did you do with the sentry?

00:46:05.920 --> 00:46:06.920
 Well, what's his name?

00:46:06.920 --> 00:46:07.920
 I wasn't there.

00:46:07.920 --> 00:46:08.920
 Okay.

00:46:08.920 --> 00:46:09.920
 So that was the other way.

00:46:09.920 --> 00:46:10.920
 It wasn't me that picture.

00:46:10.920 --> 00:46:11.920
 I posted on Twitter.

00:46:11.920 --> 00:46:16.920
 Another photo that ended up in your Slack and then re-hosted.

00:46:16.920 --> 00:46:18.920
 He was in, David was in Toronto.

00:46:18.920 --> 00:46:19.920
 So Jay met up with them.

00:46:19.920 --> 00:46:20.920
 Oh, nice.

00:46:20.920 --> 00:46:21.920
 Yeah.

00:46:21.920 --> 00:46:22.920
 A few of them.

00:46:22.920 --> 00:46:23.920
 David, Armin, Ben.

00:46:23.920 --> 00:46:25.920
 Uh, what's, what's the planet scale insights feature?

00:46:25.920 --> 00:46:26.920
 Oh, man.

00:46:26.920 --> 00:46:27.920
 This is awesome.

00:46:27.920 --> 00:46:28.920
 Someone just asked about it.

00:46:28.920 --> 00:46:29.920
 So I was going to, I can talk about it.

00:46:29.920 --> 00:46:32.920
 Someone asked, how long have I had my plant scale recommendation?

00:46:32.920 --> 00:46:36.920
 I've been using it for over a year now and I've just always had a great time with it.

00:46:36.920 --> 00:46:38.920
 So I've been recommending it since then, I guess.

00:46:38.920 --> 00:46:39.920
 Uh, yeah.

00:46:39.920 --> 00:46:45.920
 They, they turned it on for my account last week and it basically, huh?

00:46:45.920 --> 00:46:46.920
 Flex.

00:46:46.920 --> 00:46:47.920
 Nice.

00:46:47.920 --> 00:46:48.920
 Nice flex.

00:46:48.920 --> 00:46:49.920
 Yeah.

00:46:49.920 --> 00:46:50.920
 I got it early.

00:46:50.920 --> 00:46:53.920
 I got insights early to my, to my, my sequel database coveted.

00:46:53.920 --> 00:46:56.520
 Everyone should feel bad.

00:46:56.520 --> 00:46:57.520
 So I continue.

00:46:57.520 --> 00:46:58.520
 Yeah.

00:46:58.520 --> 00:46:59.520
 It's actually a very straightforward feature.

00:46:59.520 --> 00:47:04.080
 I'm sure it's like complicated to do like a lot of little details, but it's straightforward

00:47:04.080 --> 00:47:05.880
 in the sense that it's obvious.

00:47:05.880 --> 00:47:07.600
 They can see all of your queries happening.

00:47:07.600 --> 00:47:08.760
 They can see how long they take.

00:47:08.760 --> 00:47:10.760
 They can see how many rows it scans.

00:47:10.760 --> 00:47:15.080
 Uh, and it basically gives you recommendations of being like, Hey, you're querying by this

00:47:15.080 --> 00:47:16.080
 field a lot.

00:47:16.080 --> 00:47:17.080
 Consider adding an index to it.

00:47:17.080 --> 00:47:19.760
 Because you're currently scanning a lot of rows and this is like a frequent career you're

00:47:19.760 --> 00:47:20.760
 doing.

00:47:20.760 --> 00:47:22.680
 And you got like Aaron sitting over your shoulder.

00:47:22.680 --> 00:47:23.680
 Just like.

00:47:23.680 --> 00:47:24.680
 Exactly.

00:47:24.680 --> 00:47:25.680
 Hey, people are joking.

00:47:25.680 --> 00:47:28.280
 It's just him, like typing out recommendations for everyone.

00:47:28.280 --> 00:47:32.280
 He's manually going through all the queries and like, oh, found one.

00:47:32.280 --> 00:47:33.800
 But there's things that it caught.

00:47:33.800 --> 00:47:36.920
 There's one thing that it caught that was like a really stupid mistake.

00:47:36.920 --> 00:47:40.960
 Um, like most of my tables are the same and they're queried in the same way.

00:47:40.960 --> 00:47:42.840
 But there was one table that was slightly different.

00:47:42.840 --> 00:47:43.840
 It was queried in a different way.

00:47:43.840 --> 00:47:47.960
 So I forgot to put like, I need a secondary index on, on a field, but that was like the

00:47:47.960 --> 00:47:48.960
 way I queried it.

00:47:48.960 --> 00:47:53.760
 Uh, and I totally didn't notice and it was, it was like one of the most frequent queries

00:47:53.760 --> 00:47:56.120
 that happened in my system and it called it out.

00:47:56.120 --> 00:47:57.200
 It was like, here's a table here.

00:47:57.200 --> 00:47:58.200
 What's going on?

00:47:58.200 --> 00:48:00.520
 Here's a recommended index that you can add.

00:48:00.520 --> 00:48:02.920
 Uh, it didn't match my naming scheme.

00:48:02.920 --> 00:48:06.560
 So I didn't, you can just like click up there and like update your schema right there.

00:48:06.560 --> 00:48:09.840
 Oh, wait, hang on.

00:48:09.840 --> 00:48:16.840
 So planet scale, how do, how do you, how do you codify your schema?

00:48:16.840 --> 00:48:20.480
 That's why you would never use that feature because it doesn't make cause I just saw the

00:48:20.480 --> 00:48:21.480
 truth for that.

00:48:21.480 --> 00:48:22.480
 Yeah.

00:48:22.480 --> 00:48:23.480
 Yeah.

00:48:23.480 --> 00:48:24.880
 Well, my sort of truth for that is a drizzle definition.

00:48:24.880 --> 00:48:29.960
 So I went updated the drizzle definition and then, uh, did my normal kind of reference,

00:48:29.960 --> 00:48:30.960
 what they say.

00:48:30.960 --> 00:48:32.160
 And you kind of know where you should.

00:48:32.160 --> 00:48:33.160
 Yeah.

00:48:33.160 --> 00:48:34.160
 Yeah.

00:48:34.160 --> 00:48:39.000
 I did it through drizzle, even though the index I added wasn't exactly the same as what

00:48:39.000 --> 00:48:42.800
 they recommended, they understand that it fixes that problem.

00:48:42.800 --> 00:48:43.800
 Yeah.

00:48:43.800 --> 00:48:48.440
 And they automatically detect it and they close the issue, uh, which is amazing.

00:48:48.440 --> 00:48:49.440
 And then that's amazing.

00:48:49.440 --> 00:48:50.440
 Yeah.

00:48:50.440 --> 00:48:53.200
 And I found a bunch of others where like I had extra indexes from stuff I was playing

00:48:53.200 --> 00:48:55.000
 with that I forgot to remove.

00:48:55.000 --> 00:48:56.640
 Um, so it's great.

00:48:56.640 --> 00:49:01.320
 It just like, this isn't even AI, but it gives you that same feeling where it's because

00:49:01.320 --> 00:49:06.440
 it's just complete heuristic base, but he's that same feeling where you don't have the

00:49:06.440 --> 00:49:09.720
 time to like babysit the stuff and it can stuff can easily fall through the cracks.

00:49:09.720 --> 00:49:10.720
 Yeah.

00:49:10.720 --> 00:49:11.720
 But it just does it for you.

00:49:11.720 --> 00:49:16.920
 Like, yeah, you can give people good observability tools to find these issues, but like just

00:49:16.920 --> 00:49:18.680
 find it for them and then just tell them.

00:49:18.680 --> 00:49:19.680
 Yeah.

00:49:19.680 --> 00:49:20.880
 And just suggest here's how you fix it.

00:49:20.880 --> 00:49:21.880
 Yeah.

00:49:21.880 --> 00:49:23.240
 I wish more tools did that.

00:49:23.240 --> 00:49:24.240
 That's awesome.

00:49:24.240 --> 00:49:25.240
 It's great.

00:49:25.240 --> 00:49:26.240
 And I'm just like, why hasn't RDS shipped this?

00:49:26.240 --> 00:49:31.600
 They've been dominating sequel deployments for so long now.

00:49:31.600 --> 00:49:32.600
 Yeah.

00:49:32.600 --> 00:49:37.800
 I have all the data started to expose like slow query stuff, but I'm sure it's not fun.

00:49:37.800 --> 00:49:43.280
 I'm not used it, but like, do what do you have to go to cloud watch or something?

00:49:43.280 --> 00:49:44.280
 It's huge pain.

00:49:44.280 --> 00:49:49.360
 It's like, yeah, it's like even even turning on that feature is a whole set of steps.

00:49:49.360 --> 00:49:51.560
 But yeah, plant scale is great to continue to.

00:49:51.560 --> 00:49:53.120
 This is another one because it might tweet.

00:49:53.120 --> 00:49:55.360
 I was like, this is another sticky feature.

00:49:55.360 --> 00:50:00.360
 I call it out in that way because I'm also really into them as a business because I feel

00:50:00.360 --> 00:50:04.760
 like they do things in a way that makes a lot of sense.

00:50:04.760 --> 00:50:09.960
 Like when they first launched their big thing was a whole merge request flow.

00:50:09.960 --> 00:50:12.520
 And these are features where they're very different.

00:50:12.520 --> 00:50:15.960
 You probably haven't seen it before, but like as soon as you try, you're just like, fuck,

00:50:15.960 --> 00:50:17.520
 I never want to give this up.

00:50:17.520 --> 00:50:21.640
 And that's great as like a dev tool business to really like focus on like really small

00:50:21.640 --> 00:50:26.760
 things like that, that once you experience it, you'll bend over backwards to like, you

00:50:26.760 --> 00:50:29.400
 know, keep having that in your workflows.

00:50:29.400 --> 00:50:30.400
 This is another one.

00:50:30.400 --> 00:50:34.480
 Like I would say this is their second one where I'm like, okay, like it would be hard

00:50:34.480 --> 00:50:35.480
 for me to get this up now.

00:50:35.480 --> 00:50:36.480
 Yeah.

00:50:36.480 --> 00:50:39.880
 I want to talk about databases for a second because I'm like the data API came out for

00:50:39.880 --> 00:50:41.520
 serverless V2.

00:50:41.520 --> 00:50:43.440
 I think we talked about it at one point.

00:50:43.440 --> 00:50:44.440
 Yeah.

00:50:44.440 --> 00:50:46.240
 How do you have you like embraced that?

00:50:46.240 --> 00:50:51.000
 Are you still just to is plant scale too sticky and you can't leave?

00:50:51.000 --> 00:50:54.760
 Is there anything that you've discovered with data API that like, oh, it's not as good

00:50:54.760 --> 00:50:58.580
 as I thought it was on V2 or how do you feel about that?

00:50:58.580 --> 00:51:05.920
 We use it a little bit for our movies demo because like the vector database was backed

00:51:05.920 --> 00:51:06.920
 by that.

00:51:06.920 --> 00:51:07.920
 Yeah.

00:51:07.920 --> 00:51:08.920
 It worked.

00:51:08.920 --> 00:51:09.920
 There's no issues.

00:51:09.920 --> 00:51:12.480
 And I think the drizzle adapter for it works fine.

00:51:12.480 --> 00:51:15.520
 There's some like weird quirks where it can't do certain data types, which I don't fully

00:51:15.520 --> 00:51:22.620
 understand that we've just seen before and they're still present in this in this version.

00:51:22.620 --> 00:51:26.640
 Also, they only do Postgres right now, I think, like the MySQL version.

00:51:26.640 --> 00:51:27.640
 Oh, right.

00:51:27.640 --> 00:51:29.720
 They haven't they haven't rolled it out for MySQL.

00:51:29.720 --> 00:51:30.720
 Okay.

00:51:30.720 --> 00:51:35.400
 So that makes sense why you would still you'd be in plant scale until that at least one

00:51:35.400 --> 00:51:39.360
 of the next things we have to do is make our console self-hostible so that would mean

00:51:39.360 --> 00:51:44.800
 conditionally swapping out plant scale for RDS, but we're kind of waiting for my SQL support

00:51:44.800 --> 00:51:45.800
 in the data.

00:51:45.800 --> 00:51:46.800
 Yeah.

00:51:46.800 --> 00:51:47.800
 Yeah.

00:51:47.800 --> 00:51:48.800
 That makes sense.

00:51:48.800 --> 00:51:49.800
 But yeah.

00:51:49.800 --> 00:51:50.800
 Okay.

00:51:50.800 --> 00:51:55.200
 Like there's some things the plant scale model is it's just a model and there's like trade-offs

00:51:55.200 --> 00:51:56.200
 and negatives with it.

00:51:56.200 --> 00:51:59.360
 So yeah, they have a very traditional deployment.

00:51:59.360 --> 00:52:06.600
 You have a MySQL instance accessing data on disk that's local to the instance and to

00:52:06.600 --> 00:52:11.200
 scale it, you like shard potentially if you like need to scale it that way and you have

00:52:11.200 --> 00:52:18.440
 these live instances, whereas Aurora and neon do that thing where they separate everyone

00:52:18.440 --> 00:52:19.440
 loves this phrase.

00:52:19.440 --> 00:52:21.320
 We separate compute from storage.

00:52:21.320 --> 00:52:22.320
 Yeah.

00:52:22.320 --> 00:52:26.760
 So like the storage layer is completely independent and that can scale and replicate on its own

00:52:26.760 --> 00:52:32.040
 outside of like the MySQL or Postgres replication protocol.

00:52:32.040 --> 00:52:36.260
 And they can just spin up compute as needed to run your queries and there's some benefits

00:52:36.260 --> 00:52:40.680
 to this because like the whole auto scaling thing can work a lot better under that model.

00:52:40.680 --> 00:52:45.120
 That's why like serverless V2, you know, can scale up and down.

00:52:45.120 --> 00:52:49.840
 There's it's a little bit BS because when it scales up, its caches are cold.

00:52:49.840 --> 00:52:53.300
 So you kind of have this cold start thing that you probably don't even realize you have

00:52:53.300 --> 00:52:55.760
 because it's like hard to observe.

00:52:55.760 --> 00:52:59.440
 But technically the separation of storage and compute is better for that type of thing.

00:52:59.440 --> 00:53:03.640
 So if you're like, if you're adding an index on a giant database, you need a ton of compute

00:53:03.640 --> 00:53:07.160
 to compute it, it can like scale up just to add the index and then scale back down.

00:53:07.160 --> 00:53:09.200
 So there's cool stuff like that that RDS can do.

00:53:09.200 --> 00:53:10.200
 Yeah.

00:53:10.200 --> 00:53:13.560
 So it's a different model, but there's a lot about the plan scale model that's simpler

00:53:13.560 --> 00:53:17.720
 plus all these like nice features they threw up through on top.

00:53:17.720 --> 00:53:18.720
 Okay.

00:53:18.720 --> 00:53:20.040
 Can I talk about health insurance?

00:53:20.040 --> 00:53:21.040
 Yeah, please.

00:53:21.040 --> 00:53:25.840
 Or generally health care in America, I'm not trying not to make this too ranty because

00:53:25.840 --> 00:53:29.840
 sometimes I get a little fired up over some of this stuff, but it's been almost a year

00:53:29.840 --> 00:53:32.320
 since I canceled my health insurance.

00:53:32.320 --> 00:53:35.280
 And I don't know if I've ever talked about this on the podcast.

00:53:35.280 --> 00:53:37.880
 I've definitely talked about it on the internet.

00:53:37.880 --> 00:53:38.880
 I just canceled it.

00:53:38.880 --> 00:53:42.120
 I was just wondering I was like, this is stupid.

00:53:42.120 --> 00:53:47.520
 Not even just the money, but just like the healthcare system in America is stupid.

00:53:47.520 --> 00:53:49.600
 And I don't want to be involved with it.

00:53:49.600 --> 00:53:51.160
 So it was kind of like going off the grid.

00:53:51.160 --> 00:53:57.560
 It was like, I just want to be as separated as I can from what I think is a very broken

00:53:57.560 --> 00:53:59.360
 system.

00:53:59.360 --> 00:54:04.280
 Like the way the billing works and the way health care providers have to charge so much

00:54:04.280 --> 00:54:07.720
 more because the insurance companies will only pay like a certain percentage and all

00:54:07.720 --> 00:54:10.840
 this big game and everybody just knows it is just stupid.

00:54:10.840 --> 00:54:12.600
 So I stopped paying my health insurance.

00:54:12.600 --> 00:54:17.320
 In that time, I picked up Jiu Jitsu and went to the ER twice.

00:54:17.320 --> 00:54:18.320
 Yeah.

00:54:18.320 --> 00:54:20.400
 I've not gone to the ER in my adult life.

00:54:20.400 --> 00:54:25.280
 So in high school, I once had to stay in the hospital overnight for an infection.

00:54:25.280 --> 00:54:30.960
 But since then in my adult life, I hadn't really had any serious medical situations.

00:54:30.960 --> 00:54:35.720
 And I paid for health insurance for those 10 years as an adult and never really used it.

00:54:35.720 --> 00:54:42.320
 And then I gave it up and promptly ended up in the hospital twice.

00:54:42.320 --> 00:54:45.080
 There was something else that's happened health-wise.

00:54:45.080 --> 00:54:47.120
 I've had some knee stuff anyway.

00:54:47.120 --> 00:54:52.200
 It's the most I've been a participant in the health care system.

00:54:52.200 --> 00:54:56.680
 And I don't have health insurance and can't believe how much money I've saved.

00:54:56.680 --> 00:55:02.160
 Even with two emergency room visits, that's been like four months of premiums.

00:55:02.160 --> 00:55:07.000
 And I know insurance is to protect you from the big one.

00:55:07.000 --> 00:55:11.000
 I know people are listening and they're like, yeah, well, you haven't had a terminal illness

00:55:11.000 --> 00:55:16.960
 or some terrible thing that's going to cost all of your net worth or whatever happened.

00:55:16.960 --> 00:55:19.400
 And that's why you get insurance.

00:55:19.400 --> 00:55:20.480
 There's insurance for that though, right?

00:55:20.480 --> 00:55:25.920
 I know I keep looking to look into this, like a catastrophic insurance or something.

00:55:25.920 --> 00:55:26.920
 Yeah.

00:55:26.920 --> 00:55:28.880
 So I don't even have that.

00:55:28.880 --> 00:55:30.880
 I should probably look into it.

00:55:30.880 --> 00:55:35.520
 But I don't know, the health care system's stupid, and it just bothers me.

00:55:35.520 --> 00:55:38.600
 Why does it bother me so much that like- It's really annoying.

00:55:38.600 --> 00:55:40.120
 It's extremely annoying.

00:55:40.120 --> 00:55:47.960
 Part of it is just like we're in such bad health and we spend so much on health care.

00:55:47.960 --> 00:55:52.080
 And it just feels like that dichotomy, like as a country, I'm speaking specifically to

00:55:52.080 --> 00:55:57.080
 the US, like, I mean, I guess that's the whole thing with insurance is you spread around

00:55:57.080 --> 00:55:58.080
 the risk.

00:55:58.080 --> 00:56:01.320
 The risk of like dying of heart disease in America is super high.

00:56:01.320 --> 00:56:07.080
 So I don't want to pay that shared collective risk when I feel like there are measures I

00:56:07.080 --> 00:56:13.480
 can take that can prevent or lower my risk to like avoidable diseases.

00:56:13.480 --> 00:56:14.480
 Okay.

00:56:14.480 --> 00:56:17.480
 Now I'm just going to stop because I've not slept enough and I don't know where I'm headed

00:56:17.480 --> 00:56:18.480
 with this.

00:56:18.480 --> 00:56:19.480
 No, but okay.

00:56:19.480 --> 00:56:23.400
 So, but on this topic, so I do have health insurance.

00:56:23.400 --> 00:56:26.240
 I do pay for it myself.

00:56:26.240 --> 00:56:30.740
 Every year, when we have to, you know, sign up for it, we go to the same process where

00:56:30.740 --> 00:56:32.520
 we're like, okay, maybe things have changed.

00:56:32.520 --> 00:56:33.520
 Like what's going on?

00:56:33.520 --> 00:56:34.520
 We do the research.

00:56:34.520 --> 00:56:35.520
 We do the thing.

00:56:35.520 --> 00:56:40.080
 And we always pick the lowest premium option because we don't need.

00:56:40.080 --> 00:56:44.840
 I don't need any of the stuff that they try to build around it is need the most basic.

00:56:44.840 --> 00:56:46.320
 I will pay for everything myself.

00:56:46.320 --> 00:56:47.320
 Yeah.

00:56:47.320 --> 00:56:50.880
 Give me some number if I go over or you take care of for something catastrophic.

00:56:50.880 --> 00:56:51.880
 That's all I need.

00:56:51.880 --> 00:56:57.760
 It's always so annoying because that even for that, the price of that has doubled since

00:56:57.760 --> 00:57:02.320
 I started paying for it like years ago and I was like, you know, I was too old to be

00:57:02.320 --> 00:57:03.320
 on my parents.

00:57:03.320 --> 00:57:04.320
 Yep.

00:57:04.320 --> 00:57:05.320
 It's doubled.

00:57:05.320 --> 00:57:09.480
 And they keep like trying to like kind of like make it like real insurance, but I don't

00:57:09.480 --> 00:57:10.480
 really want that.

00:57:10.480 --> 00:57:14.360
 There's like no such, there's no thing out there where it's like they just do the most

00:57:14.360 --> 00:57:15.360
 bare bones stuff.

00:57:15.360 --> 00:57:17.360
 Like they don't try to give you a network.

00:57:17.360 --> 00:57:18.880
 They don't try to do any of that.

00:57:18.880 --> 00:57:22.760
 They give you literally zero and they just try to minimize previous, like it doesn't,

00:57:22.760 --> 00:57:23.760
 that doesn't exist.

00:57:23.760 --> 00:57:27.360
 And the problems with our healthcare system, because like people are always like, Oh yeah,

00:57:27.360 --> 00:57:30.240
 it should just be, you know, public, like other places.

00:57:30.240 --> 00:57:32.840
 Like there should be a government option.

00:57:32.840 --> 00:57:38.000
 The thing is like, I hate when people like don't look at problems specifically.

00:57:38.000 --> 00:57:39.000
 It's kind of look at generally.

00:57:39.000 --> 00:57:42.120
 They're just like, Oh, other companies are have like a government option.

00:57:42.120 --> 00:57:43.320
 So it'll work here.

00:57:43.320 --> 00:57:47.200
 But if you look at some of the details, we have like a mess up system in so many little

00:57:47.200 --> 00:57:51.820
 ways that making a government option, I can easily see how that's just going to make things

00:57:51.820 --> 00:57:53.720
 even worse.

00:57:53.720 --> 00:57:59.280
 If you look at a per capita spending, I think we literally outspend the next country by

00:57:59.280 --> 00:58:01.380
 two X on healthcare costs.

00:58:01.380 --> 00:58:05.520
 So we're already spending a crazy amount per person and we're not, we don't necessarily

00:58:05.520 --> 00:58:08.880
 have like double the outcomes.

00:58:08.880 --> 00:58:12.720
 It's also really complicated because a lot of the stuff gets invented here.

00:58:12.720 --> 00:58:17.720
 And like, you know, the whole world gets a benefit.

00:58:17.720 --> 00:58:21.640
 There's also this dynamic where hospitals have consolidated like crazy.

00:58:21.640 --> 00:58:26.680
 That's not a very competitive market and the prices have gone up because that's, there's

00:58:26.680 --> 00:58:28.360
 laws that are out there that make it.

00:58:28.360 --> 00:58:32.800
 So if you want to start a hospital, you have to prove that you are not competitive with

00:58:32.800 --> 00:58:33.800
 an existing one.

00:58:33.800 --> 00:58:34.800
 It's like complete, completely backwards.

00:58:34.800 --> 00:58:36.280
 You've mentioned that to me before.

00:58:36.280 --> 00:58:37.280
 Yeah.

00:58:37.280 --> 00:58:40.720
 So there's like, oh, there's like a whole cluster of problems.

00:58:40.720 --> 00:58:45.400
 And yeah, at this point, like I think what has gotten better is I think providers have

00:58:45.400 --> 00:58:48.360
 recognized that people just want to opt out of insurance.

00:58:48.360 --> 00:58:53.640
 And there are like great doctors that have like somewhat reasonable prices that will

00:58:53.640 --> 00:58:57.400
 treat you without insurance and they have an upfront, there's what it costs.

00:58:57.400 --> 00:59:02.160
 You pay me, you know, but yeah, you still, I still feel like I need that disaster insurance

00:59:02.160 --> 00:59:03.160
 just in case.

00:59:03.160 --> 00:59:04.160
 Yeah.

00:59:04.160 --> 00:59:06.000
 The cost of that is way higher than it needs to be.

00:59:06.000 --> 00:59:07.640
 It's really annoying.

00:59:07.640 --> 00:59:10.440
 And for you, like, you have a family of four or so it's probably like thousands.

00:59:10.440 --> 00:59:11.440
 Oh, yeah.

00:59:11.440 --> 00:59:12.440
 It'll be, it'll be a lot.

00:59:12.440 --> 00:59:13.440
 Yeah.

00:59:13.440 --> 00:59:14.440
 Yeah.

00:59:14.440 --> 00:59:15.440
 I don't know.

00:59:15.440 --> 00:59:19.040
 It frustrates me every time I think about it or especially if I talk about it.

00:59:19.040 --> 00:59:20.040
 Yeah.

00:59:20.040 --> 00:59:23.240
 So people are trying to say I stopped my forum and half and it was like $10,000 without

00:59:23.240 --> 00:59:24.240
 insurance.

00:59:24.240 --> 00:59:25.240
 It would have been a hundred thousand.

00:59:25.240 --> 00:59:26.640
 So here's like another stupid thing.

00:59:26.640 --> 00:59:31.480
 Like if you go to the hospital and you ask, what's the cost with insurance?

00:59:31.480 --> 00:59:32.480
 They say 10k.

00:59:32.480 --> 00:59:33.480
 What's the cost without insurance?

00:59:33.480 --> 00:59:34.480
 Do they say 100k?

00:59:34.480 --> 00:59:37.760
 The 10k, your insurance is not paying the other 90k.

00:59:37.760 --> 00:59:41.920
 It's just that's the insurance rate that they've negotiated.

00:59:41.920 --> 00:59:42.920
 Yeah.

00:59:42.920 --> 00:59:44.960
 When they say you have to pay 100k, you don't.

00:59:44.960 --> 00:59:49.600
 You just tell them you get treated and you're like, I'm not going to pay 100k.

00:59:49.600 --> 00:59:51.160
 They will make you pay 10k.

00:59:51.160 --> 00:59:53.960
 So it's just, it's just annoying for a stupid reason.

00:59:53.960 --> 00:59:57.880
 And we had this with, uh, it was got her gallbladder out and they get, it caught us like

00:59:57.880 --> 01:00:00.320
 insane number, like a hundred and something thousand dollars.

01:00:00.320 --> 01:00:04.960
 Uh, and we just said, we were just like, no, you can do funny things too.

01:00:04.960 --> 01:00:09.360
 You can, uh, you can send, if they're, if they keep sending them a bill, you can send

01:00:09.360 --> 01:00:12.080
 them $1 every time they send you a bill.

01:00:12.080 --> 01:00:17.080
 And that like resets whatever thing that they have and they eventually give up.

01:00:17.080 --> 01:00:18.080
 It's so stupid.

01:00:18.080 --> 01:00:19.080
 But you know what?

01:00:19.080 --> 01:00:20.880
 It's a stupid system and you got to act stupid in it.

01:00:20.880 --> 01:00:22.400
 I guess that's what it is.

01:00:22.400 --> 01:00:23.400
 It's a stupid system.

01:00:23.400 --> 01:00:27.640
 You just got to act stupid to not go crazy because, oh my word.

01:00:27.640 --> 01:00:28.640
 Yeah.

01:00:28.640 --> 01:00:33.160
 You've got to play a stupid game with the hospitals where these procedures are not expensive.

01:00:33.160 --> 01:00:37.320
 I mean, they're expensive, but not like outrageously expensive.

01:00:37.320 --> 01:00:38.320
 Yeah.

01:00:38.320 --> 01:00:42.360
 I mean, don't they have to charge, like they have to charge insurance companies this huge

01:00:42.360 --> 01:00:47.840
 rate because the insurance companies only pay like a fraction of what they bill.

01:00:47.840 --> 01:00:49.560
 So it's just this game where they keep billing.

01:00:49.560 --> 01:00:50.560
 Yeah.

01:00:50.560 --> 01:00:52.640
 I mean, it's, it's really, it's bad for everyone.

01:00:52.640 --> 01:00:56.840
 The hospital is like any service provider that takes insurance, you literally have no

01:00:56.840 --> 01:01:00.120
 idea what you're going to get paid when you treat a patient.

01:01:00.120 --> 01:01:01.400
 It's like so confusing.

01:01:01.400 --> 01:01:02.400
 Yeah.

01:01:02.400 --> 01:01:06.120
 Liz's uncle, he owns a giant healthcare business in Miami, like billboards everywhere.

01:01:06.120 --> 01:01:07.800
 Like everyone knows what it is.

01:01:07.800 --> 01:01:09.840
 It's crazy how big of an operation this is.

01:01:09.840 --> 01:01:13.720
 And he treats like, it's like a network of doctors and pharmacies and like everything

01:01:13.720 --> 01:01:16.840
 you need for someone elderly to like take care of all of their needs.

01:01:16.840 --> 01:01:17.840
 Yeah.

01:01:17.840 --> 01:01:21.120
 It's even gone to a point where they have like recreational stuff and it's like a whole

01:01:21.120 --> 01:01:22.120
 community.

01:01:22.120 --> 01:01:23.120
 But it's massive.

01:01:23.120 --> 01:01:26.320
 They have like so many, so many people enrolled in this.

01:01:26.320 --> 01:01:28.120
 It's a giant complicated business.

01:01:28.120 --> 01:01:31.360
 Like they put all this together over the last like 30 years.

01:01:31.360 --> 01:01:32.960
 So much work.

01:01:32.960 --> 01:01:36.280
 They make like $70 a patient per year.

01:01:36.280 --> 01:01:39.720
 It's something like in net, that's how that's what they get.

01:01:39.720 --> 01:01:40.720
 Yeah.

01:01:40.720 --> 01:01:46.840
 And they're facing an issue right now because some middleman somewhere increased something

01:01:46.840 --> 01:01:48.320
 by like roughly that amount.

01:01:48.320 --> 01:01:49.320
 Oh my word.

01:01:49.320 --> 01:01:50.840
 Now they're making like near zero.

01:01:50.840 --> 01:01:51.840
 Yeah.

01:01:51.840 --> 01:01:58.080
 So this big complicated, difficult business that he spun up just for like, this small,

01:01:58.080 --> 01:01:59.560
 I mean, he makes a lot of money.

01:01:59.560 --> 01:02:03.480
 I mean, they've made a lot of money because they're so big, but yeah.

01:02:03.480 --> 01:02:06.240
 It's just crazy how complicated it is compared to anything that I've ever done.

01:02:06.240 --> 01:02:07.240
 Yeah.

01:02:07.240 --> 01:02:08.640
 We're spoiled by the software margins.

01:02:08.640 --> 01:02:09.640
 Yeah.

01:02:09.640 --> 01:02:12.880
 Or at least what used to be software margins.

01:02:12.880 --> 01:02:13.880
 I don't know.

01:02:13.880 --> 01:02:14.880
 Maybe those are gone.

01:02:14.880 --> 01:02:15.880
 You're going away.

01:02:15.880 --> 01:02:16.880
 For sure.

01:02:16.880 --> 01:02:17.880
 Yeah.

01:02:17.880 --> 01:02:18.880
 Are interest rates climbing again?

01:02:18.880 --> 01:02:19.880
 Are they?

01:02:19.880 --> 01:02:20.880
 Are they?

01:02:20.880 --> 01:02:22.720
 Are they dropping interest rates?

01:02:22.720 --> 01:02:23.720
 Did I hear that?

01:02:23.720 --> 01:02:27.920
 They definitely committed to pausing hikes for a bit.

01:02:27.920 --> 01:02:30.400
 Pausing hikes, which is as good as lowering.

01:02:30.400 --> 01:02:31.400
 Yeah.

01:02:31.400 --> 01:02:32.400
 Some people think.

01:02:32.400 --> 01:02:33.400
 And a lot of people's eyes.

01:02:33.400 --> 01:02:37.080
 Some people think they might be, they might be forced to raise again, depending on how

01:02:37.080 --> 01:02:38.080
 pausing goes.

01:02:38.080 --> 01:02:45.000
 But you can see the market, they like the market has basically priced in rate lowering.

01:02:45.000 --> 01:02:47.600
 I think so because my portfolio is not over here.

01:02:47.600 --> 01:02:48.600
 Which is amazing.

01:02:48.600 --> 01:02:49.600
 Interesting.

01:02:49.600 --> 01:02:53.080
 It's all tied to the markets.

01:02:53.080 --> 01:02:54.080
 Yeah.

01:02:54.080 --> 01:02:55.080
 I feel great.

01:02:55.080 --> 01:03:00.720
 And I need the rates to go down so I can find someone that's buying my New York apartments.

01:03:00.720 --> 01:03:02.560
 I can buy the house that I'm moving into.

01:03:02.560 --> 01:03:03.560
 Oh, yeah.

01:03:03.560 --> 01:03:04.560
 Yeah, you're stuck with that situation.

01:03:04.560 --> 01:03:07.160
 My parents are trying to sell their house and they've had it for a while.

01:03:07.160 --> 01:03:08.640
 I haven't even tried selling it.

01:03:08.640 --> 01:03:09.640
 I'm going to list it.

01:03:09.640 --> 01:03:13.600
 I'm going to list my apartment this summer and I'm going to see if someone buys it.

01:03:13.600 --> 01:03:14.600
 I just can't.

01:03:14.600 --> 01:03:19.480
 I like can't fathom who would buy this, but I can't fathom who would buy a lot of things

01:03:19.480 --> 01:03:20.880
 and then they always sell.

01:03:20.880 --> 01:03:21.880
 Yeah.

01:03:21.880 --> 01:03:22.880
 Maybe a fan of the podcast.

01:03:22.880 --> 01:03:24.880
 Maybe somebody would be like, that's Dax's house.

01:03:24.880 --> 01:03:25.880
 Yeah.

01:03:25.880 --> 01:03:33.160
 If you want to repay an apartment in New York City in a great location, really quiet location.

01:03:33.160 --> 01:03:37.880
 Not the location you probably would think to go to, but it's a great location.

01:03:37.880 --> 01:03:38.880
 Let me know.

01:03:38.880 --> 01:03:39.880
 I have an apartment for you.

01:03:39.880 --> 01:03:40.880
 Okay.

01:03:40.880 --> 01:03:44.700
 While we're talking to our listeners, we are at an hour here and I need to get off here

01:03:44.700 --> 01:03:45.700
 before.

01:03:45.700 --> 01:03:46.700
 I just need to stop talking.

01:03:46.700 --> 01:03:49.800
 Two monitors getting out of hand, it's 1360.

01:03:49.800 --> 01:03:55.040
 I'm going to die over here and I don't have health insurance, but if you're listening

01:03:55.040 --> 01:04:00.080
 to this podcast still after an hour and whatever it's been, maybe hop into Apple Podcasts,

01:04:00.080 --> 01:04:01.080
 leave us a rating.

01:04:01.080 --> 01:04:02.560
 We don't get a lot of ratings.

01:04:02.560 --> 01:04:06.160
 I think it's just to ask for them and maybe we'll get more if I ask for them.

01:04:06.160 --> 01:04:09.520
 And I don't even honestly know what happens if you give them to us, but it makes you feel

01:04:09.520 --> 01:04:10.520
 great.

01:04:10.520 --> 01:04:11.520
 Because I look at them.

01:04:11.520 --> 01:04:12.520
 Clarify.

01:04:12.520 --> 01:04:13.520
 A good, oh, good rating.

01:04:13.520 --> 01:04:14.520
 Yeah.

01:04:14.520 --> 01:04:15.520
 If you don't like the podcast, why are you still listening?

01:04:15.520 --> 01:04:16.520
 And don't leave us a rating.

01:04:16.520 --> 01:04:20.160
 But, you know, if you want to leave a good one, it warms my heart.

01:04:20.160 --> 01:04:21.600
 I check it every once in a while.

01:04:21.600 --> 01:04:23.760
 We've got some, but not a lot.

01:04:23.760 --> 01:04:28.160
 If you want to leave a review even better, tell us what you like.

01:04:28.160 --> 01:04:30.360
 If you don't like stuff, maybe DM it to us.

01:04:30.360 --> 01:04:36.080
 Don't put it on the public record to actually have anything to add to this meta asking of

01:04:36.080 --> 01:04:37.080
 the listeners.

01:04:37.080 --> 01:04:40.560
 We should put it at the beginning of our podcast.

01:04:40.560 --> 01:04:41.960
 Oh, probably.

01:04:41.960 --> 01:04:44.800
 So that everyone who hears the podcast, because there's probably like 10 people that listen

01:04:44.800 --> 01:04:45.800
 to the end.

01:04:45.800 --> 01:04:46.800
 Yeah.

01:04:46.800 --> 01:04:48.360
 But maybe one of those 10 people.

01:04:48.360 --> 01:04:49.360
 Yeah.

01:04:49.360 --> 01:04:50.360
 That's true.

01:04:50.360 --> 01:04:51.360
 We only have one per episode.

01:04:51.360 --> 01:04:52.360
 A lot of.

01:04:52.360 --> 01:04:53.360
 Oh, yeah.

01:04:53.360 --> 01:04:54.920
 Hey, we've done like 81 episodes.

01:04:54.920 --> 01:04:56.480
 We're coming up on 100.

01:04:56.480 --> 01:04:57.960
 We got to think of something special.

01:04:57.960 --> 01:04:58.960
 I wonder if we could record.

01:04:58.960 --> 01:05:04.280
 You've been saying we've been coming up on 100 ever since we got over 50.

01:05:04.280 --> 01:05:07.400
 I wonder if we could, if it would time out, I could do the math.

01:05:07.400 --> 01:05:08.400
 But could we do?

01:05:08.400 --> 01:05:14.680
 Could we record episode 100 in Miami together when I come from Miami?

01:05:14.680 --> 01:05:15.680
 Probably way off.

01:05:15.680 --> 01:05:16.680
 That'll be like episode 86.

01:05:16.680 --> 01:05:19.280
 Wait, how many weeks is it?

01:05:19.280 --> 01:05:20.280
 It's only a four more weeks.

01:05:20.280 --> 01:05:21.280
 Right.

01:05:21.280 --> 01:05:22.280
 There's not a lot of weeks.

01:05:22.280 --> 01:05:23.280
 Six weeks.

01:05:23.280 --> 01:05:24.280
 Yeah.

01:05:24.280 --> 01:05:25.280
 Something like that.

01:05:25.280 --> 01:05:26.280
 Okay.

01:05:26.280 --> 01:05:28.760
 Well, maybe we'll record episode 100, but not release it until we record the other 15 episodes

01:05:28.760 --> 01:05:30.720
 and then that'll be episode 100.

01:05:30.720 --> 01:05:31.720
 I don't know.

01:05:31.720 --> 01:05:32.720
 We'll see.

01:05:32.720 --> 01:05:34.920
 We can do it in this office or in guess in my new house.

01:05:34.920 --> 01:05:37.520
 In the one that looks just like the night.

01:05:37.520 --> 01:05:38.520
 Yeah.

01:05:38.520 --> 01:05:40.920
 Did you see what I said about the lights in that house?

01:05:40.920 --> 01:05:41.920
 No.

01:05:41.920 --> 01:05:42.920
 So we go.

01:05:42.920 --> 01:05:45.040
 We got the keys yesterday and that they're still fixing some stuff up so we're not like

01:05:45.040 --> 01:05:46.040
 living there yet.

01:05:46.040 --> 01:05:49.560
 But we get the keys and I turn on the faucet in the sink.

01:05:49.560 --> 01:05:53.040
 So I'm like, okay, what's a water pressure like here was good, which I'm happy with.

01:05:53.040 --> 01:05:57.520
 But then the lights start flickering and I'm like, no, no, no, when you turn on the

01:05:57.520 --> 01:05:58.720
 water, I turn off the faucet.

01:05:58.720 --> 01:05:59.720
 The lights stop flickering.

01:05:59.720 --> 01:06:02.400
 I did it like three or four times to really make sure.

01:06:02.400 --> 01:06:03.400
 Oh, no.

01:06:03.400 --> 01:06:04.400
 And it's like strobing.

01:06:04.400 --> 01:06:08.080
 It's like, yeah, like what the hell is this?

01:06:08.080 --> 01:06:11.000
 And yeah, so it has a tankless water heater, which is great.

01:06:11.000 --> 01:06:12.000
 But it's true.

01:06:12.000 --> 01:06:13.000
 Love it.

01:06:13.000 --> 01:06:14.000
 Yeah.

01:06:14.000 --> 01:06:15.000
 That's so much power.

01:06:15.000 --> 01:06:16.000
 Because I think it's new.

01:06:16.000 --> 01:06:17.000
 I've made added that in recently.

01:06:17.000 --> 01:06:19.800
 Oh, so it's not like water touching something.

01:06:19.800 --> 01:06:20.800
 Sure.

01:06:20.800 --> 01:06:22.280
 That's what I was worried about.

01:06:22.280 --> 01:06:23.800
 So how are we going to track that down?

01:06:23.800 --> 01:06:24.800
 How are we going to try them?

01:06:24.800 --> 01:06:25.800
 Like where this is happening?

01:06:25.800 --> 01:06:30.800
 Well, you know, when you see the fire, that's where it was the water.

01:06:30.800 --> 01:06:31.800
 I guess the water.

01:06:31.800 --> 01:06:32.800
 Yeah, that would cost a fire.

01:06:32.800 --> 01:06:33.800
 I don't know.

01:06:33.800 --> 01:06:34.800
 Anything electrical just freaks me out.

01:06:34.800 --> 01:06:37.920
 But then I found the water heater and I turned it down and then it stopped happening.

01:06:37.920 --> 01:06:38.920
 So this is like a thing.

01:06:38.920 --> 01:06:39.920
 I got to go figure out.

01:06:39.920 --> 01:06:42.680
 I probably will have to call an electric electrician and figure it out.

01:06:42.680 --> 01:06:47.200
 So when they put it on the same circuit as the rest of the house, so like having a separate

01:06:47.200 --> 01:06:52.760
 circuit for it, there were some weird fixes that people online had.

01:06:52.760 --> 01:06:57.200
 They like, there was some kind of like ring that you can buy that's like this, it's made

01:06:57.200 --> 01:06:58.400
 out of some like metal.

01:06:58.400 --> 01:07:02.340
 And if you like loop certain wires through in the right order, it'll like stabilize whatever

01:07:02.340 --> 01:07:03.340
 is going on.

01:07:03.340 --> 01:07:04.340
 What?

01:07:04.340 --> 01:07:08.820
 And just like completely this passive thing that I don't know, I got to look into it.

01:07:08.820 --> 01:07:10.520
 That sounds dangerous.

01:07:10.520 --> 01:07:13.960
 Because you know that like electricians get shocked all the time.

01:07:13.960 --> 01:07:18.800
 I was naive and thought like, if you're, if you see an electrician, that means they're

01:07:18.800 --> 01:07:19.800
 really good.

01:07:19.800 --> 01:07:22.880
 They've never died and they've never gotten shocked, but like they get shocked all the

01:07:22.880 --> 01:07:23.880
 time.

01:07:23.880 --> 01:07:24.880
 It doesn't kill you.

01:07:24.880 --> 01:07:25.880
 I would have thought like you touch.

01:07:25.880 --> 01:07:26.880
 I don't know.

01:07:26.880 --> 01:07:27.880
 Just blow it.

01:07:27.880 --> 01:07:28.880
 AC wire.

01:07:28.880 --> 01:07:29.880
 Yeah.

01:07:29.880 --> 01:07:30.880
 In the U.S. and it would just fry your heart.

01:07:30.880 --> 01:07:32.600
 You'd fall over dead, but they get shocked all the time.

01:07:32.600 --> 01:07:38.720
 You know that some people get a magnet embedded in their finger what?

01:07:38.720 --> 01:07:43.920
 And for people that work with electricity a lot, they get, they like they augment themselves

01:07:43.920 --> 01:07:47.000
 by putting a magnet in their finger and they start to be able to feel currents and your

01:07:47.000 --> 01:07:52.080
 brain like integrates that to like a new sense that you have and helps them like quickly

01:07:52.080 --> 01:07:57.920
 realize that wires like a void, that's so cool.

01:07:57.920 --> 01:08:00.200
 It's like spidey senses or something.

01:08:00.200 --> 01:08:01.200
 Exactly.

01:08:01.200 --> 01:08:04.440
 But just for like very one specific scenario, huh?

01:08:04.440 --> 01:08:07.400
 I'm trying to think of how I could get some kind of an implant that would help me with

01:08:07.400 --> 01:08:08.400
 my work.

01:08:08.400 --> 01:08:10.400
 Can't think of anything.

01:08:10.400 --> 01:08:16.600
 I mean, neural link, I guess, I can just think and stuff would happen on my computer.

01:08:16.600 --> 01:08:17.600
 Will you get neural link?

01:08:17.600 --> 01:08:20.520
 If it like becomes a thing where you can just go in and get like an outpatient procedure

01:08:20.520 --> 01:08:22.920
 and have this thing implanted in your brain, would you do it?

01:08:22.920 --> 01:08:23.920
 Yeah.

01:08:23.920 --> 01:08:24.920
 Well, really?

01:08:24.920 --> 01:08:25.920
 Okay.

01:08:25.920 --> 01:08:26.920
 It depends what I get out of it.

01:08:26.920 --> 01:08:30.280
 Like if I'm just like controlling through with my mind, like that's not that interesting.

01:08:30.280 --> 01:08:31.480
 What else would you get out of it?

01:08:31.480 --> 01:08:36.200
 If it's like backing up every like a whole memory I have to somewhere.

01:08:36.200 --> 01:08:40.400
 So then if I die, like restores it into a clone of me, yeah, I'll do it.

01:08:40.400 --> 01:08:41.400
 Oh, interesting.

01:08:41.400 --> 01:08:42.400
 Okay.

01:08:42.400 --> 01:08:45.600
 You weren't expecting that specific of an answer, were you?

01:08:45.600 --> 01:08:46.600
 No, I was.

01:08:46.600 --> 01:08:48.680
 I thought it was just like move your mouse with your brain.

01:08:48.680 --> 01:08:49.680
 I don't know.

01:08:49.680 --> 01:08:50.680
 That's enough for you.

01:08:50.680 --> 01:08:51.680
 Yeah.

01:08:51.680 --> 01:08:53.520
 That's like the end of this technology.

01:08:53.520 --> 01:08:54.520
 Yeah.

01:08:54.520 --> 01:08:55.680
 I thought it was.

01:08:55.680 --> 01:08:56.680
 Okay.

01:08:56.680 --> 01:08:57.680
 Anyway, all right.

01:08:57.680 --> 01:08:58.680
 I'm going to stop talking.

01:08:58.680 --> 01:09:00.680
 So I don't die of carbon dioxide.

01:09:00.680 --> 01:09:01.680
 Oh my gosh.

01:09:01.680 --> 01:09:02.680
 Is that a thing?

01:09:02.680 --> 01:09:03.680
 There's carbon monoxide.

01:09:03.680 --> 01:09:07.360
 We just keep inventing new things to be worth about.

01:09:07.360 --> 01:09:10.600
 I bet I'm going to bring my, when I come to Miami, I'm bringing my monitor.

01:09:10.600 --> 01:09:11.600
 Oh, can you please bring it?

01:09:11.600 --> 01:09:12.600
 I do want to know.

01:09:12.600 --> 01:09:13.600
 I'm going to.

01:09:13.600 --> 01:09:14.600
 I want to know.

01:09:14.600 --> 01:09:15.600
 Your office is probably like 2,500.

01:09:15.600 --> 01:09:17.840
 You're probably just like, that's why your tweets are so dumb.

01:09:17.840 --> 01:09:23.280
 You're like half a brain so let tweets aren't dumb, I just, I can't even be mean.

01:09:23.280 --> 01:09:24.280
 My tweets are working.

01:09:24.280 --> 01:09:25.280
 So I should.

01:09:25.280 --> 01:09:26.280
 No, they're so good.

01:09:26.280 --> 01:09:27.280
 I should.

01:09:27.280 --> 01:09:28.280
 I should.

01:09:28.280 --> 01:09:29.280
 I should.

01:09:29.280 --> 01:09:30.280
 Don't just keep doing it.

01:09:30.280 --> 01:09:31.280
 Keep doing what you damage to my brain.

01:09:31.280 --> 01:09:32.640
 I remember when you had like 400 followers on Twitter.

01:09:32.640 --> 01:09:37.480
 It's so funny like, I remember thinking, this guy's way too smart to not have people listening

01:09:37.480 --> 01:09:38.480
 to him.

01:09:38.480 --> 01:09:39.480
 What's going on?

01:09:39.480 --> 01:09:40.480
 I think you did that one.

01:09:40.480 --> 01:09:41.480
 So that made me feel good.

01:09:41.480 --> 01:09:42.480
 I did.

01:09:42.480 --> 01:09:43.480
 I could, I could pull up the receipts.

01:09:43.480 --> 01:09:44.480
 Yeah.

01:09:44.480 --> 01:09:45.480
 Yeah.

01:09:45.480 --> 01:09:46.480
 And how look at you?

01:09:46.480 --> 01:09:47.780
 Everybody's hanging on every word.

01:09:47.780 --> 01:09:48.780
 What Dax has to say.

01:09:48.780 --> 01:09:55.280
 Anyway, I also remember when like Theo had a thousand followers and I'm trying to think

01:09:55.280 --> 01:09:56.280
 who else trash.

01:09:56.280 --> 01:09:58.120
 You just like a 120 something now.

01:09:58.120 --> 01:09:59.120
 Yeah.

01:09:59.120 --> 01:10:00.120
 I'm trying something thousand.

01:10:00.120 --> 01:10:01.120
 Yeah.

01:10:01.120 --> 01:10:02.120
 I remember when trash just had a couple thousand.

01:10:02.120 --> 01:10:04.160
 You're like, I'm possibly fast.

01:10:04.160 --> 01:10:07.400
 He did crazy fast and like he just never grew again.

01:10:07.400 --> 01:10:08.400
 Yeah.

01:10:08.400 --> 01:10:09.400
 He's hit.

01:10:09.400 --> 01:10:10.400
 Yeah.

01:10:10.400 --> 01:10:11.400
 And sailing with memes.

01:10:11.400 --> 01:10:12.400
 Yeah.

01:10:12.400 --> 01:10:15.320
 Well, I never post is trash coming to Miami.

01:10:15.320 --> 01:10:16.320
 He's coming around.

01:10:16.320 --> 01:10:17.320
 Maybe.

01:10:17.320 --> 01:10:19.760
 No, he keeps every time I ask him, he says the same thing to me.

01:10:19.760 --> 01:10:24.280
 He says Miami is a wasteland or he says, Miami is not good for me.

01:10:24.280 --> 01:10:26.600
 I was like, okay, which is it trash?

01:10:26.600 --> 01:10:28.640
 I mean, I guess both could be true.

01:10:28.640 --> 01:10:32.760
 His name is like trash, so you know, you belong in a wasteland.

01:10:32.760 --> 01:10:35.680
 I will never forget Twitchcon in Vegas.

01:10:35.680 --> 01:10:40.560
 Like the moment he touched down, I think he had like cigarette in the mouth and he was

01:10:40.560 --> 01:10:41.560
 gambling in the airport.

01:10:41.560 --> 01:10:44.400
 I mean, just like immediately turned on Vegas mode.

01:10:44.400 --> 01:10:45.400
 Oh man.

01:10:45.400 --> 01:10:46.400
 Yeah.

01:10:46.400 --> 01:10:48.720
 I thought of something we could do at the end of every episode.

01:10:48.720 --> 01:10:49.720
 What was it?

01:10:49.720 --> 01:10:53.800
 Uh, I don't remember a cheer.

01:10:53.800 --> 01:10:57.080
 No, a cheer, but it was something.

01:10:57.080 --> 01:11:04.280
 I don't know, maybe I'll think of something later, maybe episode 100 will unveil an outro.

01:11:04.280 --> 01:11:08.240
 Does it feel weird to not have everyone see it?

