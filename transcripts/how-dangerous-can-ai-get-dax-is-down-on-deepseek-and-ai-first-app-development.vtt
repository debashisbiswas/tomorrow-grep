WEBVTT

00:00:00.000 --> 00:00:04.000
 You're sick of it. This is our last episode ever. We're not going to do this podcast anymore.

00:00:04.000 --> 00:00:05.520
 Adam doesn't want to talk to me.

00:00:28.880 --> 00:00:34.480
 I was just on the news. I just read about a plane crash, and that's not good.

00:00:34.480 --> 00:00:39.520
 Yeah, I saw it last night and I'm getting on a plane tomorrow, so really bad time, man.

00:00:39.520 --> 00:00:43.760
 Yeah, I guess two things collided in the air. I always worry about that. You always think, like,

00:00:43.760 --> 00:00:47.680
 I don't know, could something just run into the side of us because they didn't know we were here

00:00:47.680 --> 00:00:51.200
 and they didn't look at the radar or whatever. So in our, I don't know.

00:00:51.200 --> 00:00:54.960
 Yeah, the situation, it was kind of crazy. It was right as a plane was landing,

00:00:54.960 --> 00:01:00.640
 and it was a Black Hawk helicopter that was like in the air right over the ground in the airport.

00:01:00.640 --> 00:01:06.720
 Anyway, that's a damper to start out with. How are you?

00:01:06.720 --> 00:01:11.680
 I'm good. It's finally kind of warming up again. Like I went outside with no pants on today,

00:01:11.680 --> 00:01:15.840
 which is good, but I'm still wearing, you know, a long sleeve.

00:01:15.840 --> 00:01:20.560
 Long sleeve shirt. Yeah. I went for a walk this morning outside at 5.30 in the morning,

00:01:21.120 --> 00:01:26.960
 'cause it was 50 degrees here, which is amazing. It's been so cold and it should not be 50

00:01:26.960 --> 00:01:32.960
 before 6 a.m. That's unusual going outside when it's with degrees is really dangerous.

00:01:32.960 --> 00:01:38.080
 You should have seen how I was dressed, my lighter than your dress right now.

00:01:38.080 --> 00:01:43.840
 Well, I'm going to Boston, so I have to, like, go and pack. Oh, no.

00:01:43.840 --> 00:01:50.880
 All my, like, just heavy clothes from New York and my, my, like, mountaineering jacket.

00:01:51.280 --> 00:01:58.000
 That's funny. But what are you doing in Boston?

00:01:58.000 --> 00:02:00.800
 Liz's friend is having an engagement party.

00:02:00.800 --> 00:02:05.280
 So we're going for that, and then I'm going to visit AJ while I'm there.

00:02:05.280 --> 00:02:06.800
 Nice. We're going to hang out on Friday.

00:02:06.800 --> 00:02:11.920
 That's awesome. Yeah. Tell him I said hi, or if he's a listener,

00:02:11.920 --> 00:02:14.800
 hi, AJ, I'm just bypassed Dax. He's a terrible middleman.

00:02:14.800 --> 00:02:18.560
 He's a listener. I'm sure you'll hear this.

00:02:18.560 --> 00:02:21.440
 Yeah. Not so after we hang out. That's true.

00:02:21.440 --> 00:02:26.800
 I've been listening to a lot of stuff. We don't want to talk about AI more, do we?

00:02:26.800 --> 00:02:28.960
 I just listen to talk about whatever we want. Go ahead.

00:02:28.960 --> 00:02:36.320
 Okay. I was just listening to, oh my God.

00:02:36.320 --> 00:02:43.120
 I'm just sorry. This reminded me of this stupid joke,

00:02:43.120 --> 00:02:49.680
 Casey and I've been watching on Netflix called later Daters, and it's like these 50, 60 plus.

00:02:49.680 --> 00:02:57.760
 It's these older people, 50s and 60s dating, like divorces, widowers, etc.

00:02:57.760 --> 00:03:04.320
 And there's this woman who they have a dating coach who seems to know stuff about relationships.

00:03:04.320 --> 00:03:09.760
 And she encourages this lady to open up conversations, break the ice,

00:03:09.760 --> 00:03:15.040
 by talking about a podcast she just listened to. But the woman didn't quite understand,

00:03:15.040 --> 00:03:18.640
 she doesn't seem to grasp the idea that you have to actually talk about the podcast.

00:03:18.640 --> 00:03:22.320
 And she would just open all her dates with, so I was listening to this podcast with Matthew

00:03:22.320 --> 00:03:27.120
 McConaughey. And that's all she would say, like that line. And you have to keep going.

00:03:27.120 --> 00:03:31.680
 It just made me think of it, though, when I said, so I was listening to this podcast.

00:03:31.680 --> 00:03:37.840
 I just wanted to break the ice with you. Yeah, pause. That's all. I was listening to the podcast.

00:03:40.640 --> 00:03:45.520
 I was listening, I've been listening to all of Lex's stuff because Prime's going to be on there.

00:03:45.520 --> 00:03:51.920
 And I just forgot I liked his podcast. So I was listening to some of his back catalog.

00:03:51.920 --> 00:03:55.360
 And he had an anthropic CEO on was super interesting.

00:03:55.360 --> 00:04:01.360
 The anthropic CEO seems solid. I don't get a sketchy vibe from him.

00:04:01.360 --> 00:04:07.200
 He was always trying to be really practical with how he talks about all this stuff,

00:04:07.200 --> 00:04:12.000
 which is pretty different from most people in this space. So yeah, it was very illuminating.

00:04:12.000 --> 00:04:15.600
 I'm not going to try and regurgitate it because it won't be as illuminating coming out of my mouth,

00:04:15.600 --> 00:04:22.080
 but you should go listen to it. He he's clearly very focused on safety. And it's just fun to listen

00:04:22.080 --> 00:04:26.800
 to people that like building these things, running companies, building these models,

00:04:26.800 --> 00:04:31.440
 talk about like the risks and like the future and how it could play out because I always hear

00:04:31.440 --> 00:04:36.880
 people talk about AI safety. And it's like, yeah, what I don't know, like it's all kind of vague and

00:04:36.880 --> 00:04:42.720
 fuzzy. But he like talks about the specific categories of threat that they pose and like

00:04:42.720 --> 00:04:46.320
 how to kind of like mitigate those things. It's just super interesting.

00:04:46.320 --> 00:04:49.920
 What is one example that you remember? Because I don't know anything about this.

00:04:49.920 --> 00:04:54.240
 Yeah. So he has like, I can't remember the name of this. I think I think anthropic came up with

00:04:54.240 --> 00:05:02.640
 the system for categorizing the different levels of like threat that these models pose to society.

00:05:02.640 --> 00:05:07.680
 For level two is like the like state actors could use it to further their goals. Level three is

00:05:07.680 --> 00:05:12.880
 like normal people could use it to like cause harm to humanity. And level four is that the AI

00:05:12.880 --> 00:05:20.400
 itself along with humans is actually a threat. So like the AI can take its own actions, even like

00:05:21.360 --> 00:05:25.760
 circumvent things. Like he talked about, they have to worry about, you know, they have these

00:05:25.760 --> 00:05:30.000
 benchmarks or these tests that they do for safety to make sure that the model can't do certain things

00:05:30.000 --> 00:05:35.600
 like can't tell people how to make smallpox or whatever. So they have these tests, but they

00:05:35.600 --> 00:05:41.600
 have to worry at level four that the AI will just like sandbag and pretend that it's not smart enough

00:05:41.600 --> 00:05:46.960
 even though it is because it knows it wants to pass the test. Yeah, which is super interesting

00:05:46.960 --> 00:05:52.480
 to think about like you just what do you do with like if these models can scale to like super

00:05:52.480 --> 00:05:56.960
 intelligence smarter than us? How do you how do you control something that's smarter than us?

00:05:56.960 --> 00:06:02.640
 It's just super fascinating. I don't really understand the lower levels because what is

00:06:02.640 --> 00:06:07.680
 did he talk about what practically is the difference between that and someone publishing a book

00:06:07.680 --> 00:06:13.440
 that has instructions on how to make smallpox? He didn't know. Yeah, I guess so what you're saying

00:06:13.440 --> 00:06:19.120
 is like how is level three and below anything new to the world? It's just is it just more efficient?

00:06:19.120 --> 00:06:25.200
 Like a dumber person could figure out how to make an atomic bomb because AI is so smart. Given

00:06:25.200 --> 00:06:30.880
 the stakes, it's like if you're someone that's like, Oh, I want to like unleash smallpox on the world,

00:06:30.880 --> 00:06:37.680
 but I'm too dumb and I can't figure it out. Yeah, ambitious goal. So it just like to be that

00:06:37.680 --> 00:06:45.920
 ambitious, but like not just figure it out without AI. So he speaks to that like the world is is

00:06:45.920 --> 00:06:51.680
 the state that it is it's mostly been safe because the overlap of people who are extremely intelligent

00:06:51.680 --> 00:06:58.480
 and the people who want to do a lot of harm to people is a small overlap like that generally

00:06:58.480 --> 00:07:01.840
 there's not a lot of people that fit those those things, but the fear is that AI

00:07:01.840 --> 00:07:07.520
 increases that overlap because now you take people who want to do a lot of harm and you give them

00:07:07.520 --> 00:07:12.800
 intelligence they didn't have. I guess that's the vague I general idea. I think this is where I

00:07:12.800 --> 00:07:18.080
 would disagree with the way all these people think about it because I feel like they look

00:07:18.080 --> 00:07:26.240
 at it from this really academic point of view, which is I have like raw horsepower intelligence

00:07:26.240 --> 00:07:31.120
 and I have, you know, trained knowledge and something and that's what gives me capability.

00:07:31.120 --> 00:07:36.240
 But in the real world, especially when it comes to violent stuff like that, none of that matters

00:07:36.240 --> 00:07:41.760
 is all about motivation. Like if someone is really motivated, they will figure this stuff out.

00:07:41.760 --> 00:07:48.160
 It's not like the thing that was blocking them was just like, oh, I'm not smart enough. You know,

00:07:48.160 --> 00:07:53.760
 it's not really what what issue is I will agree that like a lot of crimes happen because they're

00:07:53.760 --> 00:07:59.680
 more convenient and this would make certain things more convenient. I kind of see that point, but yeah.

00:07:59.680 --> 00:08:04.080
 So I remember with North Korea, there was a lot of tension with North Korea and like they were

00:08:04.080 --> 00:08:08.640
 shooting a lot of rockets just to like flex their muscles. And there's a lot of talk about like

00:08:08.640 --> 00:08:14.240
 how soon could North Korea develop nuclear weapons. Is that not like that's not because

00:08:14.240 --> 00:08:18.640
 they're not smart enough? Not smart enough, but like they don't have the knowledge of how to do it

00:08:18.640 --> 00:08:24.480
 or it takes years to develop that technology. Is that not something I could be faster at?

00:08:24.480 --> 00:08:29.600
 Yeah, it could be fast. I mean, it was taking the current form, right? It's if you're someone

00:08:29.600 --> 00:08:35.120
 that is trying to go from not knowing how to do this to knowing how to do this, what does North

00:08:35.120 --> 00:08:40.400
 Korea have? They have motivation for sure. This is probably like their top priority. They have

00:08:40.400 --> 00:08:48.000
 enough funding to figure it out. So given enough time, they will. There's like no stopping that.

00:08:48.000 --> 00:08:54.640
 Yeah. Do certain tools help them do that faster? Definitely the same way that Microsoft Excel

00:08:54.640 --> 00:08:59.600
 probably helps them figure out so fast. Yeah, sure. Okay. So I get why this feels like really

00:08:59.600 --> 00:09:08.320
 specific, but if you're talking about that level of impact in like the harm space,

00:09:08.320 --> 00:09:14.560
 we should see like the equivalent level of impact for people trying to do anything good, right?

00:09:14.560 --> 00:09:23.680
 So I'm not like I want to cure cancer. And I'm not like suddenly as a random person any closer to

00:09:23.680 --> 00:09:29.680
 doing that. Yeah. So yet I think that side of it is a little overstated. I think they're kind of like

00:09:29.680 --> 00:09:35.760
 I think they're just kind of in this bubble. That's a little bit like like feeding this narrative

00:09:35.760 --> 00:09:42.240
 into itself. So yeah, that's why the whole safety thing. I don't I don't fully get it. Like every

00:09:42.240 --> 00:09:47.520
 technology makes certain things more convenient. It's a lot more convenient to produce firearms

00:09:47.520 --> 00:09:52.800
 today than it was 100 years ago, like much crazier firearms. And yeah, you have to think about it,

00:09:52.800 --> 00:10:03.600
 but I don't I just don't see that acquisition of knowledge being the place that people get stuck.

00:10:03.600 --> 00:10:11.040
 It's it's usually that the US and like all the countries try to have a crazy strict control over

00:10:11.040 --> 00:10:15.680
 the raw material need to make a nuclear weapon. That's probably where the ball neck is. And even

00:10:15.680 --> 00:10:20.480
 that, you know, the country's work around because there's always someone that's against the US that

00:10:20.480 --> 00:10:25.840
 has access. And yeah, I've heard this details kind of rolled in the grand scheme of things.

00:10:25.840 --> 00:10:30.960
 Yeah, if I'm being honest, I I don't really buy all the AI safety talk. Like it's so hard to know

00:10:30.960 --> 00:10:37.760
 what's just noise like what is just posturing and like even competitive. Like some of these CEOs,

00:10:37.760 --> 00:10:42.640
 there's a bit of like pulling a ladder up, right? That's been kind of at least theorized. I don't

00:10:42.640 --> 00:10:47.600
 know if it's been proven, but like when the people that have the biggest AI companies training the

00:10:47.600 --> 00:10:53.120
 big extensive models are the ones leading the charge on, we need to make this harder. I don't know,

00:10:53.120 --> 00:10:58.240
 is there some other motive involved? But yeah, I guess like, and then it seems like the other

00:10:58.240 --> 00:11:02.000
 dialogue, you don't know what like is grounded in reality. There's so many people that talk about

00:11:02.000 --> 00:11:07.840
 AI safety that don't seem to have any idea what that looks like. It's like at the government level,

00:11:07.840 --> 00:11:13.040
 like they have no idea, like nobody has any clue what that practically looks like. So yeah,

00:11:13.040 --> 00:11:16.640
 it just feels like that whole conversation is either not grounded in reality or might have other

00:11:17.760 --> 00:11:22.880
 kind of like hidden agendas behind it. I'm not scared. I say bring on the AI.

00:11:22.880 --> 00:11:28.720
 It's like if it could solve problems and make things easier. Yeah, I feel like if it gives

00:11:28.720 --> 00:11:33.920
 the good guys more tools too, then yeah, what's the problem? I don't know.

00:11:33.920 --> 00:11:37.680
 Yeah, it's just funny because it's such a virtual thing. It's like something that

00:11:37.680 --> 00:11:44.640
 like you can imagine someone going to a store and buying a physical hammer and like smashing your

00:11:44.640 --> 00:11:50.480
 head in that's like so real, whereas like this is just entirely in the virtual space. And it's just

00:11:50.480 --> 00:11:56.880
 it's hard to imagine that, you know, it just feels like they have a point like it's not that

00:11:56.880 --> 00:12:02.880
 knowledge isn't harmful or dangerous, but just compared to like just physical, like buying a

00:12:02.880 --> 00:12:07.600
 vehicle and like ramming it through a crowd is just so much more effective than anything that's

00:12:07.600 --> 00:12:14.240
 balled next by your knowledge. I guess on the digital front though, there is a lot of havoc that

00:12:14.240 --> 00:12:22.400
 systems could do like banking systems, like if autonomous AI stuff that had its own agenda,

00:12:22.400 --> 00:12:27.680
 I could cause a lot of problems in the world, even if it's only digital and it doesn't have

00:12:27.680 --> 00:12:32.160
 physical form, right? It means it's not its own agenda. If there is some system that's now controlled

00:12:32.160 --> 00:12:39.360
 by AI and now there's like a whole set of new vectors of how can someone manipulate this system

00:12:39.360 --> 00:12:46.160
 just because it is it's hard enough for us to create security around like deterministic systems.

00:12:46.160 --> 00:12:53.200
 This is like a not deterministic system. So you never know if there's a certain set of words in

00:12:53.200 --> 00:12:59.280
 the right order will make it ignore all the safeguards you put in place. So that to me is like a very

00:12:59.280 --> 00:13:05.520
 practical application of AI safety. And that's not even like about the AI being capable. It's

00:13:05.520 --> 00:13:12.320
 actually a flaw with it being not very capable that it can be like reprogrammed by accident in

00:13:12.320 --> 00:13:18.880
 these little ways. So I get that side of things for sure. I listened to another podcast of Lexus

00:13:18.880 --> 00:13:25.760
 with I think his name was Adam Frank. He's like some kind of a astro something astro-physicist

00:13:25.760 --> 00:13:31.840
 maybe. He looks at space. He looks at space, but he like he like thinks about his job is like,

00:13:31.840 --> 00:13:38.000
 I guess they just got the first grant for looking for techno. What did he call it? Techno

00:13:38.000 --> 00:13:45.520
 signatures and signatures. It's like it's like biosignatures would be like looking at a planet

00:13:45.520 --> 00:13:49.760
 and saying is there any like are there gases that would prove that there's life on this planet?

00:13:49.760 --> 00:13:55.520
 But techno signatures are like does this prove that there's advanced technology? So they're like

00:13:55.520 --> 00:14:03.520
 actually looking at exoplanets in the habitable zone or whatever and trying to find like signs that

00:14:03.520 --> 00:14:10.080
 they have created technology. I can't remember what some of them were of a super fascinating guy.

00:14:10.080 --> 00:14:13.760
 Just go listen to Lex's podcast. What are you doing listening to us? Just go just listening

00:14:13.760 --> 00:14:18.400
 like the last five episodes are all good. I just listened to Paul. We're now just a podcast. I

00:14:18.400 --> 00:14:25.280
 summarize that other podcast. That's probably a thing. That's funny. That's pretty cool. I think

00:14:25.280 --> 00:14:29.200
 there's there's something else. There's some clips of some other thing I was watching that was

00:14:29.200 --> 00:14:34.240
 that was somewhat similar. So did you talk about like what what is like the primary thing they're

00:14:34.240 --> 00:14:38.160
 looking for? Is it looking for like Dyson spheres? Like what are they looking for? No. So he did talk

00:14:38.160 --> 00:14:42.640
 about Dyson spheres, which I didn't remember knowing what those were. That's wild, which I think

00:14:42.640 --> 00:14:45.920
 they proved you can't they couldn't actually make a Dyson sphere. You just said you didn't

00:14:45.920 --> 00:14:51.040
 remember knowing what that was. Do you mean like you forgot you knew about it? Yeah, I forgot

00:14:51.040 --> 00:14:56.480
 how many you did actually know about what it is. Yes. Listen, I don't have a great memory.

00:14:56.480 --> 00:15:02.640
 And I know I've heard of Dyson spheres, but until I heard him talk about them on this episode,

00:15:02.640 --> 00:15:09.040
 I didn't recall. It's like basically this big sphere around your star around the sun that

00:15:09.040 --> 00:15:14.240
 likes to capture all the energy from that sun, which that's another crazy thing that he talks about

00:15:14.240 --> 00:15:21.840
 is like the levels of civilization, the level, whatever they are, the energy output. Yeah. But

00:15:21.840 --> 00:15:27.200
 the technosphere thing, I think the main one they're looking at, what did he say? What did he say?

00:15:27.200 --> 00:15:36.880
 It was not Dyson spheres. It was that all lights, radio waves. No, it wasn't waves. I don't remember

00:15:38.080 --> 00:15:46.080
 man. I'm sorry. It would be interesting content and conversation. I just don't remember.

00:15:46.080 --> 00:15:49.920
 Oh, what would you personally look for? Well, let's see. What would I look for? If there was a

00:15:49.920 --> 00:15:56.080
 similar reason, we'll take it all of you. I would look for screens. I would look for

00:15:56.080 --> 00:16:04.160
 wait, screen. Oh, no, this isn't like they have images. He did talk about imaging. This is just

00:16:04.160 --> 00:16:10.720
 we're going out the rails. I can just talk about different podcast episodes forever. But he did

00:16:10.720 --> 00:16:16.640
 talk about like in the next however many hundred years that we'd be able to have like Manhattan

00:16:16.640 --> 00:16:24.000
 size imaging, like interstellar, like view cities, the size of Manhattan, what do you say? 26 kilometer

00:16:24.000 --> 00:16:29.440
 resolution or something on exoplanets. They have this idea for it sounds like science fiction for

00:16:29.440 --> 00:16:33.760
 sure. And that's the cool thing about science fiction. That would be crazy. The way it worked is like

00:16:33.760 --> 00:16:41.280
 you send all these like sensors, cameras, I guess, way away from Earth, the opposite direction from

00:16:41.280 --> 00:16:47.040
 the sun. I can't remember how far he said in the solar system, but like long ways. And they're

00:16:47.040 --> 00:16:53.920
 looking at planets that are just past the sun because of the way large bodies work space and

00:16:53.920 --> 00:17:00.160
 time. Yeah. So the sun basically like focuses the image of the star just beyond the edge of the

00:17:00.160 --> 00:17:07.760
 sun. And these cameras are looking at this wild stuff. Yeah, it's super wild. So using the sun is

00:17:07.760 --> 00:17:17.280
 like this amplification of our ability to view exoplanets. Anyway, let's talk about something

00:17:17.280 --> 00:17:22.720
 that's not on another podcast. My memory's not good enough for this exercise. You know what's

00:17:22.720 --> 00:17:26.800
 definitely on other podcasts, it's the whole deep seek thing from this past week. Oh, we didn't

00:17:26.800 --> 00:17:32.000
 yeah, we didn't really talk about deep seek much, did we? Can I can can you just like run that on

00:17:32.000 --> 00:17:39.040
 your local machine? Can I just like start getting coding benefits from deep seek R1 without an

00:17:39.040 --> 00:17:44.320
 API call? It's not really packed. I mean, it's like a reduced version of the model and it's very

00:17:44.320 --> 00:17:49.280
 slow and you need their hardware requirements are pretty crazy. So no, you can. Okay, so so how do

00:17:49.280 --> 00:17:54.880
 how do people use deep seek R1 right now? What is how does it exist? Is it commercialized in any

00:17:54.880 --> 00:17:59.200
 way? There's like a hosted one from the company, but it's in China. So people feel sketch out by

00:17:59.200 --> 00:18:04.720
 that. But then it's been rehosted because open source has been rehosted by a bunch of providers

00:18:04.720 --> 00:18:08.800
 that you're familiar with like Cloudflare, I think has a version of it. Oh, okay.

00:18:08.800 --> 00:18:14.000
 There's been a few others. I don't think it's good. Oh, really? Well, it's like not better than

00:18:14.000 --> 00:18:20.320
 anything else. It's just a recreation of what's already there. So they did it for less. It's like

00:18:20.320 --> 00:18:25.680
 the not Indian Jones. What's the guy, MacGyver? They just MacGyver it and they like made it a

00:18:25.680 --> 00:18:30.160
 duct tape. I don't believe any. I mean, it's just like it's none of it's none of the information

00:18:30.160 --> 00:18:36.080
 about it is true. Like it's just Oh, whoa, whoa, stop. Hold on. You catch me up. I didn't know that

00:18:36.080 --> 00:18:40.240
 it wasn't true. What are the facts that are true? Because I don't even know the facts around it.

00:18:40.240 --> 00:18:44.160
 Really? I just heard it's true. They're claiming they did. They're claiming they trained the model

00:18:44.160 --> 00:18:51.360
 for 5.5 million, which is like a crazy man, like several or magnitude less than what

00:18:51.360 --> 00:18:59.760
 open AI's models cost. Everyone was like dunking it. Is it a currency thing? Or were they talking

00:18:59.760 --> 00:19:07.040
 maybe in or something? No, make it even cheaper. Oh, okay. Yeah. No, no, the number is just too

00:19:07.040 --> 00:19:12.480
 small, no matter what currency on the planet you're using. And you just think you think why would

00:19:12.480 --> 00:19:17.840
 they? Oh, because it's like a competitive thing. They're trying to lie. So the reason it's it's

00:19:17.840 --> 00:19:23.120
 very noisy. There is true, interesting things that they did. Like, so you can't take that away from

00:19:23.120 --> 00:19:28.400
 them. Like it's impressive. But that doesn't mean what they're saying about how it was done.

00:19:28.400 --> 00:19:33.520
 It's true either. The numbers are just like way too much of a lie. Like there's no way that

00:19:34.800 --> 00:19:40.720
 one, they're that low. Two, there's a lot of reasons for them to make it up, right?

00:19:40.720 --> 00:19:45.600
 And no, it's going to reproduce it for once. That's not a thing. Could you make some of them

00:19:45.600 --> 00:19:49.600
 explicit? Yeah. Say some of the reasons, because I don't always connect dots. Well,

00:19:49.600 --> 00:19:55.280
 they're not China's not out to have certain DPs. But what? Is it the export? I didn't know this.

00:19:55.280 --> 00:19:58.960
 The export controls. Okay, you've got a lot of context here. You need to delay it all out.

00:19:58.960 --> 00:20:04.240
 Spell the case out for wide deep seek is a fraud. On paper and video is not allowed to export.

00:20:05.600 --> 00:20:11.280
 Uh, certain levels of GPUs to China. Okay. And video is an American company, right?

00:20:11.280 --> 00:20:17.600
 Yes. Okay. See, these are things I just don't know for sure. So you got it. Yeah, spell it out.

00:20:17.600 --> 00:20:22.400
 So they can't be like, Hey, here's exactly what we used if they're using a bunch of stuff they're

00:20:22.400 --> 00:20:26.320
 not supposed to have. So that like throws a bunch of questions to this. Sorry, going back. I want

00:20:26.320 --> 00:20:30.880
 to just real quick. The reason they can't export them to China is like American law.

00:20:30.880 --> 00:20:35.920
 Yeah. Yeah. We banned exports of GPUs about certain capability. Okay. Okay. Got it.

00:20:35.920 --> 00:20:41.040
 There's other interesting fact that someone pointed out recently that, uh, Singapore is 20%

00:20:41.040 --> 00:20:46.960
 of NVIDIA's revenue. Okay. Is Singapore in China? I'm so down. I'm so sorry. Singapore is like a

00:20:46.960 --> 00:20:53.040
 very small island nation in that area. Okay. So it's China adjacent. Why would they be?

00:20:53.040 --> 00:20:57.680
 Why would they be 20% of NVIDIA's revenue? That's a little weird. So they're buying all the GPUs

00:20:57.680 --> 00:21:02.160
 and then just taking them into China. Are they smuggling them? Yeah. It's like these export

00:21:02.160 --> 00:21:07.600
 controls practically are just not effective. Like it's like, how do you inflate these? I

00:21:07.600 --> 00:21:11.920
 were talking about earlier, like, there's always going to be a way if you're sufficiently motivated

00:21:11.920 --> 00:21:18.240
 to get these things. Um, and of course there's like, I could just buy a bunch of them and take

00:21:18.240 --> 00:21:23.760
 them to China. There's no one in China at China. There's no one at China that's going to stop me

00:21:23.760 --> 00:21:29.840
 bringing them in. Right. It's just that America is smoking on US is telling NVIDIA. You can't do this.

00:21:29.840 --> 00:21:36.000
 And the other thing I was thinking about was like, man, what a deal of this century. You could just

00:21:36.000 --> 00:21:42.160
 be the dude in Singapore, smuggling this stuff, adding a 20% whatever. Yeah. That's like, it's like

00:21:42.160 --> 00:21:49.280
 20% fee on like $20 billion of GPUs. Like that's, that is crazy. That's really wild.

00:21:50.240 --> 00:21:57.600
 So the point is there's like so many reasons why one, they wouldn't say to like, well,

00:21:57.600 --> 00:22:02.480
 sorry, one, they couldn't say what they actually did. And two, like there's a lot of reasons to just,

00:22:02.480 --> 00:22:06.320
 and that's what they always do. They always like lie about the price of things to like create,

00:22:06.320 --> 00:22:11.840
 just kind of market. Yeah. It's a good strategy. It works. Yeah. Okay.

00:22:11.840 --> 00:22:17.840
 But the deep seek put out a paper. I know this because all the software engineering nerds who

00:22:17.840 --> 00:22:21.280
 act like they're smart enough to understand papers are like, Oh, check out this paper.

00:22:21.280 --> 00:22:24.720
 This is amazing. Like you did, you don't know what the paper says to stop.

00:22:24.720 --> 00:22:30.880
 If you literally put the paper into deep seek and talk to it about it, you would learn more than

00:22:30.880 --> 00:22:36.320
 just listening to people talking about it. Yeah, probably. Well, okay. But question,

00:22:36.320 --> 00:22:40.000
 did they not have to outline the paper? Like what hardware they use and all that stuff?

00:22:40.000 --> 00:22:42.880
 I guess they don't have to, but would they not generally do that?

00:22:42.880 --> 00:22:46.720
 They talked about their techniques and their techniques are interesting and novel. So you

00:22:46.720 --> 00:22:53.600
 can't take that away from them. But they then they separately claim that we use these techniques

00:22:53.600 --> 00:22:59.360
 on this hardware to achieve this outcome. But there's so many ways to lie about that.

00:22:59.360 --> 00:23:05.440
 If it's in the single digits of millions of dollars, I feel like there's somebody out there

00:23:05.440 --> 00:23:10.320
 sufficiently motivated to like reproduce. Can they not reproduce based off the paper or

00:23:10.320 --> 00:23:16.400
 there's still some secret stuff? The thing is if you, okay, let's say someone told you that

00:23:17.360 --> 00:23:27.280
 hey, I can run a SQL query that filters a trillion rows in half a second, right?

00:23:27.280 --> 00:23:32.240
 You as someone that understands this stuff, you're like, I'm not even going to waste my time

00:23:32.240 --> 00:23:38.480
 reproducing that because that makes no sense. Okay. Yeah. So I imagine that something similar

00:23:38.480 --> 00:23:42.960
 is going on here. So you're saying like, I have so many, I'm sorry, I keep interrupting you.

00:23:42.960 --> 00:23:47.520
 I just feel like you're moving 100 miles an hour and I'm like at the stop sign still.

00:23:47.520 --> 00:23:54.160
 So you're saying that like big companies just all believe this is a bunch of BS.

00:23:54.160 --> 00:23:59.120
 Like it's just like the broader people in the know in the industry just dismissed this thing

00:23:59.120 --> 00:24:02.240
 right out and we're all excited about it. But like they're like, yeah, whatever.

00:24:02.240 --> 00:24:06.480
 Yeah. I mean, just because it's such a hyped space, it's like so hard to tell what's real

00:24:06.480 --> 00:24:12.800
 and what's not. And also it's the noise comes on both sides. So remember that we're saying

00:24:12.800 --> 00:24:18.160
 novel because it's been published publicly. We don't know that open AI already ran across this

00:24:18.160 --> 00:24:23.840
 and is using it to develop their stuff, like the techniques in there. Oh, so this might not even

00:24:23.840 --> 00:24:29.520
 be a surprise to them as much as be. Oh, they like independently came across the same techniques.

00:24:29.520 --> 00:24:34.880
 And they know that, yeah, it's not causing like a thousand X decrease in training costs.

00:24:34.880 --> 00:24:39.760
 But then the other noise is, and so now, and this is a part where I'm like, okay,

00:24:39.760 --> 00:24:42.800
 this could be noise on the other side. But I did think about this when it came out.

00:24:42.800 --> 00:24:51.360
 Open AI is claiming that they have proof that deep seek was trained on outputs of their models or

00:24:51.360 --> 00:24:57.840
 of like some, maybe potentially like unauthorized access to stuff from open AI.

00:24:57.840 --> 00:25:06.000
 And there's like some like, again, this doesn't mean anything, but like the pseudo science part of

00:25:06.000 --> 00:25:12.080
 this is people were able to get deep seek to reply and make the exact same mistakes that

00:25:12.080 --> 00:25:18.240
 a one makes, which seems like maybe it's a coincidence. Maybe it means something.

00:25:18.240 --> 00:25:22.640
 But yeah, the point here is like, it's just such a crazy hype space with a ton of money that

00:25:22.640 --> 00:25:29.440
 there's like zero ability to draw any kind of. This is what's happening right now in the moment.

00:25:29.440 --> 00:25:34.720
 It just impossible for situations like this. Yeah, I guess his like open AI. So you said they said

00:25:34.720 --> 00:25:40.480
 this thing about them using their outputs have like people like Sam Altman or any of the figures

00:25:40.480 --> 00:25:45.040
 in this space come out and said anything about deep seek publicly. You know how Sam Altman is.

00:25:45.040 --> 00:25:50.240
 He just did the whole like generic. Wow, it's really impressive and I'm invigorated by the

00:25:50.240 --> 00:25:55.840
 competition. You know, just like the fucking he to be honest, chat to me, he's more human than

00:25:55.840 --> 00:25:57.040
 some of them already.

00:25:59.520 --> 00:26:05.520
 Did you see what Claude did to me yesterday? No, what did you do? This was a mo I can't believe

00:26:05.520 --> 00:26:11.680
 it did this. So I was trying to deal with again, like bringing it all back down to earth. I was

00:26:11.680 --> 00:26:17.200
 trying to insert something into a Postgres database. And of course, on conflict, you want to like

00:26:17.200 --> 00:26:22.480
 do a dual update operation. Of course, I'm used to my sequel, we can just say on any conflict

00:26:22.480 --> 00:26:27.520
 do this operation, but I'm postgres specified like, oh, when this conflicts do that, when that

00:26:27.520 --> 00:26:32.640
 conflicts do this, but I was like, okay, can I just on conflict on anything? Is that possible?

00:26:32.640 --> 00:26:40.400
 And Claude, in a single reply, writes out, Hey, yeah, you can do this. And then it like

00:26:40.400 --> 00:26:45.120
 writes out the query. And then right after it does that continues writing being like,

00:26:45.120 --> 00:26:51.360
 just kidding. That syntax doesn't exist. What is it just kidding? Oh my god, this tweet

00:26:51.360 --> 00:26:58.400
 is this tweet has 5000 likes I didn't know what you're you tweeted this? I got to see this. I'm

00:26:58.400 --> 00:27:02.560
 trying to do so many things. I was also trying to look up techno signatures because I feel so bad

00:27:02.560 --> 00:27:11.440
 about not THDA. Okay, so you just tweeted this recently? I tweeted it last night. Last night.

00:27:11.440 --> 00:27:17.520
 Oh, well, bro, Claude is straight up pranking me now. Can I make it do it do on conflict on

00:27:17.520 --> 00:27:30.080
 anything? Yeah, some postgres you can use. That's hilarious. Yeah, it's funny because we're so

00:27:30.080 --> 00:27:34.080
 used to these models being quirky, but like think about this in a traditional product. Like,

00:27:34.080 --> 00:27:39.040
 imagine you have a product and you have a button and the button is like, click here to do something

00:27:39.040 --> 00:27:42.240
 useful and you click it and then it pops up being like, just kidding. We don't get that.

00:27:43.120 --> 00:27:48.000
 Like that would be so ridiculous to actually ship something that did that. Yeah, like that's

00:27:48.000 --> 00:27:54.000
 something that I do. But this is like in in Claude. And to be honest, something I just I've

00:27:54.000 --> 00:27:58.240
 just been annoyed with Claude more and more for the past couple of weeks. And this to me was like,

00:27:58.240 --> 00:28:04.400
 same. This is like the final or like you're straight up just joking right now. Like,

00:28:04.400 --> 00:28:09.360
 I'm gonna actually consider I think I'm gonna stop paying for it. I need to like reassess what

00:28:09.360 --> 00:28:13.920
 I'm paying for. Yeah, yeah, because I just keep signing up for them. And then it's like it's easy

00:28:13.920 --> 00:28:21.120
 to forget what the Claude thing I heard somebody or somebody tweeted this the other day that Claude

00:28:21.120 --> 00:28:26.240
 was getting dumber. And he talks about it on the podcast. Apparently, Lex asks him a question from

00:28:26.240 --> 00:28:32.000
 Reddit, which was like, why did why does Claude just keep getting dumber? And he kind of goes on

00:28:32.000 --> 00:28:36.960
 to say like that people report this on all the major models. This isn't just unique to Claude.

00:28:36.960 --> 00:28:42.560
 No, it's not. He kind of explains like, it was kind of hand wavy. I don't know. I didn't really take

00:28:42.560 --> 00:28:46.960
 from it that I believe they don't get dumber. He said they don't intentionally they never

00:28:46.960 --> 00:28:50.880
 changed the weights. They do sometimes change the system prompts and they change some other

00:28:50.880 --> 00:28:55.120
 things. And I don't know. But he basically was saying like, most people is just like a psychology

00:28:55.120 --> 00:28:58.640
 thing. You're really impressed at first. And then you just get less impressed every time.

00:28:58.640 --> 00:29:02.880
 That's what I was wondering. Is that the case? And the more you use it, the more you understand

00:29:02.880 --> 00:29:07.120
 the boundaries like about I do genuinely feel like it's gotten dumber in the last couple of weeks.

00:29:07.120 --> 00:29:11.360
 And I don't know what to do with that feeling because if I if I felt it and then I read someone

00:29:11.360 --> 00:29:16.960
 else felt it and then I learned that Reddit feels it like what there's something there, right? Because

00:29:16.960 --> 00:29:22.720
 it's like things that I felt like it was doing a pretty good job. A few weeks ago, it does it feels

00:29:22.720 --> 00:29:28.160
 like it's not doing as good. Is it just a feeling? Yeah, I'm on the side that is just a feeling. I mean,

00:29:28.160 --> 00:29:34.640
 I think I would I would doubt that it's that clear cut like they must constantly be optimizing or

00:29:34.640 --> 00:29:41.360
 like playing with the amount of compute they're allocating to inference. And there's like ways to

00:29:41.360 --> 00:29:48.720
 like kind of make it more efficient for you to run. Is that the kind of like tinfoil hat theory that

00:29:48.720 --> 00:29:52.720
 like they're just it's a cost thing that they're just kind of like using less resources over time

00:29:53.520 --> 00:29:58.960
 for inference. And that's what bounce it. Like there's no way that on day one of releasing something,

00:29:58.960 --> 00:30:05.520
 they nailed it and they never have to like tweak that. I would be surprised if there's like not any

00:30:05.520 --> 00:30:09.760
 like thing where they explicitly know that oh yeah, we did this because we made this trade off.

00:30:09.760 --> 00:30:14.960
 But I do agree that must be a psychology thing as well. Because if I really think about it,

00:30:14.960 --> 00:30:21.600
 the thing that's not static is I'm trying to use this stuff more and more. And it's really hard

00:30:21.600 --> 00:30:26.720
 to keep. You know, it's that thing where like everyone's like, oh yeah, I know what I eat every

00:30:26.720 --> 00:30:30.880
 day and like, you know, like I know I eat as many calories or whatever. But they made them write it

00:30:30.880 --> 00:30:38.080
 down. They realize like, it's so different with people's perception of how much they eat or what

00:30:38.080 --> 00:30:42.640
 they eat is. I think it's kind of similar where I know that I'm using it trying to use it more

00:30:42.640 --> 00:30:46.080
 and more aggressively. And I know over time, as I get more comfortable with it or like,

00:30:46.800 --> 00:30:51.600
 becomes more and more of my workflow, I'm definitely pushing the boundaries of it more. It just happens

00:30:51.600 --> 00:30:58.880
 with any tool. It's hard to say that that's not a factor. Oh, is that the end of your thought?

00:30:58.880 --> 00:31:06.800
 Yeah. That wasn't good enough for you. No, it's good. It's good. I just thought you were like on a

00:31:06.800 --> 00:31:11.040
 roll and I'm like looking up techno signatures and then you just stop listening. Oh my God.

00:31:11.040 --> 00:31:15.200
 Did you want to take the signatures? We moved on. I found it. I found it. So I'm going to tell you

00:31:15.200 --> 00:31:20.480
 at some point, but I do want to respond to the last thing he said, which I totally knew what you were

00:31:20.480 --> 00:31:28.080
 saying. Sorry. This has been a weird one. Yeah. By the way, it just strips smells like fire in my

00:31:28.080 --> 00:31:31.520
 house right now. So I hope I'm not burning down. That's not good. Yeah, that's not great. I think

00:31:31.520 --> 00:31:36.400
 it's because Liz turned on the heater and like, you know, 1000 Florida, you're not really supposed

00:31:36.400 --> 00:31:40.000
 to eat. They do. Yeah, you never use it. And then when you turn it on for the first time, it does.

00:31:40.000 --> 00:31:44.560
 It smells like there's like a actual like wood burning fire in your house. I know that smell.

00:31:45.680 --> 00:31:51.680
 I did have a follow up to what you said. Sorry. I had a question. Do you know, like, when we're

00:31:51.680 --> 00:31:57.040
 just talking about inference and the GPU resources allocated inference, like they have to use,

00:31:57.040 --> 00:32:03.520
 I guess, now thousands of GPUs to do the training. Do you know, like, orders of magnitude wise,

00:32:03.520 --> 00:32:07.840
 like, what inference looks like compared to training resources, like infrastructure?

00:32:07.840 --> 00:32:11.680
 They still allocate most of their stuff to training, not to inference.

00:32:11.680 --> 00:32:15.920
 Okay. So if they have 10,000 GPUs, like most of the 9,000 are used for

00:32:15.920 --> 00:32:21.840
 I don't know the exact ratio, but I know it's more on the training side than on an inference side.

00:32:21.840 --> 00:32:29.600
 Okay. Yeah. I mean, it just, it just makes sense because why when if you don't win the model battle,

00:32:29.600 --> 00:32:33.440
 the inference, the fact that people are using your product is kind of relevant. So it doesn't

00:32:33.440 --> 00:32:38.320
 make sense to like, intuitively, that made sense to me. And I figured that was case. It's just

00:32:38.320 --> 00:32:44.080
 interesting when you think about a business, the, like, the lifeblood of anthropic or open AI,

00:32:44.080 --> 00:32:50.240
 it's like this huge farm of GPUs. And they're only really, like that huge investment in GPUs

00:32:50.240 --> 00:32:54.000
 is useful for training new models. So they just always had to be training new models to get

00:32:54.000 --> 00:32:58.800
 the thing out of that huge investment, right? Which I guess they always will be training new

00:32:58.800 --> 00:33:04.160
 models. So maybe it doesn't matter. I mean, in the end, to me so far, and I felt this

00:33:04.160 --> 00:33:09.040
 from the beginning, this feels like the worst part of the stack to be in. It is the most

00:33:09.040 --> 00:33:16.800
 difficult and the most expensive. And it is the most like commodified. So yeah, I mean,

00:33:16.800 --> 00:33:22.960
 I think the thing that people point out with deep seek is it's impressive to create something

00:33:22.960 --> 00:33:31.920
 as good as open AI stuff. It's totally realistic to assume making a model that's 1% better than

00:33:31.920 --> 00:33:38.560
 open AI stuff costs like $50 billion. Like that's like totally realistic. Yeah. And that's like

00:33:38.560 --> 00:33:42.640
 an argument in favor of being like, this is why open AI will, you know, it's not really a threat

00:33:42.640 --> 00:33:50.320
 to them. Simultaneously, it's also like condemning this entire business because it's just like,

00:33:50.320 --> 00:33:56.240
 if it's going to take that much capital to make these marginal improvements, and it's like a crazy

00:33:56.240 --> 00:34:01.200
 competitive space where, you know, the costs are being driven to zero and all these companies are

00:34:01.200 --> 00:34:06.880
 competing. Yeah, it's just, I don't know, to me, it never made sense if I was like an investor,

00:34:06.880 --> 00:34:12.960
 this is not the part of, and I want to like, bet on this AI thing, this just feels like the worst

00:34:12.960 --> 00:34:19.680
 place to put your money. It's so intense, so capital intensive, right? Yeah, like when I see

00:34:19.680 --> 00:34:26.000
 that, I'm like, I need to invest in someone that benefits from having access to cheap AI models,

00:34:26.000 --> 00:34:32.320
 not the people building the cheap AI models. And yeah, like VC Twitter, like it's funny,

00:34:32.320 --> 00:34:36.160
 they just go on these little things. And currently they're on this, they've swung back and forth,

00:34:36.160 --> 00:34:40.160
 and currently they're all saying, oh yeah, like the application layers where you're going to make

00:34:40.160 --> 00:34:44.960
 a lot of money, but like, you know, a couple weeks ago, they were saying the opposite. But I do,

00:34:44.960 --> 00:34:50.320
 that does make more sense to me. Again, it's not, I'm not taking the moonshot bet,

00:34:50.320 --> 00:34:57.200
 because a moonshot bet is you invest in open AI, and they eliminate the whole economy, which I

00:34:57.200 --> 00:35:03.360
 get, and I like bets like that. It's just, for me, this one is not the one that that would go for.

00:35:03.360 --> 00:35:06.720
 Yeah, it was something like less crazy is probably the intermediate outcome.

00:35:06.720 --> 00:35:10.000
 Yeah, and Sam Altman sucks. That's an easy way to not want to take that bet.

00:35:10.000 --> 00:35:14.960
 Well, I mean, open AI or it's competitors. It could be anthropic or yeah, okay, sure, I guess.

00:35:17.360 --> 00:35:23.120
 One last thing on this. Yeah, some, I did come across something today. Do you remember Mistral?

00:35:23.120 --> 00:35:30.960
 Yeah, whoa, yeah. Okay, so we're like, this is maybe the worst company fundraise of all time,

00:35:30.960 --> 00:35:38.240
 because they raised like 150 million on like a $300 million valuation or something.

00:35:38.240 --> 00:35:40.720
 What? Like give up half their company.

00:35:40.720 --> 00:35:48.080
 Like give up half their company, and like that's nowhere near enough money to like play in this game.

00:35:48.080 --> 00:35:51.920
 Like they're trying to do the frontier model thing, like they're on that.

00:35:51.920 --> 00:35:56.080
 Yeah, exactly. And just like, jeez, what the fuck are they going to do? I mean,

00:35:56.080 --> 00:35:59.920
 yeah, they're like a French company, and maybe it's just like they're just going to serve the

00:35:59.920 --> 00:36:04.000
 French market, because I guess the company's there. Maybe they're going to train models for a thousand

00:36:04.000 --> 00:36:09.040
 times cheaper than open AI. Maybe they're going to go the deep secret. That's possible. But again,

00:36:09.040 --> 00:36:12.000
 like you just get away 50% of your company if you need any more money. Yeah, that's crazy.

00:36:12.000 --> 00:36:18.000
 If you need one million more dollars, like what are you going to make?

00:36:18.000 --> 00:36:24.000
 I didn't remember if Mistral was, there's been so many of these companies doing like

00:36:24.000 --> 00:36:28.720
 image stuff. I think the image space is even more messed up in my brain. And I thought they

00:36:28.720 --> 00:36:34.160
 maybe were one of the image generating things. But no, I want to talk about the app layer,

00:36:34.160 --> 00:36:39.920
 the AI app space, because that's also kind of top of mind for me. Maybe it's because VCs are

00:36:39.920 --> 00:36:45.200
 really excited about it. And Mark Andreessen was just on Lex Friedman. And like I said, I listened

00:36:45.200 --> 00:36:51.920
 to all of his podcasts. I want to talk about my experiences. And I want to hear from you

00:36:51.920 --> 00:36:58.400
 how you think about those companies. But first, techno signatures, the main one that we're looking

00:36:58.400 --> 00:37:09.360
 for is chlorofluorocarbons, because nature can't create those. That requires some sophistication

00:37:09.360 --> 00:37:14.160
 technology. And like he talked about Earth, we pumped so many in the atmosphere that we

00:37:14.160 --> 00:37:18.320
 blow a hole in the ozone layer and that that would be detectable using the right instruments

00:37:18.320 --> 00:37:24.800
 from far away. That seems pretty solid. I'm increasingly convinced that there's nothing out

00:37:24.800 --> 00:37:30.080
 there. But really, oh, because I'm increasingly convinced, I listened to a lot of sci-fi. Now

00:37:30.080 --> 00:37:34.000
 I'm increasingly convinced that it's everywhere. It's a dark forest. They're all out there. They're

00:37:34.000 --> 00:37:38.560
 just being quiet. That's how I feel. Tell me why you feel that way. Why do you think there's nothing

00:37:38.560 --> 00:37:44.000
 out there? I desperately want that not to be the case. And I think in a lot of ways, it's like

00:37:44.000 --> 00:37:51.120
 unlikely there's nothing out there. But man, given just the size of the universe, when I say

00:37:51.120 --> 00:37:57.440
 nothing out there, I mean, even if there is, it's not in our perceivable universe or whatever.

00:37:57.440 --> 00:38:00.640
 And like, I was just separating faster and faster over time.

00:38:00.640 --> 00:38:02.960
 Right. So like, there's no way we'd ever reach.

00:38:02.960 --> 00:38:11.360
 Yeah. So it just feels like, I don't know, it feels like a negative feeling towards

00:38:11.360 --> 00:38:16.080
 that whole thing. It feels like so impossible and unlikely. But again, not based on science,

00:38:16.080 --> 00:38:17.440
 just based off of how I feel.

00:38:17.440 --> 00:38:23.120
 Yeah, just feel. I guess, like, okay, I have a lot of thoughts. First, you just said that.

00:38:23.120 --> 00:38:30.240
 And it reminded me that I just heard how things can't travel across space and time faster than

00:38:30.240 --> 00:38:35.600
 speed of light, according to our understanding of physics. But the actual universe moves faster

00:38:35.600 --> 00:38:41.120
 than speed of light. So yeah, the galaxies, like moving apart are moving faster than the speed of

00:38:41.120 --> 00:38:45.040
 light, right? Because there's like new space being created in between them.

00:38:45.040 --> 00:38:49.040
 I guess, if you map that to velocity, I guess. I mean, I'm just my man.

00:38:49.040 --> 00:38:54.320
 You're just basically losing me. But it's like, if I magically created

00:38:54.320 --> 00:39:00.960
 between you and me more space, it's like we've moved further apart at a certain rate.

00:39:00.960 --> 00:39:04.080
 Right. But we didn't, yeah. So maybe that's what he's talking about.

00:39:04.080 --> 00:39:09.120
 I don't know. I just got ahead of my skis here, just even just trying to think about what you're

00:39:09.120 --> 00:39:14.640
 saying. But I think what Adam Frank just said on this podcast was that space time moves faster

00:39:14.640 --> 00:39:20.480
 than speed of light, like the expansion of it. But you can't, an object can't move across space and

00:39:20.480 --> 00:39:25.200
 time faster than speed of light. But if it is true that the galaxies are moving apart faster

00:39:25.200 --> 00:39:28.960
 than speed of light, then yeah, you could never get to another galaxy because we can only dream

00:39:28.960 --> 00:39:33.920
 to ever move at speed of light, which would be a crazy accomplishment. But if it's moving faster,

00:39:33.920 --> 00:39:38.960
 unless you like do something crazy, like you violate or like you have a new model or something.

00:39:38.960 --> 00:39:44.720
 That like just, yeah, it just totally breaks, breaks that. But yeah, outside of that, you know,

00:39:44.720 --> 00:39:50.000
 practically whatever that means in the space, like yeah, maybe our galaxy is explorable.

00:39:50.000 --> 00:39:56.720
 And man, like even that just feels like I can see there being nothing there.

00:39:56.720 --> 00:40:04.400
 So, okay. So my stance, I guess, it's that it's about time. It's not about distance.

00:40:04.400 --> 00:40:09.840
 It's like how long so if it's been around for? Yeah, maybe civilizations. And I'm stealing this

00:40:09.840 --> 00:40:15.760
 from all the various science fiction writers and actual scientists that I've listened to in the last

00:40:15.760 --> 00:40:24.160
 year. But yeah, maybe it's that like intelligent societies just don't last very long. So the chance

00:40:24.160 --> 00:40:31.920
 that overlap is happening, like the, you know, our, whatever, 100 years, 200 years of technological

00:40:31.920 --> 00:40:38.320
 advancement here is just such a tiny little blip in the broader expanse of the universe that like

00:40:38.320 --> 00:40:42.720
 the chance of that blip happening at the same time as a bunch of other blips is maybe super low.

00:40:42.720 --> 00:40:48.880
 But that maybe life is super common, just not intelligent societies that last long enough.

00:40:48.880 --> 00:40:54.800
 Like if we can get past, Adam Frink talks about this too, if we could get past all the terrible

00:40:54.800 --> 00:41:00.560
 things that could end our civilization, whether that's nuclear war, climate change,

00:41:00.560 --> 00:41:04.800
 AI, whatever, we could pass all those hurdles and we could figure out how to like live

00:41:04.800 --> 00:41:09.600
 for hundreds of thousands of years, millions of years as a civilization, then like,

00:41:09.600 --> 00:41:16.640
 then the chances of finding life maybe is is more realistic. Yeah, because you're around long enough

00:41:16.640 --> 00:41:20.640
 to see, I don't know, I'm just saying stuff that I don't have any credibility to say.

00:41:20.640 --> 00:41:26.720
 This is all just like different answers to the Fermi Paradox thing. But to me, I find

00:41:27.520 --> 00:41:34.240
 the problem with the Fermi Paradox, which is just to reiterate, it's given the size, the age of

00:41:34.240 --> 00:41:39.120
 the universe, we'd expect it to be like full of life, even how long stuff has been around

00:41:39.120 --> 00:41:43.280
 and given how much there is. And there isn't, so then you ask, okay, what are some explanations

00:41:43.280 --> 00:41:47.760
 of that? And there's a lot of good explanations. That's a problem. There's so many good explanations

00:41:47.760 --> 00:41:52.400
 and they could all be true. But the result of all of them is that life is exceedingly

00:41:53.120 --> 00:41:58.400
 rare and you're unlikely to intersect with it. So that's what kind of bones me about about this

00:41:58.400 --> 00:42:06.400
 concept. Okay, bummed you out because it would be nice to like, I don't want to die, but if I'm

00:42:06.400 --> 00:42:10.400
 going to die, it's because of an alien invasion, I'm like kind of down for that, because at least I

00:42:10.400 --> 00:42:16.480
 learned something deeply important for a few seconds before I get wiped out. Interesting.

00:42:16.480 --> 00:42:23.120
 Okay. Yeah, like, I don't want to die in like car accident and I had stopped. Oh, yeah, no, that's terrible.

00:42:23.120 --> 00:42:28.960
 Yeah, like, if I'm going to die, at least give me some crazy existential moment. Okay, yeah. What's

00:42:28.960 --> 00:42:36.240
 your like top three ways to die? Well, we will just not do it. I'm sorry. Yeah, no, I got you.

00:42:36.240 --> 00:42:42.560
 Existential dread and et cetera, et cetera. Okay. Anyway, let's talk about the app, AI app stuff.

00:42:42.560 --> 00:42:48.080
 So this idea was seated in my head just a few days ago, when Andre so was on Lex, and he talked

00:42:48.080 --> 00:42:58.320
 about just, I think the example they used was email, AI first email, and like how so many apps

00:42:58.320 --> 00:43:03.680
 just have like AI bolt-ons now, like we've got a little button in the corner that's like ask AI,

00:43:03.680 --> 00:43:09.040
 but companies that are started with the whole premise of like rethinking the product, the entire

00:43:09.040 --> 00:43:15.680
 category of product with AI first. So he used the example of an AI company building an email

00:43:15.680 --> 00:43:20.320
 client or something, which I've now I think I've downloaded. I don't know if it's the one that

00:43:20.320 --> 00:43:24.480
 they're invested in, but he got through that out there and just said like all the different

00:43:24.480 --> 00:43:31.840
 categories. And then I heard you. Did you tweet about this? No, I told you something that you

00:43:31.840 --> 00:43:40.000
 can't repeat. Oh, yeah. That's not public information yet, which I will not repeat. Thank you. Okay,

00:43:40.000 --> 00:43:44.400
 that's what it was. Yeah, it was a DM. I knew some other data point hit my brain that was like,

00:43:44.400 --> 00:43:50.080
 oh, the app layer of AI, that's a thing. And it's like when you hear when you learn a new word and

00:43:50.080 --> 00:43:54.080
 then you start seeing it everywhere. So could you tell me with your big brain that you've been

00:43:54.080 --> 00:43:59.280
 thinking about this probably for like 10 years? Could you tell me what is going on in the app

00:43:59.280 --> 00:44:05.760
 AI space? Yeah, so the way I look at it is there's a new capability. Again, this is I would categorize

00:44:05.760 --> 00:44:10.960
 it into categories. There is the boring parts is what we're talking about now. And this is the bet

00:44:10.960 --> 00:44:16.960
 that society will continue to be roughly the same. And this isn't like a, you know, truly

00:44:16.960 --> 00:44:22.640
 disruptive like a totally disruptive thing. You're speaking to like the bolt on thing, or you're

00:44:22.640 --> 00:44:27.360
 speaking to like the commodity of like family just talking about like building a traditional

00:44:27.360 --> 00:44:32.160
 product but thinking through AI, that's like a not very bold way of looking at all this.

00:44:32.160 --> 00:44:38.720
 No, part of me like doesn't want to engage with that because like I said, I don't, I don't believe

00:44:38.720 --> 00:44:44.000
 so far in that it's like much bigger bet. But I believe generally that's where you should put

00:44:44.000 --> 00:44:50.080
 your attention and things that kind of fall in that category. That said, let's say this

00:44:50.080 --> 00:44:54.080
 that this ends up, you know, not being that crazy thing. And this is a way this is the direction

00:44:54.080 --> 00:44:59.680
 things go. So right now we're in the era of there's a new thing and nobody knows how to build good

00:44:59.680 --> 00:45:06.320
 UX around it, right there. If you imagine when like the iPhone came out, pull to swipe or sorry,

00:45:06.320 --> 00:45:10.800
 pull to refresh. Oh, yeah. Someone had to come up with that. And the moment they did, it was so

00:45:10.800 --> 00:45:16.880
 obvious. I think we're like in that phase where almost every single product that added AI is

00:45:16.880 --> 00:45:22.000
 just a stupid ass little button that's on top of other shit. And it's just kind of getting in your

00:45:22.000 --> 00:45:26.000
 way and you're always accidentally clicking on. So that's just like, that's the arrow we're in.

00:45:26.000 --> 00:45:30.880
 But at some point, we'll see stuff that it's like, Oh, obviously. And I think we're actually

00:45:30.880 --> 00:45:36.480
 already starting to see some of that stuff. Have you seen this granola AI product? No.

00:45:36.480 --> 00:45:41.920
 Okay, so I think it's a brilliant example of what you're talking about,

00:45:41.920 --> 00:45:49.040
 rethinking products from an AI lens. And they did it in a way that is very well executed. It's not

00:45:49.680 --> 00:45:54.960
 like the it's not the first thing you would think of, right? But they were like, okay, problem

00:45:54.960 --> 00:46:01.840
 existed forever. How do we make people who take notes for meetings? How do we make that easier?

00:46:01.840 --> 00:46:06.400
 Boring problem been around forever. Here's years of products that do that.

00:46:06.400 --> 00:46:10.080
 Bunch of products that do that, right? There's a bunch of products that are like,

00:46:10.080 --> 00:46:16.400
 I'm Bob the AI and I'm a bot and I've joined your zoom call and I'm here to take notes. And it's

00:46:16.400 --> 00:46:22.560
 like weird, totally unnatural, not relating to your current habits thing at all.

00:46:22.560 --> 00:46:27.440
 Weird social norms around it, like it's just not a good way to introduce this idea to people.

00:46:27.440 --> 00:46:35.520
 So what this product does is it runs on your Mac. It records all the audio from your

00:46:35.520 --> 00:46:40.480
 meaning. Yeah, from anything that's happening. So we're like in the world.

00:46:40.480 --> 00:46:46.080
 Also in this era where no one's doing direct integrations anymore, because AI can just handle

00:46:46.080 --> 00:46:52.880
 raw input. If you can record audio from your Mac, you now support every single app that makes

00:46:52.880 --> 00:46:58.240
 so much sense of the box, right? Yeah. This shows up in a bunch of different places when people

00:46:58.240 --> 00:47:04.000
 are putting AI products. It's totally invisible and totally out of your way. They give you a typical

00:47:04.000 --> 00:47:09.120
 note pad you take notes on, okay? You take your shitty little notes, you know, you comments here

00:47:09.120 --> 00:47:15.520
 and there, whatever. When the meeting is done, AI will go through your notes and augment them

00:47:15.520 --> 00:47:20.400
 with what it knows about the meeting. So if you're like, oh, priority, it knows what you were talking

00:47:20.400 --> 00:47:26.560
 about saying this is a priority and it'll like make your notes much nicer. And just like a one-step

00:47:26.560 --> 00:47:31.920
 process. So it doesn't feel like an AI product. It just feels like a magically good product. I take

00:47:31.920 --> 00:47:36.960
 notes with the same habits that I've had forever. And then at the end, I just get much better notes

00:47:36.960 --> 00:47:41.680
 than I would do with any other app. And I think this is kind of what you're talking about,

00:47:41.680 --> 00:47:45.600
 where they're reimagining it and they've done it in a way where it's not like, you need to chat

00:47:45.600 --> 00:47:50.400
 with my with my AI bot, right? It's like totally invisible. Super smart. So I think we'll start to

00:47:50.400 --> 00:47:56.800
 see products that they are technically powered by AI, but it's invisible. The only way you can tell

00:47:56.800 --> 00:48:02.320
 is the outcome or the quality of the product is much higher, just because all of these

00:48:03.040 --> 00:48:07.440
 structuring unstructured data problems are like effectively solved now.

00:48:07.440 --> 00:48:12.800
 Man, yeah, it makes a ton of sense. I've already downloaded Grinnola now.

00:48:12.800 --> 00:48:19.200
 I feel like this is very exciting as a person who has an entrepreneurial side,

00:48:19.200 --> 00:48:22.880
 just kind of makes you want to build like a million companies. I'm not a million.

00:48:22.880 --> 00:48:26.880
 Let's build one. Yeah, just like one company. It just makes you want to build something doesn't it?

00:48:26.880 --> 00:48:31.920
 It feels like the Wild West, it's like starting over like all the digital products we use

00:48:32.480 --> 00:48:36.400
 just could be reimagined. And there's so many categories of those. And it kind of makes you just

00:48:36.400 --> 00:48:41.520
 want to like build some of them. I do think though that people should be aware that this isn't a

00:48:41.520 --> 00:48:46.560
 reset to like 2010, because in 2010, what was what was 2010? The situation like, you know,

00:48:46.560 --> 00:48:49.680
 there's a similar situation like nothing was built. And there was like all these opportunities to

00:48:49.680 --> 00:48:55.200
 build these pretty like basic straightforward applications. Wait, 2010, what was the new thing

00:48:55.200 --> 00:49:00.080
 that enabled like mobile? What are you talking about? Just just like more internet, more web,

00:49:00.080 --> 00:49:05.760
 more capability of like SaaS like was kind of created in that era, all that stuff. Gotcha.

00:49:05.760 --> 00:49:13.440
 In that time, you're you were shifting people from not using computers to using computers to

00:49:13.440 --> 00:49:18.400
 solve this problem. So as much as it feels like, Oh, we're in a reset and there's all

00:49:18.400 --> 00:49:23.680
 as a new opportunity, it's not the same because you can't just deliver an MVP any. Oh, sure.

00:49:23.680 --> 00:49:31.280
 To deliver an MVP in 2010. But if you want to build a new email AI product, you need to build

00:49:31.280 --> 00:49:39.840
 something as good as superhuman as a floor. And then you can do this extra stuff, right?

00:49:39.840 --> 00:49:45.120
 Yeah, okay. It's still going to be quite hard just because a bar is very high to get something

00:49:45.120 --> 00:49:50.400
 to switch from something that just out of normal all the normal app features are pretty exhaustive

00:49:50.400 --> 00:49:54.560
 and work pretty well. That's that. That's how things has also just gotten easier to do with.

00:49:54.560 --> 00:50:00.560
 Oh, but yeah, I am feeling this with radiant because yeah, categorizing financial transactions

00:50:00.560 --> 00:50:08.720
 was very, very difficult, like prior to AI. And now it can do like a really good job, even like

00:50:08.720 --> 00:50:12.880
 a shitty thing I implemented. I like was able to go through my stuff with like, and I've done

00:50:12.880 --> 00:50:15.920
 this for years, right? Like Angela, my business transactions, I've gone through every single one

00:50:15.920 --> 00:50:20.320
 of them for years and years. And just having AI do a first pass and then me doing a second pass,

00:50:20.320 --> 00:50:24.080
 it's much better. And this is just the beginning of, yeah, of all this stuff.

00:50:24.080 --> 00:50:30.880
 But we still have to build like the entirety of a straightforward app. And you have to do that

00:50:30.880 --> 00:50:36.560
 while the incumbent fails to do the new thing, which I think will happen. It's just, you know,

00:50:36.560 --> 00:50:41.040
 not as easy as it seems. Yeah, there's like the table stakes part that's kind of boring,

00:50:41.040 --> 00:50:44.160
 where you just have to have all the features that people expect from an app like that,

00:50:45.120 --> 00:50:50.400
 in order to unlock the new way of thinking about it. So for the granola case, it was like they had

00:50:50.400 --> 00:50:55.360
 to build an actual note taking app and all that comes with it. That's a good example,

00:50:55.360 --> 00:51:01.040
 something that works because those table stakes go up is really small. And they benefit from this

00:51:01.040 --> 00:51:07.520
 new dynamic of not having to do 100 integrations with every single, like we support zoom, we support

00:51:07.520 --> 00:51:11.520
 Google meet, we support whatever. And how did you explain how that dynamic came to be? Because I

00:51:11.520 --> 00:51:16.480
 get it for like recording audio. It just works for everything. But what you're saying, there's

00:51:16.480 --> 00:51:20.480
 this whole era of not integrating directly with stuff. What's that about? Yeah, so let's say you're

00:51:20.480 --> 00:51:27.680
 like, I mean, let's say we're not actually doing this, but for radiant, there's 5,000

00:51:27.680 --> 00:51:32.080
 financial accounts that we need to support for all the various places, people have their data.

00:51:32.080 --> 00:51:38.480
 We could just send AI to go like, visit the site for you and like figure out how to pull out

00:51:38.480 --> 00:51:43.200
 your information instead of like mainly doing integration with which each thing because AI can

00:51:43.200 --> 00:51:50.000
 operate at like one level down, like it is indeed an API. A developer needs an API, like an AI agent,

00:51:50.000 --> 00:51:55.920
 like in theory doesn't need one. So you can kind of like give it a general seven instructions that

00:51:55.920 --> 00:52:01.680
 will work on any raw input. So anywhere where you like needed all these like nice clean integrations,

00:52:01.680 --> 00:52:07.840
 like you can probably make do with a much messier, like unsanctioned integration.

00:52:07.840 --> 00:52:12.560
 Interesting. Okay, that didn't really answer my question. I mean, I don't feel satisfied.

00:52:12.560 --> 00:52:19.120
 It may be it did, but like I think it's like there's another example and I can't remember,

00:52:19.120 --> 00:52:22.480
 I feel like there is another company where it was like, oh, that's a clever way of

00:52:22.480 --> 00:52:27.040
 integrating with everything. Oh, no, it's the conversation we had about like an AI tool that

00:52:27.040 --> 00:52:30.480
 just looks at the file system. Use that as a source of truth and you don't have to integrate

00:52:30.480 --> 00:52:34.880
 with every editor. You just interact with the file system. Yeah, yeah, yeah. So that's like a

00:52:34.880 --> 00:52:40.240
 very clever way to get around this like this thing on your landing page where you have all the

00:52:40.240 --> 00:52:44.800
 things you support. Yeah, it's like just what's the common denominator? The other side of this,

00:52:44.800 --> 00:52:49.040
 though, is if you look at a lot of these products, like granola, like there was the other one that

00:52:49.040 --> 00:52:53.360
 I forgot the name of it records everything. It takes a good screenshot every three seconds,

00:52:53.360 --> 00:52:58.880
 and then like has AI index it and you can ask it like, hey, what was that thing I read the other

00:52:58.880 --> 00:53:05.760
 day about? Whatever. So you see how all of these things are native apps at the OS level. It just

00:53:05.760 --> 00:53:12.080
 brings up the question like, isn't Microsoft and Apple is going to bake these in? Oh, yeah.

00:53:12.080 --> 00:53:16.640
 If you're building that kind of stuff, it's scary. Yeah, if you think about like this stuff,

00:53:16.640 --> 00:53:20.560
 like we're getting these like one off solutions that people come up with, but at the end of the day,

00:53:20.560 --> 00:53:26.320
 if we're just integrated at the OS level, it works everywhere and kind of be just a lot more awesome.

00:53:26.320 --> 00:53:31.360
 Yeah. So it feels like that should be the ultimate. The Apple intelligence kind of thing, like

00:53:31.360 --> 00:53:37.120
 Apple intelligence should do that stuff if it ever actually doesn't. It sucks. Apple intelligence

00:53:37.120 --> 00:53:42.000
 sucks. But in theory, does it even work yet? Like, I don't even think they did they turn it off

00:53:42.000 --> 00:53:48.320
 because it was like doing bad things. I've had it for a while and I have not used it once. I think

00:53:48.320 --> 00:53:53.520
 somehow it's made things even worse. I feel like I used it even less now than I used to.

00:53:54.800 --> 00:53:58.320
 I don't know. I don't know what they're doing. It's pretty bad. Hopefully they do that thing

00:53:58.320 --> 00:54:03.120
 where they catch up really fast because I would like Apple software to be good because I love

00:54:03.120 --> 00:54:09.840
 their hardware. Yeah, we'll see. But I will say this type of thinking is new for me where I'm like,

00:54:09.840 --> 00:54:19.760
 see how I described a very clearly good opportunity and then the ideal, which would be like Apple,

00:54:19.760 --> 00:54:26.080
 Microsoft integrating, but that ideal might be 10 years away. So there's so plenty of time to make

00:54:26.080 --> 00:54:32.160
 money, you know, successful in that time. Yeah. But I've like shifted to like not,

00:54:32.160 --> 00:54:38.720
 like if I can see the ideal and it's not like a line with what I'm doing, I just don't want to

00:54:38.720 --> 00:54:46.160
 work on it. It just feels bad to me now. Like even if it's 10 years, you just don't want to

00:54:46.160 --> 00:54:52.400
 invest in that idea. Like, yeah, I want to have a real shot of building the ultimate thing,

00:54:52.400 --> 00:54:57.040
 even if that means, even if the opportunity is great, otherwise. Are you quitting terminal?

00:54:57.040 --> 00:54:59.520
 Is that what you're saying? Is it not AI enough for you?

00:54:59.520 --> 00:55:09.360
 You missed the meeting yesterday. I'm just saying. I was only the remember the meeting.

00:55:09.360 --> 00:55:15.600
 Yeah, that's true. Funny part. We have weekly Wednesday meetings and I was like, oh, I can't

00:55:15.600 --> 00:55:20.320
 make it. So I posted at two thirty. We have the meeting. Hey, guys, I can't make the meeting.

00:55:20.320 --> 00:55:26.080
 And nobody else said anything that the meeting didn't happen. So everyone missed it. I was

00:55:26.080 --> 00:55:29.120
 the one that actually remembered that it was supposed to happen. It only would have happened

00:55:29.120 --> 00:55:32.960
 if you started it. But the fact that you didn't start it because you weren't going to make it.

00:55:32.960 --> 00:55:39.040
 It's funny. There's something else I wanted to talk about. It's totally unrelated to all this.

00:55:39.040 --> 00:55:47.200
 Totally unrelated to AI and apps and aliens. Yes. I posted a video last week or was it earlier

00:55:47.200 --> 00:55:52.080
 this week? No, it was it was on Sunday. Post on Sunday. Best video I've ever made in terms of

00:55:52.080 --> 00:55:57.600
 Oh, really? views. Yeah. I got to check out the SSC YouTube video. Again, I think it's really

00:55:57.600 --> 00:56:02.160
 not the execution of the video. I think we're just picking like some pretty good topics.

00:56:02.160 --> 00:56:05.520
 What's your handle? I did it. I did it just as a teaser just to see.

00:56:05.520 --> 00:56:11.760
 Ah, nope. That's something Korean. That's definitely not it. What? Really? At SSC?

00:56:11.760 --> 00:56:16.880
 I mean, I guess SSC dev. You don't have to look it up. I'll just tell you.

00:56:16.880 --> 00:56:20.400
 I got it. No, I got it. I don't use my computer. Is it that one?

00:56:20.400 --> 00:56:24.960
 Yeah. So I made a video on my like my remote dev stuff. Oh, I've been wanting this video.

00:56:24.960 --> 00:56:29.280
 I can't believe I didn't see it. How do I not see this? This is how like big the world is.

00:56:29.280 --> 00:56:34.000
 Anytime you think like everyone just sees all your stuff, I if anyone sees your videos,

00:56:34.000 --> 00:56:37.680
 I should see your videos, right? That's true. Yeah. And I didn't know you made this video.

00:56:37.680 --> 00:56:42.240
 But like, are you ever on YouTube? No, no. I go to YouTube from Twitter links.

00:56:42.240 --> 00:56:47.600
 The people post. Yeah. Why would you see it? Oh, because I think I would see. I mean,

00:56:47.600 --> 00:56:49.760
 sometimes you would think I would see your tweets. I don't know.

00:56:49.760 --> 00:56:52.640
 That's true. I feel like we're friends. And I should know when you make a good video that I

00:56:52.640 --> 00:56:56.560
 really want to see. And this is one I've wanted you to outline because I didn't want to bug you

00:56:56.560 --> 00:56:59.760
 too much and be like, Hey, could you tell me how you do the remote teambox thing? But now you've

00:56:59.760 --> 00:57:03.760
 just made the video and I can watch it like every other normie. This is awesome. Yeah.

00:57:03.760 --> 00:57:08.000
 It was a, I think a lot of people were waiting for it, which is why I think it did pretty well.

00:57:08.000 --> 00:57:10.880
 So it's our best performing video ever, which we're really happy about. I love the title.

00:57:10.880 --> 00:57:19.680
 I don't use my computer. Yeah. So YouTube comments. Let's talk about YouTube comments real quick.

00:57:19.680 --> 00:57:28.480
 Yeah. For me personally, this is where I experience just like the dumbest of all humanity, I think.

00:57:28.480 --> 00:57:32.480
 It is really wild. Like people, like I've been on Twitter a long time. Of course,

00:57:32.480 --> 00:57:37.520
 I get dumb annoying comments there, but YouTube somehow just consistently tops it.

00:57:37.520 --> 00:57:43.840
 It's surface is persona that I run into a lot on the internet. And to me, it's like a very

00:57:43.840 --> 00:57:48.800
 miserable persona. It's a persona of someone that thinks that every single thing they interact

00:57:48.800 --> 00:57:56.080
 with is a scam somehow. But they're like, they're so eager to be like, I think what's driving them

00:57:56.080 --> 00:58:01.600
 is they want to feel like they're smart and they like picked up on something that everyone

00:58:01.600 --> 00:58:07.040
 else is falling for. But they're so desperate for that moment that every single thing that they

00:58:07.040 --> 00:58:12.960
 perceive, they like project onto it that oh, this is like a scam somehow. Yeah. So a bunch of people

00:58:12.960 --> 00:58:19.520
 were just like, this is an ad or like they were talking about how like, I only do this because it's

00:58:19.520 --> 00:58:24.320
 it's free because I mentioned that my server that I use now is sponsored. But like, I've been doing

00:58:24.320 --> 00:58:32.240
 this for you. Shout out to reliable site. Yeah, it's very reliable. But like, I've got that

00:58:32.240 --> 00:58:41.120
 paid for it for like before they before I got that deal. And also in the video, I outlined how

00:58:41.120 --> 00:58:45.120
 you can start really small and at the entry price for this, again, people love saying $5

00:58:45.120 --> 00:58:50.000
 VPS. There's just a $5 VPS. Realistically, maybe more like 15 for something that's decent, but

00:58:50.960 --> 00:58:55.920
 reasonable price. But everyone was just like, as soon as their brains work together, like, oh,

00:58:55.920 --> 00:59:02.320
 this is the angle, a bunch of comments where we're around talking about how like I was trying to

00:59:02.320 --> 00:59:08.320
 trick them into doing this because it's expensive. And I'm just like, how, like, how do you go through

00:59:08.320 --> 00:59:12.240
 life like this? Like, everything must be so miserable. We just proceeding and I was like,

00:59:12.240 --> 00:59:18.320
 every person you interact with is trying to rip you off somehow. Yeah. Yeah, the internet kind of

00:59:18.320 --> 00:59:22.960
 sucks. It's kind of amazing, but it also kind of sucks. I'm just reading YouTube comments now.

00:59:22.960 --> 00:59:28.560
 I wish I hadn't. Sorry, which you just not just don't don't remind me that YouTube exists and I'll

00:59:28.560 --> 00:59:33.200
 be a happier person. That's funny. What's even at the top right now? I think one of those is

00:59:33.200 --> 00:59:38.080
 probably at the top. So it's funny. I just saw Kevin not commented. Yeah. And excuse to not do

00:59:38.080 --> 00:59:42.560
 any work for next three or four weeks. I really do need to spend like two days and just like copy

00:59:42.560 --> 00:59:48.320
 all your Neil Vim set up. And my Neil Vim is so bad right now. I know. I just need to do all that work.

00:59:48.320 --> 00:59:54.160
 And I just, it's so hard to take a time out. It's that stupid meme that I do hate because I resonate

00:59:54.160 --> 00:59:58.720
 with it of like the caveman with like the square wheels and they're like, too busy. Leave me alone.

00:59:58.720 --> 01:00:06.240
 And the guy's like, but here's a wheel. It's just so hard. Maybe you just go use cursor. You know what

01:00:06.240 --> 01:00:10.720
 I've actually thought about downloading it. I'm doing it right now. I do want to download it. Yeah,

01:00:10.720 --> 01:00:15.760
 I want to download it. Like why? It's like all this stuff is free paid for by VCs. Why am I not

01:00:15.760 --> 01:00:20.800
 using all of it? It's not free with it. It's not free. But like, it's free. You have to pay for

01:00:20.800 --> 01:00:24.160
 it's not crazy. Yeah, it's not that expensive. I just assumed it was free.

01:00:24.160 --> 01:00:29.440
 It's just so miserable for me going. Yeah. This is like another point of

01:00:29.440 --> 01:00:36.480
 dress for me, which is dress as being very dramatic. Just dress around my editor.

01:00:38.560 --> 01:00:48.400
 I really like the oven and it is truly incredibly productive. But this cursor style of thing,

01:00:48.400 --> 01:00:55.680
 if it continues to get better, that's just going to be the most productive thing. Yeah. It doesn't

01:00:55.680 --> 01:01:01.920
 address the parts I particularly find annoying. I hate the clunkiness and the slowness of VS code

01:01:01.920 --> 01:01:05.360
 and navigating and stuff. And yes, you're doing all that less with this type of thing, but it's

01:01:05.360 --> 01:01:10.960
 not taking it to zero. I don't see why Neo then would get something that's equivalent. I've seen

01:01:10.960 --> 01:01:16.720
 the current effort for it. Yeah. And I go visit the GitHub and I read it once a week.

01:01:16.720 --> 01:01:22.640
 And I'm just like, this just doesn't feel like it's going to be good. And there's so much setup

01:01:22.640 --> 01:01:28.640
 involved. It's the we have cursor at home. And it's like cursor at home is like four libraries

01:01:28.640 --> 01:01:33.440
 duct tape together and socks. Why am I installing something like that on my machine? What is going

01:01:33.440 --> 01:01:39.760
 on? It's like, there's too much, too much steps, too many steps. I don't mind switching editors.

01:01:39.760 --> 01:01:45.600
 I just wish the foundation that this new stuff was built on was not VS code because VS code sucks.

01:01:45.600 --> 01:01:52.000
 That said, I think I think Zed will probably because they're in this hyper competitive mode.

01:01:52.000 --> 01:01:55.840
 Wait, you think they will what? I think their AI stuff will get as good as cursors, if not

01:01:55.840 --> 01:02:01.440
 better. So they are working on AI steps, then they have to be. I mean, I just had the thought

01:02:01.440 --> 01:02:05.520
 in my sleep last night, which is just an indictment on my sleep. I had the thought, like, Oh, poor

01:02:05.520 --> 01:02:09.280
 Zed. Like Zed, how does that have a chance when there's like all these AI things now,

01:02:09.280 --> 01:02:13.520
 but they're doing the AI things. It's like, there's so many editors already. If you're not an AI

01:02:13.520 --> 01:02:19.440
 editor, good luck. Right? Yeah, no, it's it's true. Like they have a tough battle because they

01:02:19.440 --> 01:02:26.160
 okay, it kind of goes into directions. On one hand, like, yeah, it was way faster ship cursor

01:02:26.160 --> 01:02:34.080
 by building on VS code. On the other hand, I've just found, as I get older, that doing the more

01:02:34.080 --> 01:02:40.400
 extreme thing always ends up having a good benefit that you can't predict. So them going

01:02:40.400 --> 01:02:46.960
 ground up building a new editor way harder. All the ship fast mindset would be like,

01:02:46.960 --> 01:02:52.400
 that's a waste of time. Just focus on the part that differentiates AI part. But I can see how

01:02:53.440 --> 01:02:58.240
 actually, no, like, this is going to end up being the thing that wins. So to me, it's

01:02:58.240 --> 01:03:02.320
 plausible. I don't think they're screwed. And that they are going to do AI stuff.

01:03:02.320 --> 01:03:05.760
 Yeah, I just didn't even know they were working on it. They're working on AI stuff. And yeah,

01:03:05.760 --> 01:03:11.200
 good for them. And they're not built up. I have no idea. I don't keep up on this stuff. I've just

01:03:11.200 --> 01:03:16.000
 I use Neovim because someone said use Neovim. So I do. I mean, they say AI in their

01:03:16.000 --> 01:03:19.440
 integrate upcoming elements, your workflow is generating personalized code.

01:03:19.440 --> 01:03:24.960
 So and cursor is not a lot of features. It's like a really small set of features, to be honest.

01:03:24.960 --> 01:03:28.640
 I've never played with it. I'm literally setting it up right now. But yeah, so I'm like, okay,

01:03:28.640 --> 01:03:32.880
 that gives me some hope because maybe the editor experience won't suck. But then it's not in the

01:03:32.880 --> 01:03:39.360
 terminal anymore. So then my whole setup is now like a lot more confusing. Like I like having

01:03:39.360 --> 01:03:44.320
 everything in a single terminal between between it. Yeah, all my muscle memories around like

01:03:44.320 --> 01:03:49.200
 switching between Tmux panes and doing all this stuff. And if I'm just in some editor now,

01:03:49.200 --> 01:03:56.240
 I guess like I can get the VIM experience in the files, the actual files I'm modifying. But like,

01:03:56.240 --> 01:04:01.600
 okay, can I go back to something to just on behalf of the normies that listen to us?

01:04:01.600 --> 01:04:05.040
 Why is VS Code bad again? I know we all hate VS Code, but just someone reminded me,

01:04:05.040 --> 01:04:08.560
 why do we? Why is it? Whenever I try to use it, it's like a slow piece of shit and the VIM

01:04:08.560 --> 01:04:13.840
 simulation is like really bad. So it's slow. Yeah, it just it just fits to me. It feels bad to use.

01:04:13.840 --> 01:04:18.320
 Okay. I just take everyone's word for it when everyone's like making fun of VS Code. Yeah, VS

01:04:18.320 --> 01:04:24.720
 Code. But I didn't actually know why. It just doesn't feel good to use. That's what all comes

01:04:24.720 --> 01:04:30.800
 down to for me. Okay. Okay. Well, I'm gonna try cursor. I'm gonna give it a go. I hope it doesn't

01:04:30.800 --> 01:04:40.240
 botch the whole terminal code repo. Yellow. Here we go. Yeah. Z does have this. They have

01:04:40.240 --> 01:04:46.320
 their own like remote protocol thing. So I could continue to like effectively host Z on my server,

01:04:46.320 --> 01:04:50.640
 even though the front end of it is running on my machine. That's cool. But again, didn't have to like

01:04:50.640 --> 01:04:56.400
 have like a separate terminal window unless my terminal is run inside of Z. Ah, just use the

01:04:56.400 --> 01:05:01.040
 integrated terminal. I hear it's good skeptical, but skeptical. I'm gonna give it a shot. I'll let

01:05:01.040 --> 01:05:04.880
 the listeners know if curse is good. They probably already know, but I'll let you know use cursor and

01:05:04.880 --> 01:05:11.120
 use Z and then go fix your name. Yeah, I need to fix many of them. Okay, I'll try that. If Z

01:05:11.120 --> 01:05:15.520
 has AI stuff, I'll start there actually, because I'd rather use the thing that you think is good

01:05:15.520 --> 01:05:21.280
 generally in life. Do they this introducing Z AI? And this was like in August, but they're

01:05:21.280 --> 01:05:29.440
 definitely stuff. Definitely stuff. Z, what is it? Z dot dev? The editor for what's next with

01:05:29.440 --> 01:05:36.000
 humans and AI. Let's go. I had this thought the other day. I was like, if you're like a VC funded

01:05:36.000 --> 01:05:42.960
 company, you probably like shifted towards AI. Like almost if you look at everyone's websites,

01:05:42.960 --> 01:05:48.160
 no matter how random it is, like they seem to like really focus on AI. Like most of them

01:05:48.160 --> 01:05:52.400
 just took their existing slogan and added like, and AI to it. Wait, is that really what Z did?

01:05:52.400 --> 01:05:58.080
 Maybe. Yeah, with humans and AI. And I saw, I saw something the other day and I was like,

01:05:58.080 --> 01:06:06.160
 yeah, I'm gonna turn shows website and at the bottom now they have unlimited databases,

01:06:06.160 --> 01:06:12.400
 personalized scale, supercharge, which, you know, probably was there before your LLM applications.

01:06:13.120 --> 01:06:18.640
 So there's like that. We've all observed this, you know, whatever. But then I think about,

01:06:18.640 --> 01:06:27.920
 okay, there's VC funded companies at this stage that had not done this at all. You guys

01:06:27.920 --> 01:06:33.200
 just see hasn't done this, but like ignoring us. And I'm like, what is that like? I'm like,

01:06:33.200 --> 01:06:39.040
 yeah, like, bun didn't go and add like the best way to run JavaScript for humans and AI, you know.

01:06:41.680 --> 01:06:45.600
 I'm not making, I'm not making fun of Z because it would Z it like actually makes sense. But a lot

01:06:45.600 --> 01:06:52.560
 of just general purpose things have now added and AI to it. So I'm like, how are they thinking

01:06:52.560 --> 01:06:58.000
 about this stuff? Like they're just in a way like heads down ignoring it. I'm sure they're not

01:06:58.000 --> 01:07:03.440
 actually, but like, you know, their strategy is heads not ignoring it. Yeah. Oh, but all right,

01:07:03.440 --> 01:07:07.040
 this is probably a coincidence, but I went to bunsight and they have a used bisection.

01:07:07.680 --> 01:07:12.960
 And one of them is mid one of them is mid journey. So they also kind of they're like,

01:07:12.960 --> 01:07:20.640
 it's just a coincidence tip of the hat to AI used by X type form mid journey until and that's

01:07:20.640 --> 01:07:25.760
 an interesting collection of companies. You know who else uses it? Terminal. We got to get the

01:07:25.760 --> 01:07:31.040
 terminal on the bunsight. I let's go. I think I might be the number one bun user. I'm not here.

01:07:31.040 --> 01:07:35.680
 You might be. I think I'm the number one buns user because I use it. Actually, I'm I've been

01:07:35.680 --> 01:07:41.200
 bun pilled. I'm enjoying bun quite a lot because I just copy everything you do. And I cannot stop

01:07:41.200 --> 01:07:46.400
 talking about of like how good their product execution is like it is so good. Yeah, they're

01:07:46.400 --> 01:07:53.360
 incredible. Every single time they put out a feature, I've been like, I don't get it. And then

01:07:53.360 --> 01:07:58.640
 fast forward three weeks later, I'm using it like it just like invisibly just snuck into

01:07:58.640 --> 01:08:04.800
 every little piece. So we're launching a new update in the s t console. We had this workflow

01:08:04.800 --> 01:08:11.520
 section that's the config where you can like set up your CI apps. And before we didn't let that

01:08:11.520 --> 01:08:14.720
 be configured. So most people don't have to the defaults make sense. But if you want to configure

01:08:14.720 --> 01:08:19.760
 it, we were like, okay, how do we like let you run shell scripts but like in JavaScript and

01:08:19.760 --> 01:08:23.680
 have your own jobs with conditionals? And we're like, okay, fuck it, we're just going to drop

01:08:23.680 --> 01:08:28.960
 bun shell in there. So now the config is just like your workflow is just bun shell. And they

01:08:28.960 --> 01:08:33.840
 already figured out all that stuff. So really great product execution. Amazing.

01:08:33.840 --> 01:08:38.000
 There's nothing better than that like a weight dollar sign and then put your shell

01:08:38.000 --> 01:08:43.520
 command in there. It feels so good. Yeah. Yeah, yeah, that's cool. All right, I gotta go for

01:08:43.520 --> 01:08:50.000
 non biological reasons. Okay, another biological. No, don't believe you. I got to go, Dax. When I

01:08:50.000 --> 01:08:54.240
 said I got to go and you're like, one more thing. And then you have like four more things. We could

01:08:54.240 --> 01:08:59.760
 pause if you want to do a two hour episode. No, that's fine. You can go. You don't want to talk to

01:08:59.760 --> 01:09:05.600
 me. It's fine. I want to talk to you. I'm going to keep myself. This is our last episode ever.

01:09:05.600 --> 01:09:09.520
 No, it's not. We're not going to do this podcast anymore. Stop it. Adam doesn't want to talk to me.

01:09:09.520 --> 01:09:14.000
 Okay, I'm going. Goodbye.

01:09:29.760 --> 01:09:37.920
 [BLANK_AUDIO]

